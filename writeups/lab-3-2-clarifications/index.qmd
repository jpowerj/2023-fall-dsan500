---
title: "Lab 3.2 Clarifications"
date: last-modified
categories:
  - "Clarifications"
metadata-files: 
  - ../../_doc-meta.yml
---

::: {.callout-note title="Lab 3.2 Assignment Link"}

<a href='https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/labs/assignments/lab-3.2-NB-and-feature-selection/assignment.html' target='_blank'>Lab 3.2 Assignment Page <i class='bi bi-box-arrow-up-right ps-1'></i></a>

:::

## Assignment-1: Spearman vs. Pearson Correlation Coefficients

For the part labeled *Assignment-1* within your Lab 3.2 Assignment, you are asked to fill in the code for the following function:

```r
def merit(x,y,correlation="pearson"):
    # x=matrix of features 
    # y=matrix (or vector) of targets 
    # correlation="pearson" or "spearman"
    
    # INSERT CODE HERE  
```

As written here, this function therefore has to accept an **optional argument** called `correlation`, such that:

* If someone calls the function **without** specifying a value for the `correlation` argument, then it should default to using the **Pearson correlation coefficient**:

```r
merit(x, y) # Should use Pearson method
```

* Otherwise, if someone calls the function **with a value specified** for the `correlation` argument, then it should use the specified approach:

```r
merit(x, y, 'pearson') # Should use Pearson method
merit(x, y, 'spearman') # Should use Spearman method
```

### What Are Spearman and Pearson Coefficients?

To clarify this conceptually, I've found the following diagram extremely helpful:

![Image Source: [*Daily Dose of DS* Substack post](https://www.blog.dailydoseofds.com/p/the-biggest-limitation-of-pearson){target='_blank'}](images/pearson_vs_spearman.jpg)

That is, both approaches should agree on the correlation between $x$ and $y$ values which are **linearly** correlated, but (at least, in the context of the data that the Substack post is talking about) the **Spearman** approach can be better at "detecting" **non-linear** relationships between $x$ and $y$ values.

### The Practical Difference (In Code)

For your sake, though, it is probably more important to know the difference in terms of the **code** you would use to find the specific type of coefficient that the person calling your function is asking for. So, by Googling "Spearman correlation in Python" and "Pearson correlation in Python", you can find a bunch of approaches (and, you should look at them!), but I have found that Python's **SciPy library** is extremely helpful for us here, since it contains straightforward implementations of **both** the Spearman and Pearson approaches within the same module:

* <a href='https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html' target='_blank'>`pearsonr()` in SciPy</a>
* <a href='https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html' target='_blank'>`spearmanr()` in SciPy</a>

### What Do `rho_xx` and `rho_xy` Represent?

I also wanted to clarify, quickly, that the code immediately **above** the *Assignment-1* section of the assignment page is not actually **computing the merit score for a dataset**, it is just **showing example values that you might obtain** for the merit score.

Meaning, the reason why you don't see any actual data (any actual `DataFrame`s or NumPy matrices, for example) in those code blocks is because there is no data: these code blocks are to show you the types of numeric values that you would **obtain** for the merit score **if you already knew** the values to plug in.

In your case, though, you cannot just plug in some numeric value like `rho_xx=0` or `rho_xy=-1`. You have to **compute** these two values **from** a dataset containing $X$ and $y$ data (for example, the data that is generated by the code block with `#GENERATE INTENTIONALLY CORRELATED DATA` at the top), as follows:

`rho_xx`:

The mean of $\textsf{corr}(x_i, x_j)$, where $x_i$ represents the vector of data for feature $i$, $x_j$ represents the vector of data for feature $j$, and $\textsf{corr}()$ is the specified correlation method (Pearson or Spearman)

`rho_xy`:

The mean of $\textsf{corr}(x_i, y)$, where $x_i$ represents the vector of data for feature $i$, $y$ represents the vector containing **labels** for each observation, and $\textsf{corr}()$ is the specified correlation method.

All of this is to say that, **when writing the `merit()` function**, I **strongly recommend writing two additional helper functions**, like I do in the feature selection lab writeup:

* `compute_mean_xx_corr(X)` and
* `compute_mean_xy_corr(X, y)`

Where `compute_mean_xx_corr(X)` will compute the mean of **all pairwise correlations between columns of $X$**, and hence will produce `rho_xx`, while `compute_mean_xy_corr(X, y)` will compute the mean of **the correlation between each column of $X$ and the vector $y$**, and hence will produce `rho_xy`.

Then, using these two functions as "subroutines" for your `merit()` function, it will start to look a lot more like the code blocks above the start of the *Assignment-1* section. As general starter code, for example, your code could look something like the following:

```r
def merit(X, y, correlation='pearson'):
  k = _ # Number of columns in X
  rho_xx = _ # Something that computes mean of corr(x_i, x_j)
  rho_xy = _ # Something that computes mean of corr(x_i, y)
  merit_numerator = k * rho_xy
  merit_denominator = np.sqrt(k + k * (k - 1) * rho_xx)
  merit_score = merit_numerator / merit_denominator
  return merit_score
```

Hopefully that helps, in terms of thinking about the **additional stuff** you have to do to **obtain** the `rho_xx` and `rho_xy` values, that you then **use** to compute the merit score using the equation given at the beginning of the assignment.

## Assignment-1: Seeding NumPy's Random Number Generator for Replicable Results

One complicating factor for finishing the section labeled **Assignment-1** is that, each time you run the code, it will generate a **different** random `x` matrix and `y` vector. This is good for ensuring that your function works for different possible matrices and label values, but also makes it impossible to compare values between implementations (meaning, there is no "correct" merit score value, since the merit scores are relative to the randomly-generated matrix `x` and vector `y`).

So, to make sure that your results are **replicable**---for example, to make sure that your code is running the same on your laptop and on your TA's laptop[^ta-changes], you can add the following line creating a NumPy `rng` (Random Number Generator) object to the end of the import code:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, pearsonr
import sklearn

# SEED THE NUMPY RNG PLEASEEEEEE
rng = np.random.default_rng(seed=5000)
```

and then you can change the example code given to you (the code that generates a **random** `x` matrix and `y` vector) to the following, which will ensure that the `x` matrix and `y` vector that get generated each time you restart your notebook will be the same:

```python
#GENERATE INTENTIONALLY CORRELATED DATA
N=100
x=np.zeros((N,4))
#UN-CORRELATED INPUTS
x[:,0]=rng.uniform(0,10,N)
x[:,1]=rng.uniform(0,10,N)

#CORRELATED INPUTS
#print(np.random.normal(0,1,N).shape,N)
x[:,2]=x[:,0]+rng.normal(0,1,N)
x[:,3]=x[:,1]+rng.normal(0,1,N)

#CORRELATED OUTPUT
y=(3*x[:,0]).reshape(N,1)+5*(x[:,1]).reshape(N,1)
```

Note that this code is **almost identical** to the code you were given in the assignment, with one aspect changed: everywhere in the original code that used `np.random.uniform()`, we now use `rng.uniform()`, and everywhere in the original code that used `np.random.normal()` we now use `rng.normal()`. This ensures that those functions, rather than generating truly pseudo-random numbers each time the code is run, will now generate the **same** sequence of pseudo-random numbers each time you restart the notebook and re-run the code block.

This will be especially helpful for me being able to work with you to debug your code, since if you and I both use the same **seed** (in this case, the number `5000`) for our `rng` object, then we will produce the same sequence of random `x` and `y` values, so that we will be able to compare whether we get the same output in terms of the merit scores.

[^ta-changes]: Note, however, that one of the things your TAs will be **checking** is precisely that your functions work for **any** randomly-generated `x` matrix and `y` vector, not just the `x` matrix and `y` vector generated by a certain seed.