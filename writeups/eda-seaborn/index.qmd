---
title: "Exploratory Data Analysis (EDA) Using Seaborn"
format:
  html:
    cache: false
metadata-files: 
  - "../../_doc-meta.yml"
---

In this writeup we will look at how to take a dataset and perform some simple EDA-style visualizations using the [**Seaborn data visualization library**](https://seaborn.pydata.org/index.html){target="_blank"} for Python.

Just like with NumPy or Pandas, Seaborn is a **third-party** library (i.e., it is not built-in to Python), so if you haven't installed it already you'll have to install it using for example (in the Terminal / PowerShell)

```bash
pip install seaborn
```

Once it is installed, then just like how we created the **aliases** `pd` for Pandas and `np` for NumPy using

```python
import pandas as pd
import numpy as np
```

We also typically import Seaborn using the **alias** `sns`, like

```python
import seaborn as sns
```

So that the Seaborn functions we use from here on out will be those prefixed with `sns.`

## EDA Task Overview

{{< include ../../_py_globals.qmd >}}

```{python}
#| label: load-seaborn-dataset
import seaborn as sns
```

Since the [Lab Demo on the main website](https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/seaborn-introduction/notes.html){target="_blank"} walks you through visualizing a **numeric** dataset (data on cars and their engines), here I want to help you get more comfortable with **data derived from texts** (text-as-data), so we will be using a **corpus** which is an expanded version of the 3-text corpus I've been using in class: the corpus will contain a **mixture** of (a) 20th-century works of experimental fiction and (b) NLP textbooks, and our EDA will be focused on exploring the **distinctive features** of these two groups of texts, so that by the end I hope you can see how each type of visualization that Seaborn is capable of producing can help us analyze data from a range of different perspectives!

## Gathering the Corpus

In general, if you are carrying out a full-on NLP-based project, it is crucial for reproducibility and transparency that you **document your text-gathering steps** as you build your corpus. In the same way that **we can't use statistics to study a sample of individuals if we don't know how these individuals were sampled**, we also **can't use NLP to study a sample of texts if we don't know how these texts were sampled/selected for inclusion in the corpus**.

### The 20th-Century Fiction Corpus

In our case here, the subcorpus of 20th-century texts was created by taking a list of **all** popular 20th-century works of fiction and computing the **word-to-word entropy** of these texts: long story short, you can think of this as the "unpredictability" of the text, in the sense that these are the texts where $\Pr(\text{next word} \mid \text{current word})$ is hardest to model. Under the definition used here, the book *The Cat in the Hat* is an example of a **low-entropy text**, since (e.g.) seeing the word "cat" allows you to predict with a degree of certainty that the next 3 words will be "in the hat".^[For anyone interested in this concept, as applied to works of fiction, here is a [presentation](https://docs.google.com/presentation/d/1AKOYO2E5MkosMh58wZZQuW-wvQKlh43BEXcRh4GbeqU/edit?usp=sharing){target="_blank"} that me and a friend from Columbia's CS program gave as our final project for a grad-level NLP class! (As in, if you are stumped as to what to do for your final project, but you know you're interested in data science using NLP, feel free to choose something like this as your project topic, and I will be happy to help with it! ðŸ˜œ)]

### The NLP Textbook Corpus

Gathering a corpus of NLP textbooks was much simpler, since there are far fewer NLP textbooks than there are 20th-century works of fiction. So, in this case, I used a collection of syllabi collected from various NLP courses taught at universities across the world, and selected the top 10 most-frequently-assigned textbooks across this sample of syllabi.

### Reproducibility

Hopefully this section makes it clear how you could **reproduce** my sample: if you take the same lists that I use, and apply the same rules (top-10 in terms of entropy for fiction, top-10 most-frequently-assigned for textbooks), you should end up with the same corpus. Then, for **replication** of any results I find, the idea would be to try gathering ten highly-entropic works of 20th-century fiction and ten popular NLP textbooks in a **different manner**, and see if the results are mostly similar. If they are, that means that we can have high confidence that the result was not based on some **specific aspect of the *sample***, but is a result that holds in general for corpora that are split half-and-half between experimental fiction and NLP textbooks. This type of replication is often therefore called a **robustness check** or **sensitivity analysis**: we are seeing how **sensitive** our results are to the choice of corpus, for example.

## Loading and Cleaning the Texts

### Loading `.txt` Files into Python Dicts

```{python}
import glob
import os

def read_txt(txt_fpath):
  """
  Helper function for taking filepaths (fpaths) and reading then into Python as strings
  """
  # Ensure it returns None if the file could not be read
  txt_contents = None
  with open(txt_fpath, 'r', encoding='utf-8') as infile:
    txt_contents = infile.read()
  return txt_contents

def load_doc(txt_fpath, subcorp_name):
  # First we use read_txt to get the text contents
  # of the doc
  txt_contents = read_txt(txt_fpath)
  # The basename() function here just extracts the *name* of the file (rather than the entire path)
  filename = os.path.basename(txt_fpath)
  return {
    'filename': filename,
    'subcorpus': subcorp_name,
    'text': txt_contents
  }

corpus_paths = [
  "./fiction-corpus/",
  "./textbook-corpus/",
]
# This will eventually hold one *dictionary* for each
# text in our corpus, with keys called `filename` and `text`
corpus_docs = []
for cur_path in corpus_paths:
  print(f"Scanning corpus: {cur_path}")
  # Create wildcard string like "./fiction-corpus/*.txt" to give to glob
  glob_str = os.path.join(cur_path, "*.txt")
  # Use the glob library to get a list of all *.txt files in the directory, as filepaths
  fpaths = glob.glob(glob_str)
  # Read each file into a string (so that this line produces a *list* of strings)
  docs = [load_doc(fpath, subcorp_name = cur_path) for fpath in fpaths]
  # And *extend* corpus_docs to contain these newly-loaded strings
  corpus_docs.extend(docs)

# To check that the loop ran successfully, we print
# out the *filenames* of each document that was loaded
print("Loaded files:")
for doc in corpus_docs:
  print(doc['filename'], doc['subcorpus'])
```

### Preprocessing

There are a lot of details that you need to keep in mind when preprocessing, sadly but truly (see slides for examples of why), but for my use cases the "standard" processing steps I usually perform are those in the following helper function:

```{python}
from unidecode import unidecode
from nltk.tokenize import word_tokenize
def preprocess_text(text_str):
  # 1. Lowercasing
  text_cleaned = text_str.lower()
  # 2. Converting "special" Unicode to "standard" ASCII characters
  # (you **DO NOT** want to do this if you are processing non-English text, though!)
  text_cleaned = unidecode(text_cleaned)
  # 3. Tokenizing

```

