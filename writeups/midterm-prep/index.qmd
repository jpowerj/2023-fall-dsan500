---
title: "Midterm Prep Practice Problems"
date: last-modified
categories:
  - "Extra Writeups"
format:
  html:
    df-print: kable
    cache: false
metadata-files: 
  - ../../_doc-meta.yml
---

{{< include ../../_py-globals.qmd >}}

::: {.callout-tip title="Update Log"}

* *2023-10-15, 8pm EST*: Added example essay question on **Classification vs. Regression**
* *2023-10-15, 7pm EST*: Added example multiple choice and essay questions on **EDA**
* *2023-10-15, 6pm EST*: Added R and Python code demonstrating **Wide vs. Long Datasets** and how to transform data between these two formats
* *2023-10-15, 5pm EST*: Added overviews and practice problems on **Conda**, **Typecasting**
* *2023-10-14, 11pm EST*: Added overview of **object-oriented programming in Python**, along with a practice problem on this topic.
* *2023-10-14, 10pm EST*: Initial version of document, emailed out in announcement to students.

:::

This writeup is organized so that each section represents a particular topic/subtopic that the exam may cover, such that the example problems within each topic/subtopic will prepare you for questions on that topic/subtopic which may appear on the midterm!

## Data Science Fundamentals

### Filepaths

For these practice problems, assume we have a directory structure on our computer that looks as follows, and that **this is the entire directory structure** (that is, there are no additional files or folders on the computer that are not displayed in the diagram).

```{dot}
digraph G {
    rankdir="TB"
    node[shape="plaintext",fontname="Courier New"];
    "nodeRoot" [
        label=<<table>
        <tr>
          <td border="0"><b>/</b></td>
        </tr>
        <tr>
          <td>file1.txt</td>
        </tr>
        <tr>
          <td>file2.txt</td>
        </tr>
        <tr>
          <td>rootImage.jpg</td>
        </tr>
        </table>>
    ];
    "nodeUsers" [
        label=<<table>
        <tr>
          <td border="0"><b>Users</b></td>
        </tr>
        </table>>
    ];
    "nodeApps" [
        label=<<table>
        <tr>
          <td border="0"><b>Applications</b></td>
        </tr>
        <tr>
          <td>App1.app</td>
        </tr>
        <tr>
          <td>App2.app</td>
        </tr>
        <tr>
          <td>appImage.png</td>
        </tr>
        </table>>
    ];
    "nodeJeff" [
        label=<<table>
        <tr>
          <td border="0"><b>jpj</b></td>
        </tr>
        <tr>
          <td>doc1.qmd</td>
        </tr>
        <tr>
          <td>doc2.qmd</td>
        </tr>
        <tr>
          <td>jeffImage.jpeg</td>
        </tr>
        </table>>
    ];
    nodeRoot -> nodeUsers;
    nodeRoot -> nodeApps;
    nodeUsers -> nodeJeff;
}
```

The way this diagram works is:

* The **names of folders** are bolded, and if a **folder** `X` is contained within a folder `Y`, then the diagram has an arrow between folder `X` and folder `Y`.
    * So, for example, since the **Users** folder here is a subfolder of the **/** folder (the *root folder*), there is an arrow pointing from the box with label **/** to the box with label **Users**.
* If a **file** `f` is contained within a folder, then the file is displayed as a box within the box representing the folder.
    * So, for example, since the document `doc1.qmd` is contained within the folder labeled **`jpj`**, there is a smaller box labeled `doc1.qmd` within the box labeled **`jpj`**.

Please answer the following questions, with reference to this directory structure:

1. **True** or **False**: If I am writing the document `doc1.qmd`, and I want to include the image `jeffImage.jpeg` from within the same folder, I can use the **relative path** `jeffImage.jpeg`, like

    ```
    ![Photo caption](jeffImage.jpeg)
    ```

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

**True**: Since `jeffImage.jpeg` is in the same directory as the `.qmd` file we're authoring, we can reference it using a relative path to the file which is actually just the name of the file (since the Quarto compiler doesn't need to navigate to any other folder to retrieve the file).
:::

2. **True** or **False**: If I am writing the document `doc1.qmd`, and I want to include the image `jeffImage.jpeg` from within the same folder, I can use the **absolute path** `/Users/jpj/jeffImage.jpeg`, like

    ```
    ![Photo caption](/Users/jpj/jeffImage.jpeg)
    ```

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

**True**: In this case, it actually doesn't matter where the `.qmd` file we're authoring exists on the filesystem, since we're using an **absolute path**. This means that, no matter where the `.qmd` file is, the Quarto compiler will be able to find the image file, by starting at the root directory `/`, navigating to the `Users` subdirectory, then the `jpj` subdirectory, and finally locating the `jeffImage.jpeg` file within this `jpj` subdirectory.
:::

3. **True** or **False**: If I am writing the document `doc1.qmd`, and I want to include the image `jeffImage.jpeg` from within the same folder, I can use the **relative path** `images/jeffImage.jpeg`, like

    ```
    ![Photo caption](images/jeffImage.jpeg)
    ```

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

**False**: In this case, the Quarto compiler will crash with an error. It will attempt to look for an `images` subdirectory within the directory where the `.qmd` file is located, and it will not find a subdirectory called `images` (since there are no subdirectories within the `jpj` folder), producing an error.
:::

### HTTP

Please match each of the following **HTTP codes** on the left to their **meanings** on the right (that is, what it means if we receive that code as a response to our call to the server):

::: {layout="[1,2]"}

| | HTTP Code |
| - | - |
| 1 | 200 |
| 2 | 301 |
| 3 | 403 |
| 3 | 404 |
| 4 | 500 |

| | Meaning |
| - | - |
| A | Error: the server software could not find the requested file or data |
| B | Successful request (the body of the request should thus contain the html code, or json data, or whatever else was requested) |
| C | Server error: the server software crashed or experienced an error while trying to respond to the request |
| D | Not authorized: the requested file is protected by a password that was not given correctly, for example, or it is restricted only to logged-in users and the user making the request was not logged in. |
| E | Redirect: A response with this code indicates that the server is going to redirect the user to a different page within the website, or a different website entirely. |

: {tbl-colwidths="[10,90]"}

:::

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

1. **B**. 200 is the HTTP code for a **successful request**
2. **E**. 301 is the HTTP code for a **redirect**
3. **D**. 403 is the HTTP code for a **non-authorized** request
4. **A**. 404 is the HTTP code for **file not found**
5. **C**. 500 is the HTTP code for an **unknown/unspecified server error**
:::

### Git/GitHub

[From [Week 04 Slides](../../w04/index.qmd){target='_blank'}] Please match each of the following `git` commands with the description of what it does:

::: {layout="[1,2]"}

| | Command |
| - | - |
| 1 | `git clone` |
| 2 | `git init` |
| 3 | `git add` |
| 4 | `git reset` |
| 5 | `git status` |
| 6 | `git commit -m "message"` |
| 7 | `git push` |
| 8 | `git pull` |
| 9 | `git merge` |

| | Description |
| - | - |
| A | Downloads a repo from the web to our local computer |
| B | Merges remote versions of files with local versions |
| C | Creates a new, blank Git repository on our local computer (configuration/change-tracking stored in `.git` subfolder) |
| D | Shows currently staged files and their status (created, modified, deleted) |
| E | "Saves" the current version of all staged files, ready to be pushed to a backup dir or remote server like GitHub |
| F | Transmits local commits to remote server |
| G | **Stages** a file(s): Git will now track changes in this file(s) |
| H | Downloads commits from remote server to local computer |
| I | Undoes a `git add` |

: {tbl-colwidths="[1,2]"}

:::

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

1. **A**: The `git clone` command downloads a repo from the web to our local computer
2. **C**: The `git add` command creates a new, blank Git repository on our local computer (configuration/change-tracking stored in `.git` subfolder) 
3. **G**: The `git add` command **stages** files; Git will now track changes in these files
4. **I**: The `git reset` command undoes a previous `git add` command
5. **D**: The `git status` command shows currently staged files and their status (created, modified, deleted)
6. **E**: The `git commit` command "saves" the current version of all staged files, ready to be pushed to a backup dir or remote server like GitHub
7. **F**: The `git push` command transmits local commits to remote server
8. **H**: The `git pull` command downloads commits from remote server to local computer
9. **B**: The `git merge` command merges remote versions of files with local versions
:::

### Python: Object-Oriented Programming (Overview)

We didn't get to talk about this much in class, so here I will give a quick overview and then a practice problem afterwards.

The way that <a href='https://en.wikipedia.org/wiki/Object-oriented_programming' target='_blank'>Object-Oriented Programming</a> is implemented in Python is as follows:

Python provides you with a set of "default" object types: starting from the base <a href='https://docs.python.org/3/library/functions.html#object' target='_blank'>`object` class</a>, for example, it provides classes like `int`, `float`, `string`, `boolean`, and so on.

However, sometimes these base classes on their own, or combinations of them, cannot serve the purposes that you need for your particular project/application. In these cases, you can create your own **custom classes**, with syntax that looks like the following

*(**NOTE**: Despite the fact that we're not explicitly defining `Vehicle` here to be a sub-class of any other class, **by default** in Python **all classes are subclasses of the `object` class mentioned above**)*

```{python}
class Vehicle:
    def __init__(self, maker, num_wheels):
        self.maker = maker
        self.num_wheels = num_wheels
```

By defining the class using the line `class Vehicle` at the top, and then defining a **constructor function** called `__init__()`, we can now create new objects with the **type** `Vehicle` by using syntax like the following:

```{python}
my_vehicle = Vehicle("Toyota", 4)
```

Which will call the `__init__()` function defined above, such that now the value `"Toyota"` will be stored in the **instance variable** called `maker` within the `my_car` object, and the value `4` will be stored in the instance variable called `num_wheels` within the `my_obj` object. The **object** `my_obj` here is therefore an **instance** of the class `Vehicle`.

This means that, from this point onwards, we can access this information (the values contained in `maker` and `num_wheels`) using the **dot operator** `.` on the object, like:

```{python}
print(my_vehicle.maker, my_vehicle.num_wheels)
```

Now that we've seen how to define a **class**, providing a **template** for creating objects, and how to **create objects** using the **constructor function** within a **class**, let's now look at the notion of **inheritance**.

**An issue with OOP in *Python* (unlike many other languages) is that Python does *not* allow you to create multiple versions of the same function within the same class**.

This means, for example, that we could **not** add a `start_vehicle()` function to our `Vehicle` class which would behave differently given different input types. The following new version of `Vehicle` will produce an error:

```{python}
class Vehicle:
    def __init__(self, maker, num_wheels):
        self.maker = maker
        self.num_wheels = num_wheels
    
    def start_vehicle(self):
        """
        Version for cars
        """
        # Start the car

    def start_vehicle(self):
        """
        Version for motorcycles
        """
        # Start the motorcycle
```

So, if we find ourselves in a case where we want to model a **hierarchy** of more-general to more-specific versions of a class, we can use **inheritance** to define **sub-classes** of a given class which represent a more specific "version" of this class.

So, using our `Vehicle` class as our working example, we may find ourselves defining **four-wheeled** vehicles very often, since **cars** are common vehicles which always have four wheels, but also **two-wheeled** vehicles very often, since **motorcycles** are common vehicles which always have two wheels.

So, to make our lives easier, rather than having to specify the number of wheels each time we want to create an object representing a car or motorcycle, instead we can create **sub-classes** of `Vehicle` representing cars and motorcycles, as follows:

```{python}
class Car(Vehicle):
    def __init__(self, maker):
        super().__init__(maker, 4)

class Motorcycle(Vehicle):
    def __init__(self, maker):
        super().__init__(maker, 2)
```

Now that we have these two sub-classes, defined in this way, we can create **`Car`** and **`Motorcycle`** objects, which will have those types (`Car` and `Motorcycle`, respectively) but will **also** both have type `Vehicle`, and will have their `num_wheels` value set automatically, without us having to provide these values explicitly to the `Car` and/or `Motorcycle` constructors:

```{python}
my_car = Car("Toyota")
my_motorcycle = Motorcycle("Kawasaki")
```

And now, despite the fact that `my_car` and `my_motorcycle` are of different sub-types, since they are both objects of type `Vehicle` we know that we can access a field called `num_wheels` on both:

```{python}
print(my_car.num_wheels, my_motorcycle.num_wheels)
```

So, we can see from this output that `num_wheels` has been set correctly, automatically, despite the fact that we did not explicitly specify the values `4` or `2` when creating our `my_car` and `my_motorcycle` objects above.

### Python: Object-Oriented Programming (Practice Problem)

Given the above overview, hopefully the following practice problem becomes more manageable!

**Fill in the blanks to make the statement correct, by choosing the correct answer from each dropdown menu**:

```{=html}
Defining a <select name="cars" id="cars" style="display: inline-block">
  <option value="answer1">class</option>
  <option value="answer2">string</option><option value="answer3">struct</option>
</select> in Python (which should include a constructor function with the name <select name="cars2" id="cars2" style="display: inline-block"><option value="answer1">__init__()</option><option value="answer2">__start__()</option><option value="answer3">__letsgooo__()</option></select>), allows us to create <b>objects</b>, each of which is <select><option value="answer1">an instance</option><option value="answer1">a version</option></select> of the class.
```

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

Defining a **class** in Python (which should include a constructor function with the name **`__init__()`**), allows us to create <b>objects</b>, each of which is **an instance** of the class.
:::

### Conda

*(Material from [**Week-02 course notes**](https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/coding-methods/notes.html#conda){target='_blank'})*

Please match each of the `conda` **commands** on the left with their corresponding **descriptions** on the right:

::: {layout="[1,1]"}

| | Command |
| - | - |
| 1 | `conda deactivate` |
| 2 | `conda activate myenv` |
| 3 | `conda info --envs` |
| 4 | `conda remove --name myenv --all` |
| 5 | `conda rename -n old_name new_name` |

| | Description |
| - | - |
| A | Remove an environment named "myenv" |
| B | Deactivate current environment |
| C | Rename an environment |
| D | List all environments |
| E | Activate an environment named `myenv` |

: {tbl-colwidths="[10,90]"}

:::

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

1. **B**: The `conda deactivate` command deactivates the current environment.
2. **E**: The `conda activate myenv` command activates an environment named `myenv`
3. **D**: The `conda info --envs` command lists all environments
4. **A**: The command `conda remove --name myenv --all` removes an environment named `myenv`
5. **C**: The command `conda rename -n old_name new_name` renames the environment named `old_name` to be named `new_name`
:::


### Typecasting: Overview

The concept of **typecasting** is one we didn't get to discuss much in class, but it's an operation that we use all the time when writing Python code: for example, every time we **print the value of an `int` or `float` variable**, we are implicitly **typecasting** the `int` or `float` to a **`string`** variable, as in the following example:

```{python}
one_plus_one = 1 + 1
print(f"One plus one is {one_plus_one}")
```

A **crucial concern with typecasting** (the main reason we worry about it as data scientists) is that we need to ensure that **the information we want to retain is indeed retained** when we perform a typecasting operation, whether implicitly or explicitly.

For example, if we make a variable called `my_pi` which we're hoping to use to represent $\pi$ to the maximum precision possible (e.g. for computing areas of circles), then we need to make sure that we never **typecast** its value to a numeric type like `int` that cannot represent the decimal precision necessary for using $\pi$:

```{python}
import math
my_pi = math.pi
print(my_pi)
my_pi = int(my_pi)
print(my_pi)
```

This may seem like a silly example, since for values like `my_pi` we would usually be aware and careful about loss of precision, but it can become more and more easy to make these types of mistakes---where typecasting leads to a **loss of information**---when you are working with a large collection of numeric variables.

More formally, we can say that **information is lost** when typecasting a variable `x` into some other type to produce a new variable `y` if it would not be possible to recover the information contained in `x` from the new variable `y`. So, given this definition,

```{python}
my_int = 3
my_str = str(my_int)
```

Would **not** be a case where information is lost, since we could always recover the original `my_int` by typecasting **back** to the `int` type;

```{python}
my_new_int = int(my_str)
print(my_new_int, my_int)
my_new_int == my_int
```

However, in the following code block

```{python}
my_float = 3.14
my_int = int(my_float)
```

We have performed a typecasting operation which has resulted in a **loss of information**, since unlike in the previous example, we are unable to re-create the original `my_float` value by typecasting `my_int` back to the `float` type:

```{python}
my_new_float = float(my_int)
print(my_float, my_new_float)
my_float == my_new_float
```

### Typecasting: Practice Problem

1. Assume we have just created a variable `my_float` using the line `my_float = 3.14`. Which of the following Python expressions represents a **typecasting** operation where **information is lost**?
    a. `my_int = int(my_float)`
    b. `my_str = str(my_float)`
    c. `my_float2 = float(my_float)`
    d. `my_float3 = np.float64(my_float)`

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

The correct answer is **a**: When we convert from the `float` type to the `int` type, the decimal information from the `float` is **lost**, since the resulting `int` will contain only the non-decimal information from the `float`. So, for example, `int(3.14)` produces the `int` value `3`, `int(-10.001)` produces the `int` value `-10`, and so on.
:::

## Steps in the Data Science Pipeline

### Data Cleaning: Overview

When talking about **tabular datasets** (as in, datasets which can be represented as tables, and hence as `DataFrame`s in Pandas or `tibble`s in Tidyverse), we call a dataset:

* **Long** if the dataset contains **multiple rows** for each **unit of observation** in the data, and
* **Wide** if the dataset only contains **one row** for each **unit of observation** in the data.

For example, if the unit of observation is a **country**, then the following dataset is in **long** format, since it contains **two rows** for each country:

```{r}
#| label: long-dataset-tidyverse
#| warning: false
#| code-fold: true
library(tidyverse)
long_df <- table2 |> filter(
    year == 2000
) |> select(
    country, type, count
) |> rename(
    variable = type,
    value = count
)
long_df |> write_csv("assets/long_df.csv")
long_df
```

On the other hand, the following dataset (with the same unit of observation) is in **wide** format, since each country's information is contained in one and only one row.

```{r}
#| label: tidyverse-wide-df
wide_df <- table1 |> filter(
    year == 2000
) |> select(
    country, cases, population
)
wide_df |> write_csv("assets/wide_df.csv")
wide_df
```

We can use the Pandas function `pd.melt()`, however, to **convert this second dataset from wide to long format**, so it matches the format of the first dataset, as follows:

```{python}
#| label: wide-to-long-pandas
import pandas as pd
wide_df = pd.read_csv("assets/wide_df.csv")
long_df = pd.melt(
    wide_df,
    id_vars=['country'],
    value_vars=['cases','population']
)
disp(long_df)
```

### Data Cleaning: Practice Problems

1. Consider the following dataset, where the unit of observation is a **person** on a mailing list:

    | id | name | email | ip_address |
    | - | - | - | - |
    | 0 | Jimmy Fishburn | jfishburn0@ask.com | 10.14.177.175 |
    | 1 | Chadwick Arrowsmith | carrowsmith1@photobucket.com | 186.209.191.254 |
    | 2 | Antony Hartington	| ahartington2@constantcontact.com | 200.241.210.61 |

    Is this dataset in **long** or **wide** format?

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

This dataset is in **wide** format, since the unit of observation is a **person**, and each person is represented as a **single row** in the dataset, so that each column corresponds to a property of a single person.
:::

2. Consider the following dataset, where the unit of observation is again a **person** on mailing list:

    | id | variable | value |
    | - | - | - |
    | 0 | name | Jimmy Fishburn |
    | 0 | email | `jfishburn0@ask.com` |
    | 0 | ip_address | `10.14.177.175` |
    | 1 | name | Chadwick Arrowsmith |
    | 1 | email | `carrowsmith1@photobucket.com` |
    | 1 | ip_address | `186.209.191.254` |
    | 2 | name | Antony Hartington |
    | 2 | email | `ahartington2@constantcontact.com` |
    | 2 | ip_address | 200.241.210.61 |

    Is this dataset in **long** or **wide** format?

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

This dataset is in **long** format, since the unit of observation is a **person**, but the information we have about each person is **spread over three rows**.
:::

3. What Pandas function could we use to transform the dataset from the first problem (the wide-format dataset) into the dataset from the previous problem (the long-format dataset)?
    a. `pd.sort_values()`
    b. `pd.melt()`
    c. `pd.to_numeric()`
    d. `pd.unique()`

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

The correct answer is **a**: `pd.melt()` is the function we use to transform wide-format datasets into long-format datasets, where we supply this function with an `id_vars` parameter telling it the column which **uniquely identifies the unit of observation** in the dataset.
:::

**NOTE:** For more details on the syntax of `pd.melt()` and other functions used for transforming data, please see my writeup on [`pd.melt()` and `pd.pivot()` here](../melt-and-pivot/){target='_blank'}.

### EDA

Exploratory Data Analysis or "EDA" is a broad term, describing what is essentially a toolkit of methods you can use to start to get an understanding of your data **before** you begin developing and testing hypotheses. Given this, on the midterm you may encounter questions about EDA like:

* Multiple-choice questions asking you whether a given technique would be considered an EDA or CDA (Confirmatory Data Analysis) step,
* An essay question asking you to describe a few different examples of EDA techniques,
* An essay question asking you to describe a single EDA technique in-depth, or
* An essay question asking you to describe the differences between Exploratory Data Analysis and Confirmatory Data Analysis.

An example multiple choice question would be one like the following question:

1. Which of the following techniques would **not** be considered part of the Exploratory Data Analysis (EDA) process:
    a. Outlier Detection
    b. Data Transformation
    c. Hypothesis Testing
    d. Bivariate Analysis (Bivariate Plots and Cross-Tabulations)

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

The correct answer is **c**: Hypothesis Testing is very specifically **not** part of the EDA process, since the idea of EDA is that we **"explore"** the data **before** we develop explicit hypotheses. It is on the basis of EDA that we **then** form hypotheses, using techniques from **Confirmatory** Data Analysis to **evaluate** these hypotheses on the basis of the available data.
:::

Then, an example essay question could be as follows:

2. Explain the process of **Outlier Detection** in-depth, and why it is important with respect to the goals of **Exploratory Data Analysis** (**EDA**).

*(Here, just to be safe, I'm placing the solution in a static block, rather than an expand/collapse block, but you should try writing your own essay first before looking at the solution!)*

::: {.callout-note collapse="false" title="Solution"}

An example short essay could be as follows:

**Outlier detection** is the process of analyzing a dataset with respect to a hypothesized **data-generating process** (**DGP**) which is assumed to have generated the observed data, then identifying specific data points which are in the observed data yet would be **unlikely to occur** as a result of this data-generating process.

Although outlier detection is therefore always performed with respect to some model of how the observed data came about (the DGP), there are a set of **widely-used heuristics** which data scientists have found useful to use over the years, as simple "rules of thumb" for identifying these unlikely/extreme values without having to spend too much time worrying about the DGP for every variable.

The two most widely-used outlier detection methods are as follows:

* The **Tukey Rule**: This rule works for a wide range of data distributions and says that, if we have a set of $n$ values for some variable $X$, $\{x_1, x_2, \ldots, x_n\}$, then one of the values $x_i$ can be considered an **outlier** relative to the other values if the value of $x_i$ is less than $1.5 \cdot Q_1$ or more than $1.5 \cdot Q_3$:

    $$
    x_i \not\in [Q_1 - 1.5 \cdot IQR, \; Q_3 + 1.5 \cdot IQR],
    $$

    where $Q_1$ is the first quartile of the data $X$ (the 25th percentile), $Q_3$ is the third quartile of the data $X$, and $IQR$ is the **interquartile range**, defined as $IQR = Q_3 - Q_1$.

* The **Three-Sigma Rule**: This rule was specifically created as a "rule of thumb" for identifying outliers in **normally-distributed** data, but technically could be used for any set of values $X = \{x_1, x_2, \ldots, x_n\}$ for which we know the mean $\mu_X$ and standard deviation $\sigma_X$.

    The rule here is that a data point, in this case a particular value $x_i$, is considered an **outlier** relative to the other values of $X$ if its value is **more than three standard deviations away from the mean**---that is, if

    $$
    x_i \not\in [\mu_X - 3 \cdot \sigma_X, \; \mu_X + 3 \cdot \sigma_X].
    $$

These two rules, along with other methods for detecting outliers, are important for the EDA process because our eventual **Confirmatory** Data Analysis can be **inaccurate**, skewed, biased, and so on, if there are erroneous values present in the data for a given variable.

For example, if a dataset represents the digitized version of a dataset of selling prices for a product which was originally compiled by hand, and the digitization failed to detect the hand-written **decimal points** for some of the prices in the data, then prices like \$9.99 could be erroneously translated into `999`, skewing the values of these prices by two orders of magnitude, while prices like \$10 or \$20 would be unaffected. Outlier detection could thus potentially identify this issue, allowing the researcher to go in and manually fix the errors before moving to the Confirmatory Data Analysis and hypothesis testing stage.

:::

## Text Analysis

We also only had a small amount of time to cover text analysis and Natural Language Processing (NLP) techniques in class, but in this case we really only want you all to know some basics, since you can take an entire class after this one which is solely about NLP!

So, given the basic technique we talked about in class, of **taking a corpus of texts and representing it as a *Document-Term Matrix* (DTM)**, you should be able to answer the following multiple-choice question:

1. Assume we have four documents, `A.txt`, `B.txt`, `C.txt`, and `D.txt`, as follows:

    ```txt {filename="A.txt"}
    Green eggs are sweet.
    ```

    ```txt {filename="B.txt"}
    Ham is sour.
    ```

    ```txt {filename="C.txt"}
    Cats love sweet food.
    ```

    ```txt {filename="D.txt"}
    Dogs love sour food.
    ```

    Which of the following matrices represents the **document-term matrix** encoding these four documents (assuming that we have performed **lowercasing**, **word tokenization**, and **stopword removal** on each document before vectorizing it)?
    
    *(Note: The name for each table is contained in the upper-left-most cell of the table: since (A) appears in the upper-left-most cell of the first table below, for example, this table represents choice (A))*

| (A) | `green` | `eggs` | `sweet` | `ham` | `sour` | `cats` | `love` | `food` | `dogs` |
| - | - | - | - | - | - | - | - | - | - |
| `A` | 4 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 1 |
| `B` | 3 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |
| `C` | 2 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 1 |
| `D` | 1 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |

| (B) | `green` | `eggs` | `sweet` | `ham` | `sour` | `cats` | `love` | `food` | `dogs` |
| - | - | - | - | - | - | - | - | - | - |
| `A` | 1 | 1 | 1 | 0 | 0 | 2 | 2 | 0 | 0 |
| `B` | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |
| `C` | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 0 |
| `D` | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |

| (C) | `green` | `eggs` | `sweet` | `ham` | `sour` | `cats` | `love` | `food` | `dogs` |
| - | - | - | - | - | - | - | - | - | - |
| `A` | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
| `B` | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |
| `C` | 0 | 0 | 1 | -1 | -5 | 1 | 1 | 1 | 0 |
| `D` | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |

| (D) | `green` | `eggs` | `sweet` | `ham` | `sour` | `cats` | `love` | `food` | `dogs` |
| - | - | - | - | - | - | - | - | - | - |
| `A` | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 |
| `B` | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |
| `C` | 0 | 0 | 1 | 0 | 0 | 1 | 1 | 1 | 0 |
| `D` | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |

::: {.callout-note collapse="true" title="Solution (Click to expand)"}

The correct answer is **(D)**. We can tell that this final table is the correct DTM representation for our four documents by a process of elimination, for example:

* The first table (A) contains a value of `4` in the cell representing the count of the token `green` in document `A`, whereas document `A` in fact only contains the token `green` **one time**.
* The second table (B) contains a value of `2` in the cell representing the count of the token `love` in document `A`, whereas document `A` in fact does not contain the token `love` at all.
* The third table (C) contains **negative values**: for example, the cell representing the count of the token `sour` in document `C` has the value `-5`. Since documents cannot contain tokens a negative number of times, we know that this is an invalid DTM in general.
:::

## Machine Learning

### Types of Learning Methods

At a general level (so, for essay questions for example), we want you to understand the distinction between **supervised**, **unsupervised**, and **reinforcement learning**, as well as how the following terms relate to these three types of learning:

* Classes
* Labels
* Binary classification
* Multiclass classification
* Scalar regression
* Vector regression

So, for example, we could ask the following type of essay question to test your understanding of these terms and their interrelationships:

1. Describe how **regression** tasks differ from **classification** tasks in terms of Machine Learning, and then describe how we could **transform** the data we have for our regression task in such a way as to turn it into a **classification** task, and vice-versa. For example, imagine that the codebase at the company you're working for only contains **classification** algorithms, so that you will be unable to learn anything about your data without being able to plug it into a classification algorithm, but all of the variables in the dataset are **continuous**. Then imagine that the codebase instead only contains **regression** algorithms, but that you have features and labels for a **classification** problem that you'd like to solve.

::: {.callout-note collapse="false" title="Solution"}

An example response to this essay question could be as follows:

**Regression** tasks differ from **classification** tasks, in Machine Learning, in that regression tasks involve discovering statistical relationships between a set of features and a **continuous** label variable, while classification tasks involve discovering statistical relationships between a set of features an a **discrete** label variable.

The following two mostly-similar examples illustrate this difference:

* **Classification task**: Predict whether a review for a product will be a **thumbs-up** or **thumbs-down** review, on the basis of the text of the review
* **Regression task**: Predict the numeric score given to a product by a reviewer (where scores are allowed to be any number in the range $[0,100]$), on the basis of the text of the review.

In the former case, our goal is to take the text of a review and use it to predict a **binary label** $\ell$ for this review from the set $\{\text{thumbs up}, \text{thumbs down}\}$, whereas in the latter case our goal is to take the text of a review and use it to predict a **real-valued score** $s$ from the interval $[0,100]$.

Although both of these Machine Learning tasks present their own difficulties, we can often transform one type into the other type via a set of transformations.

For example, to transform a **binary classification** task into a **regression** task, we could ask the regression algorithm to try and predict a real-valued score $s \in [0,1]$, interpret this number as a "confidence score", and convert the real-valued prediction into a binary prediction $\widehat{\ell}$ of the true label $\ell$ by rounding to the nearest integer:

$$
\widehat{\ell}(s) = \begin{cases}
0 &\text{if }s \leq 0.5, \\
1 &\text{otherwise.}
\end{cases}
$$

Conversely, we can transform **regression** tasks into **classification** tasks by **binning** the continuous values into discrete bins.

For example, if we have a dataset with **continuous labels** representing scores $s \in [0,100]$, we could convert these labels into discrete values by **dividing the range $[0,100]$ into three equal bins**, calling values in $\left[0,\frac{1}{3}\right)$ "low" values, values in $\left[ \frac{1}{3}, \frac{2}{3} \right]$ "medium" values, and values in $\left( 2/3, 1 \right]$ "high" values. We could then give all "low" values the label $0$, all "medium" values the label $1$, and all "high" values the label $2$, producing the following mapping from continuous labels (appropriate for regression algorithms) to discrete labels $\ell$ (appropriate for classification algorithms):

$$
\ell(s) = \begin{cases}
0 &\text{if }0 \leq x \leq \frac{1}{3}, \\
1 &\text{if }\frac{1}{3} \leq x \leq \frac{2}{3}, \\
2 &\text{if }\frac{2}{3} \leq x \leq 1. 
\end{cases}
$$

And now we could use a **multi-class classification algorithm** (as opposed to a **binary** classification algorithm, which only works for labels with two possible values) to predict low, medium, or high score values. Then, if we really needed to guess a **continuous** value, we could guess the **midpoint** of each interval, so that the following function could map the predictions $\widehat{\ell}_i$ made by our multi-class classification algorithm back into continuous values $\widehat{s}_i \in [0,100]$:

$$
\widehat{s}_i = \begin{cases}
\frac{1}{6} &\text{if }\widehat{\ell}_i = 0, \\
\frac{1}{2} &\text{if }\widehat{\ell}_i = 1, \\
\frac{5}{6} &\text{if }\widehat{\ell}_i = 2.
\end{cases}
$$

:::

### Distance Metrics

### Precision, Recall, F1 Score

### Loss Functions

* Loss functions: MSE, RMSE, MAE

* Confusion matrix
* Overfitting
* KNN for classification and/or regression