---
title: "Week 9: Clustering Analysis"
date: "Tuesday, October 24, 2023"
date-format: full
lecnum: 9
categories:
  - "Class Sessions"
bibliography: "../_DSAN5000.bib"
format:
  revealjs:
    output-file: slides.html
    cache: false
    footer: "DSAN 5000-<span class='sec-num'>02</span> Week 9: Clustering Analysis"
  html:
    output-file: index.html
    html-math-method: mathjax
    cache: false
    code-fold: true
    warning: false
metadata-files: 
  - "../_slide-meta.yml"
---

::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Schedule {.smaller .small-title .crunch-title .crunch-callout data-name="Schedule"}

{{< include ../_r-globals.qmd >}}

{{< include ../_py-globals.qmd >}}

{{< include ../_tex-globals.qmd >}}

Today's Planned Schedule (Section <span class='sec-num'>02</span>):

{{< include ../_components/sched-w09.qmd >}}

# From W06: Data Generating Process (DGP) {.title-13 .smaller .nostretch .not-title-slide data-stack-name="DGPs for Clusters"}

* You are a teacher trying to assess the **causal impact** of **studying** on **homework scores**
* Let $S$ = hours of studying, $H$ = homework score

![](images/pgm_homework.svg){fig-align="center" width="300"}

* So far so good: we could estimate the relationship via (e.g.) regression

$$
h_i = \beta_0 + \beta_1 s_i + \varepsilon_i
$$

## My Dog Ate My Homework {.smaller}

* The issue: for some students $h_i$ is **missing**, since **their dog ate their homework**
* Let $D = \begin{cases}1 &\text{if dog ate homework} \\ 0 &\text{otherwise}\end{cases}$
* This means we don't observe $H$ but $H^* = \begin{cases} H &\text{if }D = 0 \\ \texttt{NA} &\text{otherwise}\end{cases}$
* In the **easy case**, let's say that dogs eat homework **at random** (i.e., without reference to $S$ or $H$). Then we say $H$ is "missing at random".  Our PGM now looks like:

![](images/pgm_hw_missing_at_random.svg){fig-align="center"}

## My Dog Ate My Homework Because of Reasons {.smaller .small-title .nostretch}

![](images/reasons_crop.jpeg){.absolute top="-15" right="10%"}

There are scarier alternatives, though! What if...

::: {layout-ncol=3}

::: {#dog-study}

Dogs eat homework because their owner studied so much that the dog got ignored?

![](images/pgm_hw_missing_study.svg){fig-align="center" width="280"}

:::

::: {#dog-bad-hw}

Dogs hate sloppy work, and eat bad homework that would have gotten a low score

![](images/pgm_hw_missing_badhw.svg){fig-align="center" width="280"}

:::

::: {#dog-noisy}

Noisy homes ($Z = 1$) cause dogs to get agitated and eat homework more often, **and** students do worse

![](images/pgm_hw_missing_noisy.svg){fig-align="center" width="280"}

:::

:::

## Why Do We Need To Think About DGPs? {.nostretch}

::: {layout-ncol=2}

![Figure (and DGP analysis) from @dignazio_numbers_2020](images/kidnappings.jpg){fig-align="center"}

::: {#kidnapping-dgp}

Presumed DGP:

![$K$ = Kidnappings, $C$ = Counts](images/dgp_kidnappings.svg){fig-align="center" width="300"}

Actual DGP:

![$K$ = Kidnappings, $R$ = **News reports** about kidnappings, $C$ = Counts](images/dgp_kidnappings_true.svg){fig-align="center" width="300"}

:::

:::

## Relevance to Clustering {.smaller .crunch-title .crunch-figures .crunch-code .crunch-p .crunch-quarto-figure-center .crunch-ul}

* Let $\boldsymbol\mu_1 = (0.2, 0.8)^\top$, $\boldsymbol\mu_2 = (0.8, 0.2)^\top$, $\Sigma = \text{diag}(1/64)$, and $\mathbf{X} = (X_1, X_2)$.
* $X_1 \sim \boldsymbol{\mathcal{N}}_2(\boldsymbol\mu_1, \Sigma)$, $X_2 \sim \boldsymbol{\mathcal{N}}_2(\boldsymbol\mu_2, \Sigma)$

```{r}
#| label: two-gaussian-clusters
#| fig-align: center
#| warning: false
library(tidyverse)
library(ggforce)
library(MASS)
library(patchwork)
N <- 50
Mu1 <- c(0.2, 0.8)
Mu2 <- c(0.8, 0.2)
sigma <- 1/24
# Data for concentric circles
circle_df <- tribble(
  ~x0, ~y0, ~r, ~Cluster, ~whichR,
  Mu1[1], Mu1[2], sqrt(sigma), "C1", 1,
  Mu2[1], Mu2[2], sqrt(sigma), "C2", 1,
  Mu1[1], Mu1[2], 2 * sqrt(sigma), "C1", 2,
  Mu2[1], Mu2[2], 2 * sqrt(sigma), "C2", 2,
  Mu1[1], Mu1[2], 3 * sqrt(sigma), "C1", 3,
  Mu2[1], Mu2[2], 3 * sqrt(sigma), "C2", 3
)
#print(circle_df)
Sigma <- matrix(c(sigma,0,0,sigma), nrow=2)
#print(Sigma)
x1_df <- as_tibble(mvrnorm(N, Mu1, Sigma))
x1_df <- x1_df |> mutate(
  Cluster='C1'
)
x2_df <- as_tibble(mvrnorm(N, Mu2, Sigma))
x2_df <- x2_df |> mutate(
  Cluster='C2'
)
cluster_df <- bind_rows(x1_df, x2_df)
cluster_df <- cluster_df |> rename(
  x=V1, y=V2
)
known_plot <- ggplot(cluster_df) +
  geom_point(
    data = circle_df,
    aes(x=x0, y=y0)
  ) +
  geom_circle(
    data = circle_df,
    aes(x0=x0, y0=y0, r=r, fill=Cluster),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_point(
    data=cluster_df,
    aes(x=x, y=y, fill=Cluster),
    size = g_pointsize / 2,
    shape = 21
  ) +
  dsan_theme("full") +
  coord_fixed() +
  labs(
    x = "x",
    y = "y",
    title = "Data with Known Clusters"
  ) + 
  scale_fill_manual(values=c(cbPalette[2], cbPalette[1], cbPalette[3], cbPalette[4])) +
  scale_color_manual(values=c(cbPalette[1], cbPalette[2], cbPalette[3], cbPalette[4]))
unknown_plot <- ggplot(cluster_df) +
  geom_point(
    data=cluster_df,
    aes(x=x, y=y),
    size = g_pointsize / 2,
    #shape = 21
  ) +
  dsan_theme("full") +
  coord_fixed() +
  labs(
    x = "x",
    y = "y",
    title = "Same Data with Unknown Clusters"
  )
cluster_df |> write_csv("assets/cluster_data.csv")
known_plot + unknown_plot
```

## Clusters as Latent Variables

* Recall the Hidden Markov Model (one of many examples):

![](images/hmm.svg){fig-align="center"}

## Modeling the Latent Distribution {.smaller .crunch-title .crunch-ul .crunch-figures .math-75 .small-inline .crunch-quarto-figure-center .crunch-ul-left}

* This observed/latent distinction gives us a **modeling framework** for inferring "underlying" **distributions** from **data**!
* Let's begin with an overly-simple model: only **one** cluster (all data drawn from a **single** normal distribution)


::: columns
::: {.column width="50%"}

![](images/hmm_clusters_single_dist.svg){fig-align="center" width="75%"}

:::
::: {.column width="50%"}

* Probability that RV $X_i$ takes on value $\mathbf{v}$:

  $$
  \begin{align*}
  &\Pr(X_i = \mathbf{v} \mid \param{\boldsymbol\theta_\mathcal{D}}) =
  \varphi_2(\mathbf{v}; \param{\boldsymbol\mu}, \param{\mathbf{\Sigma}})
  \end{align*}
  $$

  where $\varphi_2(\mathbf{v}; \boldsymbol\mu, \mathbf{\Sigma})$ is pdf of $\boldsymbol{\mathcal{N}}_2(\boldsymbol\mu, \mathbf{\Sigma})$.

* Let $\mathbf{X} = (X_1, \ldots, X_N)$, $\mathbf{V} = (\mathbf{v}_1, \ldots, \mathbf{v}_N)$
* Probability that RV $\mathbf{X}$ takes on **values** $\mathbf{V}$:

$$
\begin{align*}
&\Pr(\mathbf{X} = \mathbf{V} \mid \param{\boldsymbol\theta_\mathcal{D}}) \\
&= \Pr(X_1 = \mathbf{v}_1 \mid \paramDist) \times \cdots \times \Pr(X_N = \mathbf{v}_N \mid \paramDist)
\end{align*}
$$

:::
:::

## So How Do We *Infer* Latent Vars From Data? {.smaller .title-12}

* If only we had some sort of method for estimating which values of our **unknown parameters** $\paramDist$ are *most likely* to produce our **observed data** $\mathbf{X}$ ![](images/thinking_transparent.png){width="42"}
* The diagram on the previous slide gave us an equation

  $$
  \begin{align*}
  \Pr(\mathbf{X} = \mathbf{V} \mid \param{\boldsymbol\theta_\mathcal{D}}) = \Pr(X_1 = \mathbf{v}_1 \mid \paramDist) \times \cdots \times \Pr(X_N = \mathbf{v}_N \mid \paramDist)
  \end{align*}
  $$

* And we know that, when we consider the **data** as **given** and view this probability as a function of the **parameters**, we write it as

  $$
  \begin{align*}
  \lik(\mathbf{X} = \mathbf{V} \mid \param{\boldsymbol\theta_\mathcal{D}}) = \lik(X_1 = \mathbf{v}_1 \mid \paramDist) \times \cdots \times \lik(X_N = \mathbf{v}_N \mid \paramDist)
  \end{align*}
  $$

* We want to find the *most likely* $\paramDist$, that is, $\boldsymbol\theta^*_\mathcal{D} = \argmax_{\paramDist}\mathcal{L}(\mathbf{X} = \mathbf{V} \mid \paramDist)$

* This value $\boldsymbol\theta^*_\mathcal{D}$ is called the **Maximum Likelihood Estimate** of $\paramDist$, and is easy to find using calculus tricks^[If you're in my DSAN5100 class, then you already know this! If not, [check out the MLE slides here<i class='bi bi-box-arrow-up-right ps-1'></i>](https://jjacobs.me/dsan5100-03/463a01339cf0f456ba54a1849df50d1a22c247e3/w09/slides.html#maximum-likelihood-estimation){target='_blank'} for more details]

## Handling Multiple Clusters {.smaller .crunch-title .crunch-ul .crunch-ul-left .crunch-figures .smaller-math .smaller-inline}

::: columns
::: {.column width="50%"}

![](images/hmm_clusters.svg){fig-align="center" width="85%"}

:::
::: {.column width="50%"}

* Probability $X_i$ takes on value $\mathbf{v}$:

  $$
  \begin{align*}
  &\Pr(X_i = \mathbf{v} \mid \param{C_i} = c_i; \; \param{\boldsymbol\theta_\mathcal{D}}) \\
  &= \begin{cases}
  \varphi_2(v; \param{\boldsymbol\mu_1}, \param{\mathbf{\Sigma}}) &\text{if }c_i = 1 \\
  \varphi_2(v; \param{\boldsymbol\mu_2}, \param{\mathbf{\Sigma}}) &\text{otherwise,}
  \end{cases}
  \end{align*}
  $$

  where $\varphi_2(v; \boldsymbol\mu, \mathbf{\Sigma})$ is pdf of $\boldsymbol{\mathcal{N}}_2(\boldsymbol\mu, \mathbf{\Sigma})$.

* Let $\mathbf{C} = (\underbrace{C_1}_{\text{RV}}, \ldots, C_N)$, $\mathbf{c} = (\underbrace{c_1}_{\mathclap{\text{scalar}}}, \ldots, c_N)$

:::
:::

* Probability that RV $\mathbf{X}$ takes on **values** $\mathbf{V}$:

$$
\begin{align*}
&\Pr(\mathbf{X} = \mathbf{V} \mid \param{\mathbf{C}} = \mathbf{c}; \; \param{\boldsymbol\theta_\mathcal{D}}) \\
&= \Pr(X_1 = \mathbf{v}_1 \mid \param{C_1} = c_1; \; \paramDist) \times \cdots \times \Pr(X_N = \mathbf{v}_N \mid \param{C_N} = c_N; \; \paramDist)
\end{align*}
$$

* *It's the same math as before!* Find $(\mathbf{C}^*, \boldsymbol\theta^*_\mathcal{D}) = \argmax_{\param{\mathbf{C}}, \, \paramDist}\mathcal{L}(\mathbf{X} = \mathbf{V} \mid \param{\mathbf{C}}; \; \param{\boldsymbol\theta_\mathcal{D}})$

## In Code! {.smaller .crunch-code}

```{python}
#| label: gaussian-mixture-mle
#| output: false
#| code-fold: show
import pandas as pd
import numpy as np
import sklearn
sklearn.set_config(display = 'text')
from sklearn.mixture import GaussianMixture
cluster_df = pd.read_csv("assets/cluster_data.csv")
X = cluster_df[['x','y']].values
estimator = GaussianMixture(n_components=2, covariance_type="spherical", max_iter=20, random_state=5000);
estimator.fit(X);
y_pred = estimator.predict(X)
# Gather everything into a .csv
# The estimated means
mean_df = pd.DataFrame(estimator.means_).transpose()
mean_df.columns = ['x','y']
mean_df['Estimated Centroid'] = ['mu1','mu2']
mean_df['Estimated Cluster'] = ['E1', 'E2']
mean_df['Estimated Cluster Mapped'] = ['C1', 'C2']
cov_array = estimator.covariances_
sigma1 = np.sqrt(cov_array[0])
sigma2 = np.sqrt(cov_array[1])
mean_df['Estimated Sigma'] = [sigma1,sigma2]
mean_df['Estimated 2Sigma'] = [2*sigma1, 2*sigma2]
mean_df['Estimated 3Sigma'] = [3*sigma1, 3*sigma2]
mean_df.to_csv("assets/sklearn_mean_df.csv", index=False)
# The estimated covariance matrix
cov_df = pd.DataFrame({'c1': [1, cov_array[0]], 'c2': [cov_array[1], 1]})
cov_df.to_csv("assets/sklearn_cov_df.csv", index=False)
# And the predicted clusters
pred_df = pd.DataFrame({'Estimated Cluster': y_pred})
pred_df['Estimated Cluster'] = pred_df['Estimated Cluster'].apply(lambda x: 'E1' if x == 0 else 'E2')
pred_df['Estimated Cluster Mapped'] = pred_df['Estimated Cluster'].apply(lambda x: 'C1' if x == 'E1' else 'C2')
pred_df.to_csv("assets/sklearn_pred_df.csv", index=False)
```

## Distribution Estimation Results {.smaller}

```{r}
#| label: compare-clusters
# Data for concentric circles
mean_df <- read_csv("assets/sklearn_mean_df.csv")
#print(mean_df)
cov_df <- read_csv("assets/sklearn_cov_df.csv")
#print(cov_df)
pred_df <- read_csv("assets/sklearn_pred_df.csv")
# Merge the predicted clusters into cluster_df
cluster_df <- bind_cols(cluster_df, pred_df)
cluster_df <- cluster_df |> mutate(
  Correct = as.numeric(`Estimated Cluster Mapped` == Cluster)
)
correct_df <- cluster_df |> filter(Correct == 1)
incorrect_df <- cluster_df |> filter(Correct == 0)
#print(cluster_df)
# Keep only the 3*Sigma circle
# compare_circle_df <- circle_df |> filter(
#   whichR == 3
# )
#print(mean_df)
#print(compare_circle_df)
compare_plot <- ggplot(cluster_df) +
  geom_point(
    data = mean_df,
    aes(
      x=x, y=y,
      #fill=`Estimated Cluster`
    )
  ) +
  geom_point(
    data = circle_df,
    aes(
      x=x0, y=y0,
      #fill=Cluster
    )
  ) +
  geom_circle(
    data = mean_df,
    aes(
      x0=x, y0=y,
      r=`Estimated Sigma`,
      color=`Estimated Cluster`
      #fill=`Estimated Centroid`
    ),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_circle(
    data = mean_df,
    aes(
      x0=x, y0=y,
      r=`Estimated 2Sigma`,
      color=`Estimated Cluster`
      #fill=`Estimated Centroid`
    ),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_circle(
    data = mean_df,
    aes(
      x0=x, y0=y,
      r=`Estimated 3Sigma`,
      color=`Estimated Cluster`
      #fill=`Estimated Cluster`
    ),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_circle(
    data = circle_df,
    aes(
      x0=x0, y0=y0,
      r=r,
      color=Cluster
      #fill=Cluster
    ),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  dsan_theme("full") +
  coord_fixed() +
  labs(
    x = "x",
    y = "y",
    title = "DGP vs. Estimated Clusters",
    fill = "MLE",
    color = "MLE",
    shape = "Correct?"
  ) +
  scale_shape_manual(values = c(4,21)) +
  scale_color_manual(values=c(cbPalette[2], cbPalette[1], cbPalette[3], cbPalette[4]))
compare_plot
```

## Clustering Results {.smaller .crunch-code .crunch-figures .crunch-p}

```{r}
#| label: plot-gaussian-mixture-mle
est_plot <- ggplot(cluster_df) +
  # geom_point(
  #   data = mean_df,
  #   aes(x=x, y=y)
  # ) +
  geom_circle(
    data = mean_df,
    aes(
      x0=x, y0=y,
      r=`Estimated Sigma`,
      fill=`Estimated Cluster Mapped`
    ),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_circle(
    data = mean_df,
    aes(x0=x, y0=y, r=`Estimated 2Sigma`, fill=`Estimated Cluster Mapped`),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_circle(
    data = mean_df,
    aes(x0=x, y0=y, r=`Estimated 3Sigma`, fill=`Estimated Cluster Mapped`),
    linewidth = g_linewidth,
    alpha = 0.25
  ) +
  geom_point(
    data=cluster_df,
    aes(
      x=x, y=y,
      fill=factor(`Estimated Cluster Mapped`),
      shape=factor(Correct)
    ),
    size = g_pointsize / 2,
    stroke = 0.75
  ) +
  geom_point(
    data=incorrect_df,
    aes(x=x, y=y),
    color='black',
    shape = 4,
    size = g_pointsize / 2,
    stroke = 2
  ) +
  geom_point(
    data=incorrect_df,
    aes(x=x, y=y),
    color=cbPalette[4],
    shape = 4,
    size = g_pointsize / 2,
    stroke = 0.8
  ) +
  dsan_theme("full") +
  coord_fixed() +
  labs(
    x = "x",
    y = "y",
    title = "Data with Estimated Clusters",
    fill = "MLE",
    color = "MLE",
    shape = "Correct?"
  ) +
  scale_shape_manual(values = c(4,21)) +
  scale_fill_manual(values=c(cbPalette[3], cbPalette[4])) +
  scale_color_manual(values=c(cbPalette[3], cbPalette[4]))
known_plot + est_plot
```

## Clustering In General

* What makes a **"good"** cluster?
* In previous example, "good" meant that it **recovered our DGP**
* In an **EDA** context, we may not have a well-formulated DGP in mind...

## Unsupervised Clustering: Heuristics {.smaller .title-12}

| Heuristic 1 | Heuristic 2 |
| - | - |
| Points in **same** cluster should be **similar** | Points in **different** clusters should be **dissimilar** |

*You could take these two heuristics, formalize them, and derive the rest of lecture* 😉

![Image from [*Introduction to Machine Learning*](http://courses.washington.edu/css490/2012.Winter/lecture_slides/10_clustering_basics_1.pdf){target='_blank'}, University of Washington (2012)](images/clustering-heuristics.jpeg){fig-align="center"}

## Hierarchical Clustering {.crunch-title .crunch-ul .crunch-quarto-figure .crunch-figures .crunch-images .crunch-lists}

* If our task is to **partition**, we're done!
* If our task is to infer a **hierarchy**, we can also think about a spectrum of **coarse-grained** to **fine-grained** clusterings:

::: {layout="[1,1]" layout-valign="center"}

![](images/hierarchical-clustering.png){fig-align="center"}

![](images/hierarchical-flat-v2.svg){fig-align="center"}

:::

## Hierarchical Clustering $\rightarrow$ Multilevel Modeling {.smaller .title-11 .crunch-title .crunch-quarto-figure .crunch-figures .crunch-fig-top .crunch-images .crunch-p}

::: {layout="[1,1]" layout-valign="center"}

::: {#multilevel-text}

* **Extremely powerful modeling tool** for statistical inference!
  * Learning about **eagles** $\implies$ learning about **birds**
  * Surveying **DSAN 5000 students** $\implies$ **Georgetown students**
  * We can **pool** information from US **states** into **national** estimates
* And, when we compute **averages**, we can take into account our **relative certainty/uncertainty** about units at **each level** (read more [here](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html){target='_blank'})

<div style="float: right;">
*Image from @gelman_red_2009*
</div>

:::

![](images/gelman-states.jpeg){fig-align="center" width="80%"}

:::

## In Practice {.smaller .crunch-title .crunch-images .crunch-fig-top .crunch-quarto-figure .crunch-figcaption}

::: {layout="[5,5]" layout-valign="center"}

![*Above*: Data from Soviet archives; *Above Right*: US Military archives; *Below Right*: NATO archives](images/1960-1980_Katsakioris_RegionalTotals.PNG)

::: {#right-col}

![](images/1970-1978_regional-totals-crop.jpeg)

![](images/departures.jpeg){width="100%"}

:::
:::

# K-Means Clustering {data-stack-name="K-Means Clustering"}

## What is K-Means Clustering? {.crunch-title .crunch-ul .crunch-images .crunch-quarto-layout-panel .crunch-lists}

* **Operationalizes** our two heuristics by simultaneously:
  * **Maximizing** within-cluster similarity
  * **Minimizing** between-cluster similarity

::: {#fig-voronoi}
::: {layout-ncol=3}

![](images/voronoi_k2.png)

![](images/voronoi_k3.png)

![](images/voronoi_k5.png)

:::

Voronoi diagrams for $K = 2$ (left), $K = 3$ (center), and $K = 5$ (right) [[Source](https://antoinebrl.github.io/blog/kmeans/){target='_blank'}]
:::

## K-Means Clustering Algorithm {.smaller .crunch-title .crunch-callout .ol-95 .fix-callout-font .crunch-lists .crunch-ol .crunch-p}

::: columns
::: {.column width="50%"}

<center>
**What we're given:**
</center>

* **Data** [$\mathbf{X} = (X_1 = \mathbf{x}_1, \ldots, X_N = \mathbf{x}_N)$]{.small-inline}
* **Distance metric** $d(\mathbf{v}_1, \mathbf{v}_2)$
* **Hyperparameter value** for $K$ (⁉️)

:::
::: {.column width="50%"}

<center>
**Our goal:**
</center>

* Assign each **point** [$\mathbf{x}_i$]{.small-inline} to a **cluster** [$C_i \in \{1, \ldots, K\}$]{.small-inline} (so [$S_j = \{\mathbf{x}_i \mid C_i = j\}$]{.small-inline} is the set of points assigned to cluster [$j$]{.small-inline})

:::
:::

::: {.callout-note .full-width-callout icon="false" title="<i class='bi bi-info-circle pe-1 pt-1'></i> K-Means Clustering Algorithm"}

1. Initialize $K$ random **centroids** $\boldsymbol\mu_1, \ldots, \boldsymbol\mu_K$
2. **(Re-)Compute distance** $d(\mathbf{x}_i, \boldsymbol\mu_j)$ from **each point** $\mathbf{x}_i$ to **each centroid** $\boldsymbol\mu_j$
3. **(Re-)Assign points** to class of **nearest centroid**: $C_i = \argmin_j d(\mathbf{x}_i, \boldsymbol\mu_j)$
4. **(Re-)compute centroids** as mean of points in each cluster:

    $$
    \mu_j^\text{new} \leftarrow \frac{1}{|S_j|}\sum_{\mathbf{x}_i \in S_j}\mathbf{x}_i
    $$

5. **Repeat Steps 2-4** until convergence

:::

## In Pictures {.smaller .crunch-title}

```{=html}
<script src="assets/d3.v3.min.js"></script>
<script src="assets/choose.js"></script>
<script src="assets/generate.js"></script>
<script src="assets/config.js"></script>
<script src="assets/kmeans.js"></script>
<script src="assets/main.js"></script>
<div id="svg_area" style="margin-top: 0px; font-size: 0.75em;"></div>
<div id="button_container" style="display: flex; flex-direction: row;">
<div id="button_area" style="margin-top: 4px; flex-grow: 1;"></div><div style="float: right; margin-top: 4px;">Adapted from <a href='https://www.naftaliharris.com/blog/visualizing-k-means-clustering/' target='_blank'>this blog post</a></span>
</div>
<script>restart();</script>
```

## In Code + Pictures {.smaller .crunch-code .crunch-title}

```{python}
#| label: sklearn-kmc
#| fig-cap: Adapted from [Scikit-learn User Guide](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py){target='_blank'}
import pandas as pd
import matplotlib.colors
from sklearn.cluster import KMeans
cluster_df = pd.read_csv("assets/cluster_data.csv")
X = cluster_df[['x','y']].values
kmc_model = KMeans(
  n_clusters=2,
  init='k-means++',
  verbose=0,
  random_state=5000,
  copy_x=True,
  algorithm='lloyd'
);
kmc_model.fit(X);
y_pred_vals = kmc_model.predict(X)
y_pred_df = pd.DataFrame({'y_pred': y_pred_vals})
y_pred_df.to_csv("assets/kmc_preds.csv", index=False)
kmc_centroid_df = pd.DataFrame(kmc_model.cluster_centers_.transpose(), columns=['x','y'])
#disp(kmc_centroid_df)
kmc_centroid_df.to_csv("assets/kmc_centroids.csv", index=False)

import matplotlib.pyplot as plt
# Step size of the mesh. Decrease to increase the quality of the VQ.
h = 0.01  # point in the mesh [x_min, x_max]x[y_min, y_max].

# Plot the decision boundary. For that, we will assign a color to each
bpad = 0.05
x_min, x_max = X[:, 0].min() - bpad, X[:, 0].max() + bpad
y_min, y_max = X[:, 1].min() - bpad, X[:, 1].max() + bpad
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))

# Obtain labels for each point in mesh. Use last trained model.
Z = kmc_model.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
#custom_cmap = matplotlib.colors.LinearSegmentedColormap.from_list("", ["red","violet","blue"])
#custom_cmap = matplotlib.colors.ListedColormap(['white', 'red'])
#custom_cmap = matplotlib.colors.ListedColormap(cb_palette).reversed()
custom_cmap = matplotlib.colors.ListedColormap([cb_palette[0], cb_palette[2], cb_palette[1]])
Z = Z.reshape(xx.shape)
plt.figure(1)
plt.clf()
plt.imshow(
    Z,
    interpolation="nearest",
    extent=(xx.min(), xx.max(), yy.min(), yy.max()),
    #cmap=plt.cm.Paired,
    cmap=custom_cmap,
    aspect="auto",
    origin="lower",
)

# And plot the points
plt.plot(X[:, 0], X[:, 1], "o", markersize=6, color='white', markerfacecolor='black', alpha=0.75)
# Plot the centroids as a white X
centroids = kmc_model.cluster_centers_
plt.scatter(
    centroids[:, 0],
    centroids[:, 1],
    marker="*",
    s=250,
    linewidths=1.5,
    color='white',
    facecolor='black',
    zorder=10,
)
# Plot gaussian means as... smth else
plt.scatter(
    [0.2,0.8],
    [0.8,0.2],
    marker="*",
    s=250,
    linewidths=1.5,
    color=cb_palette[3],
    facecolor='black',
    zorder=9,
)
plt.title(
    "K-means clustering on the Gaussian mixture data"
)
plt.legend([
  'Original Data',
  'K-Means Centroids',
  'True Centroids (DGP)'
])
#plt.xlim(x_min, x_max);
#plt.ylim(y_min, y_max);
#plt.xticks(());
#plt.yticks(());
plt.show()
```

# Fancier Clustering and Hyperparameter Tuning Methods {data-stack-name="Fancier Methods"}

## DBSCAN {.smaller .crunch-code .crunch-title}

```{python}
#| label: dbscan-grid
#| fig-cap: Adapted from [Scikit-learn User Guide](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html){target='_blank'}
import time
import warnings
from itertools import cycle, islice

import matplotlib.pyplot as plt
import numpy as np

from sklearn import cluster, datasets, mixture
from sklearn.neighbors import kneighbors_graph
from sklearn.preprocessing import StandardScaler

# ============
# Generate datasets. We choose the size big enough to see the scalability
# of the algorithms, but not too big to avoid too long running times
# ============
n_samples = 500
seed = 30
noisy_circles = datasets.make_circles(
    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed
)
noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)
blobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)
rng = np.random.RandomState(seed)
no_structure = rng.rand(n_samples, 2), None

# Anisotropicly distributed data
random_state = 170
X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
transformation = [[0.6, -0.6], [-0.4, 0.8]]
X_aniso = np.dot(X, transformation)
aniso = (X_aniso, y)

# blobs with varied variances
varied = datasets.make_blobs(
    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state
)

# ============
# Set up cluster parameters
# ============
plt.figure(figsize=(9 * 2 + 3, 13))
plt.subplots_adjust(
    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01
)

plot_num = 1

default_base = {
    "quantile": 0.3,
    "eps": 0.3,
    "damping": 0.9,
    "preference": -200,
    "n_neighbors": 3,
    "n_clusters": 3,
    "min_samples": 7,
    "xi": 0.05,
    "min_cluster_size": 0.1,
    "allow_single_cluster": True,
    "hdbscan_min_cluster_size": 15,
    "hdbscan_min_samples": 3,
    "random_state": 42,
}

datasets = [
    (
        noisy_circles,
        {
            "damping": 0.77,
            "preference": -240,
            "quantile": 0.2,
            "n_clusters": 2,
            "min_samples": 7,
            "xi": 0.08,
        },
    ),
    (
        noisy_moons,
        {
            "damping": 0.75,
            "preference": -220,
            "n_clusters": 2,
            "min_samples": 7,
            "xi": 0.1,
        },
    ),
    # (
    #     varied,
    #     {
    #         "eps": 0.18,
    #         "n_neighbors": 2,
    #         "min_samples": 7,
    #         "xi": 0.01,
    #         "min_cluster_size": 0.2,
    #     },
    # ),
    (
        aniso,
        {
            "eps": 0.15,
            "n_neighbors": 2,
            "min_samples": 7,
            "xi": 0.1,
            "min_cluster_size": 0.2,
        },
    ),
    (blobs, {"min_samples": 7, "xi": 0.1, "min_cluster_size": 0.2}),
    (no_structure, {}),
]

for i_dataset, (dataset, algo_params) in enumerate(datasets):
    # update parameters with dataset-specific values
    params = default_base.copy()
    params.update(algo_params)

    X, y = dataset

    # normalize dataset for easier parameter selection
    X = StandardScaler().fit_transform(X)

    # estimate bandwidth for mean shift
    bandwidth = cluster.estimate_bandwidth(X, quantile=params["quantile"])

    # connectivity matrix for structured Ward
    connectivity = kneighbors_graph(
        X, n_neighbors=params["n_neighbors"], include_self=False
    )
    # make connectivity symmetric
    connectivity = 0.5 * (connectivity + connectivity.T)

    # ============
    # Create cluster objects
    # ============
    two_means = cluster.MiniBatchKMeans(
        n_clusters=params["n_clusters"],
        n_init="auto",
        random_state=params["random_state"],
    )
    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)
    # spectral = cluster.SpectralClustering(
    #     n_clusters=params["n_clusters"],
    #     eigen_solver="arpack",
    #     affinity="nearest_neighbors",
    #     random_state=params["random_state"],
    # )
    dbscan = cluster.DBSCAN(eps=params["eps"])
    average_linkage = cluster.AgglomerativeClustering(
        linkage="average",
        metric="cityblock",
        n_clusters=params["n_clusters"],
        connectivity=connectivity,
    )
    birch = cluster.Birch(n_clusters=params["n_clusters"])
    gmm = mixture.GaussianMixture(
        n_components=params["n_clusters"],
        covariance_type="full",
        random_state=params["random_state"],
    )

    clustering_algorithms = (
        ("MiniBatch\nKMeans", two_means),
        #("Affinity\nPropagation", affinity_propagation),
        ("MeanShift", ms),
        #("Spectral\nClustering", spectral),
        #("Ward", ward),
        ("Agglomerative\nClustering", average_linkage),
        ("DBSCAN", dbscan),
        #("HDBSCAN", hdbscan),
        #("OPTICS", optics),
        ("BIRCH", birch),
        ("Gaussian\nMixture", gmm),
    )

    for name, algorithm in clustering_algorithms:
        t0 = time.time()

        # catch warnings related to kneighbors_graph
        with warnings.catch_warnings():
            warnings.filterwarnings(
                "ignore",
                message="the number of connected components of the "
                + "connectivity matrix is [0-9]{1,2}"
                + " > 1. Completing it to avoid stopping the tree early.",
                category=UserWarning,
            )
            warnings.filterwarnings(
                "ignore",
                message="Graph is not fully connected, spectral embedding"
                + " may not work as expected.",
                category=UserWarning,
            )
            algorithm.fit(X)

        t1 = time.time()
        if hasattr(algorithm, "labels_"):
            y_pred = algorithm.labels_.astype(int)
        else:
            y_pred = algorithm.predict(X)

        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)
        if i_dataset == 0:
            plt.title(name, size=18)

        colors = np.array(
            list(
                islice(
                    cycle(
                        [
                            "#377eb8",
                            "#ff7f00",
                            "#4daf4a",
                            "#f781bf",
                            "#a65628",
                            "#984ea3",
                            "#999999",
                            "#e41a1c",
                            "#dede00",
                        ]
                    ),
                    int(max(y_pred) + 1),
                )
            )
        )
        # add black color for outliers (if any)
        colors = np.append(colors, ["#000000"])
        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])

        plt.xlim(-2.5, 2.5)
        plt.ylim(-2.5, 2.5)
        plt.xticks(())
        plt.yticks(())
        plt.text(
            0.99,
            0.01,
            ("%.2fs" % (t1 - t0)).lstrip("0"),
            transform=plt.gca().transAxes,
            size=15,
            horizontalalignment="right",
        )
        plot_num += 1

plt.show()
```

## Hyperparameter Tuning: More Heuristics {.smaller .crunch-title}

::: {layout-ncol=2 layout-valign="center"}

::: {#elbow-text}
* **Inertia**: A measure of how "well-clustered" a dataset is
* Sum of squared distances of samples to their closest cluster center
* A good model is one with low inertia **and** low $K$ (tradeoff, akin to bias-variance)
* **Elbow Method**: Find the $K$ value after which **decrease in inertia** begins to slow &rarr;

:::

![Figure from [this blog post](https://ankitajhumu.medium.com/selecting-number-of-clusters-in-k-mean-clustering-d60a1f85d65b){target='_blank'} by [Ankita Banerji](https://ankitajhumu.medium.com/){target='_blank'}](images/elbow-method.png){fig-align="center"}

:::

## References

::: {#refs}
:::
