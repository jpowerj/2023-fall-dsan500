---
title: "Week 6: Exploratory Data Analysis (EDA)"
date: "Tuesday, September 26, 2023"
date-format: full
#date: last-modified
#date-format: "dddd MMM D, YYYY, HH:mm:ss"
lecnum: 6
categories:
  - "Class Sessions"
bibliography: "../_DSAN5000.bib"
format:
  revealjs:
    cache: false
    footer: "DSAN 5000-<span class='sec-num'>02</span> Week 6: Exploratory Data Analysis"
  html:
    output-file: index.html
    html-math-method: mathjax
metadata-files: 
  - "../_slide-meta.yml"
---


::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Schedule {.smaller .small-title .crunch-title .crunch-callout data-name="Schedule"}

::: {.hidden}

```{r}
#| label: r-source-globals
source("../_globals.r")
```

```{python}
#| label: python-globals
cb_palette = ["#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7"]

from IPython.display import Markdown
def disp(df, floatfmt='g', include_index=True):
  return Markdown(
    df.to_markdown(
      floatfmt=floatfmt,
      index=include_index
    )
  )

def summary_to_df(summary_obj, corner_col = ''):
    reg_df = pd.DataFrame(summary_obj.tables[1].data)
    reg_df.columns = reg_df.iloc[0]
    reg_df = reg_df.iloc[1:].copy()
    # Save index col
    index_col = reg_df['']
    # Drop for now, so it's all numeric
    reg_df.drop(columns=[''], inplace=True)
    reg_df = reg_df.apply(pd.to_numeric)
    my_round = lambda x: round(x, 2)
    reg_df = reg_df.apply(my_round)
    numeric_cols = reg_df.columns
    # Add index col back in
    reg_df.insert(loc=0, column=corner_col, value=index_col)
    # Sigh. Have to escape | characters?
    reg_df.columns = [c.replace("|","\|") for c in reg_df.columns]
    return reg_df
```

{{< include ../_tex-globals.qmd >}}

:::

Today's Planned Schedule (Section <span class='sec-num'>02</span>):

{{< include ../_components/sched-w06.qmd >}}

# Week 05 Recap {data-stack-name="Recap"}

* NLP
* Tidyverse
* Merging, Reshaping Data

## NLP Recap {.smaller .small-title .crunch-title .crunch-figures}

{{< include ../_components/doc-term-matrix.qmd >}}

## Your NLP Toolbox {.smaller .crunch-title}

* Processes like **lowercasing** and **stemming** allowed the computer to recognize that `text` and `texts` should be counted together in this context, since they refer to the same semantic concept.
* As we learn NLP, we'll develop a "toolbox" of ideas, algorithms, and tasks allowing us to **quantify, clean, and analyzing text data**, where each tool will help us at some level/stage of this analysis:
  * Gathering texts
  * Preprocessing
  * Learning (e.g., estimating parameters for a model) about the texts
  * Applying what we learned to **downstream tasks** we'd like to solve

## The Items In Our Toolbox {.smaller .crunch-title .crunch-ul .crunch-ul-left}

::: {layout="[20,54,26]" layout-valign="top"}

::: {#gathering-text}

<div style="border: 1px solid black !important; padding: 8px !important;">
<center>
**Corpus Construction**
</center>

&bull; **Corpus**: The collection of documents you're hoping to analyze

&bull; Books, articles, posts, emails, tweets, etc.
</div>

:::
::: {#nlp}

<center>
**Corpus/Document Level NLP**
</center>

&bull; **Vocabulary**: The collection of **unique tokens** across **all documents** in your corpus

&bull; **Segmentation**: Breaking a document into parts (paragraphs and/or sentences)

<center>
&darr;<br>**Sentence/Word Level NLP**
</center>

&bull; **Tokenization**: Break sentence into **tokens**

&bull; **Stopword Removal**: Removing non-**semantic** (syntactic) tokens like "the", "and"

&bull; **Stemming**: Na√Øvely (but quickly) "chopping off" ends of tokens (e.g., plural &rarr; singular)

&bull; **Lemmatization**: Algorithmically map tokens to linguistic roots (slower than stemming)



:::
::: {#downstream}

<div style="border: 1px solid black !important; padding: 8px !important; margin-bottom: 3px;" class="crunch-p">
<center>
**Vectorization**
</center>

Transform textual representation into numeric representation, like the **DTM**
</div>

<center>
&darr;
</center>

<div class="crunch-p" style="border: 1px solid black !important; padding: 8px !important; margin-top: 3px !important;">
<center>
**Downstream Tasks**
</center>

&bull; **Text classification**

&bull; **Named entity recognition**

&bull; **Sentiment analysis**

</div>

:::

:::

## Tidyverse {.smaller}

* Think of data science tasks as involving **pipelines**:

```{dot}
//| fig-height: 1.5
//| echo: false
digraph G {
  rankdir=LR;
  node[label="Raw Data"] raw;
  subgraph cluster_00 {
    label="Data-Processing Pipeline 1"
    node[label="Transformation A\n(select(), filter())"] tr1;
    node[label="Transformation B\n(mutate(), summarize())"] tr2;
    tr1 -> tr2;
  }
  raw -> tr1;
  node [label="Visualization"] viz;
  tr2 -> viz;
}
```

```{dot}
//| fig-height: 1.5
//| echo: false
digraph G {
  rankdir=LR;
  node[label="Raw Data"] raw;
  subgraph cluster_00 {
    label="Data-Processing Pipeline 2"
    node[label="Transformation C\n(select(), filter())"] tr1;
    node[label="Transformation D\n(mutate(), summarize())"] tr2;
    tr1 -> tr2;
  }
  raw -> tr1;
  node [label="      Result     "] viz;
  tr2 -> viz;
}
```

* Tidyverse lets you **pipe output** from one transformation as the **input** to another:

```r
raw_data |> select() |> mutate() |> visualize()
raw_data |> filter() |> summarize() |> check_result()
```

## *Select*ing Columns {.smaller}

`select()` lets you keep only the **columns** you care about in your current analysis:

::: {layout-ncol=2}

```{r}
#| label: tidyverse-select-load-data
library(tidyverse)
table1
```

```{r}
#| label: tidyverse-select
#| code-fold: show
table1 |> select(country, year, population)
```

:::

## *Filter*ing Rows {.smaller}

`filter()` lets you keep only the **rows** you care about in your current analysis:

::: {layout-ncol=2}

```{r}
#| label: tidyverse-filter-year
#| code-fold: show
table1 |> filter(year == 2000)
```

```{r}
#| label: tidyverse-filter-country
#| code-fold: show
table1 |> filter(country == "Afghanistan")
```

:::

## Merging Data {.smaller .crunch-title}

* The task: Analyze relationship between population and GDP (in 2000)
* The data: One dataset on population in 2000, another on GDP in 2000
* Let's get the data **ready for merging** using R

::: columns
::: {.column width="40%"}

```{r}
#| label: tidyverse-filter-select
#| code-fold: show
df <- table1 |>
  select(country, year, population) |>
  filter(year == 2000)
df |> write_csv("assets/pop_2000.csv")
df
```

:::

::: {.column width="60%"}

```{r}
#| label: load-gdp-data
#| code-fold: show
gdp_df <- read_csv("https://gist.githubusercontent.com/jpowerj/c83e87f61c166dea8ba7e4453f08a404/raw/29b03e6320bc3ffc9f528c2ac497a21f2d801c00/gdp_2000_2010.csv")
gdp_df |> head(5)
```

:::

:::

## Selecting/Filtering in Action {.smaller}

```{r}
#| label: clean-gdp-data
#| code-fold: show
gdp_2000_df <- gdp_df |>
  select(`Country Name`,Year,Value) |>
  filter(Year == "2000") |>
  rename(country=`Country Name`, year=`Year`, gdp=`Value`)
gdp_2000_df |> write_csv("assets/gdp_2000.csv")
gdp_2000_df |> head()
```

## Recommended Language: Python {.smaller .crunch-title .crunch-ul}

Pandas provides an easy-to-use `df.merge(other_df)`!

::: columns
::: {.column width="50%"}

```{python}
#| label: py-load-pop-data
#| echo: false
#| output: false
import pandas as pd
from IPython.display import Markdown
pop_df = pd.read_csv("assets/pop_2000.csv")
```

```{python}
#| label: py-load-gdp-data
#| echo: false
gdp_df = pd.read_csv("assets/gdp_2000.csv")
```

<center>
**Left Join**
</center>

```{python}
#| label: py-merge
#| code-fold: show
merged_df = pop_df.merge(gdp_df,
  on='country', how='left', indicator=True
)
Markdown(merged_df.to_markdown())
```

:::
::: {.column width="50%"}

<center>
**Inner join** (&asymp; Intersection ($\cap$))
</center>

```{python}
#| label: py-merge-right
#| code-fold: show
merged_df = pop_df.merge(gdp_df,
  on='country', how='inner', indicator=True
)
Markdown(merged_df.to_markdown())
```

:::
:::

## Reshaping Data {.smaller}

Sometimes you can't merge because one of the datasets looks like the table on the left, but we want it to look like the table on the right

::: columns
::: {.column width="55%"}

In data-cleaning jargon, this dataset is **long** (more than one row per observation)

```{r}
#| label: r-long-data
table2 |> write_csv("assets/long_data.csv")
table2 |> head()
```

:::
::: {.column width="45%"}

In data-cleaning jargon, this dataset is **wide** (one row per obs; usually **tidy**)

```{r}
#| label: r-wide-data
table1 |> write_csv("assets/wide_data.csv")
table1 |> head()
```

:::
:::

## Reshaping Long-to-Wide in Python: `pd.pivot()` {.smaller}

::: columns
::: {.column width="45%"}

```{python}
#| label: py-load-long-data
#| echo: false
#| output: false
long_df = pd.read_csv("assets/long_data.csv")
```

Create unique ID for **wide** version:

```{python}
#| label: py-create-wide-id
#| code-fold: show
long_df['id'] = long_df['country'] + '_' + long_df['year'].apply(str)
# Reorder the columns, so it shows the id first
long_df = long_df[['id','country','year','type','count']]
disp(long_df.head(6))
```

:::
::: {.column width="55%"}

```{python}
#| label: py-long-to-wide
#| code-fold: show
reshaped_df = pd.pivot(long_df,
  index='id',
  columns='type',
  values='count'
)
disp(reshaped_df)
```

:::
:::

## The Other Direction (Wide-to-Long): `pd.melt()` {.smaller}

::: columns
::: {.column width="40%"}

```{python}
#| label: py-read-wide
#| code-fold: show
wide_df = pd.read_csv("assets/wide_data.csv")
disp(wide_df)
```

:::
::: {.column width="60%"}

```{python}
#| label: py-wide-to-long
#| code-fold: show
long_df = pd.melt(wide_df,
  id_vars=['country','year'],
  value_vars=['cases','population']
)
disp(long_df.head(6))
```

:::
:::

## Wide-to-Long in R: `gather()` {.smaller}

::: columns
::: {.column width="40%"}

```{r}
#| label: r-display-wide-for-reshape
#| code-fold: show
table1
```

:::
::: {.column width="60%"}

```{r}
#| label: r-reshape
#| code-fold: show
long_df <- gather(table1,
  key = "variable",
  value = cases,
  -c(country, year)
)
long_df |> head()
```

:::
:::

# Quiz Time!

* [Quiz 3.1 Link <i class='bi bi-box-arrow-up-right ps-1'></i>](https://georgetown.instructure.com/courses/173310/quizzes/201583){target="_blank"}

# Introduction to EDA {data-stack-name="EDA"}

## Exploratory Data Analysis (EDA)

* In contrast to **confirmatory** data analysis

```{dot}
//| echo: false
//| fig-height: 4
digraph G {
  rankdir=LR;
  nodedir=LR;
  //nodesep=0.5;
  ranksep=0.5;
  //overlap=false;
  //forcelabels=true;

  subgraph cluster_clean {
    margin=28
    label=<
      <table border="0">
        <tr><td><font point-size="24">Clean</font></td></tr>
      </table>
    >

    subgraph cluster_import {
      label="&nbsp;"
      penwidth=0
      margin=0
      node[label=<
        <table border="0">
          <tr><td><font point-size="20">Import</font></td></tr>
        </table>
      >] Import;
      node[label=<
        <table border="0">
          <tr><td>R: <font face="Courier New">read_csv()</font></td></tr>
          <tr><td>Python: <font face="Courier New">pd.read_csv()</font></td></tr>
        </table>
      >,penwidth=0] importLibs;
    }

    subgraph cluster_tidy {
      label="&nbsp;"
      penwidth=0
      margin=0
      node[label=<
        <table border="0">
          <tr><td><font point-size="20">Tidy</font></td></tr>
        </table>
      >] Tidy;
      tidyLibraries[label=<
        <table border="0">
          <tr><td>R: <font face="Courier New">tidyverse</font></td></tr>
          <tr><td>Python: <font face="Courier New">Pandas</font></td></tr>
        </table>
      >,penwidth=0]
    }

    Import -> Tidy
  }
  
	
  subgraph cluster_understand {
    label = <
      <table border="0">
        <tr><td><font point-size="24">Understand</font></td></tr>
      </table>
    >
    margin=10
    subgraph cluster_expl {
      label = "Exploratory";
      margin = 18;
      labeljust="center";
      V[label=<
        <table border="0">
          <tr><td><font point-size="20">Visualize</font></td></tr>
        </table>
      >]
      vizLibraries[label=<
        <table border="0">
          <tr><td>R: <font face="Courier New">ggplot2</font></td></tr>
          <tr><td>Python: <font face="Courier New">seaborn</font></td></tr>
        </table>
      >,penwidth=0]
    }
    subgraph cluster_conf {
      node [label=<
        <table border="0">
          <tr><td><font point-size="20">Model</font></td></tr>
        </table>
      >] Model;
      label = "Confirmatory";
      margin = 18;
      confLibraries[label=<
        <table border="0">
          <tr><td>R: <font face="Courier New">e1071</font></td></tr>
          <tr><td>Python: <font face="Courier New">scikit-learn</font></td></tr>
        </table>
      >,penwidth=0]
    }
  }
  node [label=<
    <table border="0">
      <tr><td><font point-size="20">Communicate</font></td></tr>
    </table>
  >] Communicate;
  
  Tidy -> V;
  V -> Model;
  Model -> V;
  Model -> Communicate;
}
```


## Exploratory Data Analysis (EDA)

* So you have reasonably clean data... now what?

![(Image source: [*Oldies but Goldies: Statistical Graphics Books*](https://www.sumsar.net/blog/2014/03/oldies-but-goldies-statistical-graphics-books/){target="_blank"})](images/eda_books.jpeg)

## What is EDA? {.crunch-title .crunch-ul .small-footnotes}

From <a href="https://www.ibm.com/topics/exploratory-data-analysis" target="_blank">IBM</a>:

* Look at data **before making any assumptions**[^next-slide]
* Screen data and identify obvious **errors**
* Better understand **patterns** within the data
* Detect **outliers** or anomalous events
* Find **interesting relations** among the variables.

Overarching goal to keep in mind: **does this data have what I need to address my research question?**

[^next-slide]: (See next slide)

## Assumption-Free Analysis? {.crunch-title .crunch-ul .small-footnotes .small-captions}

::: columns
::: {.column width="50%"}

Important question for EDA: Is it actually possible to analyze data without assumptions?[^cs-polisci] 

> **Empirical results** are laden with **values** and **theoretical commitments**.<br>[[@boyd_theory_2021]]{style="font-size: 0.5em !important;"}

:::
::: {.column width="50%"}

Ex: Estimates of optimal tax rates in Econ journals vs. Economist ideology scores

```{r}
#| label: political-lang-in-econ
#| echo: false
#| fig-cap: Computed based on @jelveh_political_2023
tr_df <- read_csv("assets/political_lang_estimates.csv")
my_dexp <- function(x) 0.322 + 0.633 * exp(-0.633 * (x+1))
ggplot(tr_df, aes(x=ideology, y=tr_est)) +
  geom_point(size = g_pointsize) +
  #geom_line(linewidth = g_linewidth) +
  stat_function(fun=my_dexp, linewidth = g_linewidth, linetype = "dashed") +
  labs(
    title = "Ideology vs. Optimal Tax Rate Estimates",
    x = "Ideology Score (Campaign Contributions)",
    y = "Published Estimate"
  ) +
  dsan_theme("half") +
  scale_y_continuous(labels = scales::percent) +
  expand_limits(x=c(-1,1))
```

:::
:::

[^cs-polisci]: (When I started as a PhD student in Computer Science, I would have answered "yes". Now that my brain has been warped by social science, I'm not so sure.)

## Statistical EDA {.smaller .crunch-title .crunch-figures .crunch-ul .nostretch}

* **Iterative process**: Ask questions of the data, find answers, generate more questions
* You're probably already used to **Mean** and **Variance**: Fancier EDA/robustness methods build upon these two!
* Why do we need to visualize? Can't we just use mean, $R^2$?
* ...Enter <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet" target="_blank">Anscombe's Quartet</a>

```{python}
#| label: anscombe-quartet
#| fig-align: center
#| fig-height: 3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="ticks")
# https://towardsdatascience.com/how-to-use-your-own-color-palettes-with-seaborn-a45bf5175146
sns.set_palette(sns.color_palette(cb_palette))

# Load the example dataset for Anscombe's quartet
anscombe_df = sns.load_dataset("anscombe")
#print(anscombe_df)

# Show the results of a linear regression within each dataset
anscombe_plot = sns.lmplot(
    data=anscombe_df, x="x", y="y", col="dataset", hue="dataset",
    col_wrap=4, palette="muted", ci=None,
    scatter_kws={"s": 50, "alpha": 1},
    height=3
);
anscombe_plot;
```

## The Scariest Dataset of All Time {.smaller .smaller-table-text .crunch-title .crunch-code .small-captions .crunch-images .nostretch}

```{python}
#| label: anscombe-again
#| echo: false
anscombe_plot
```

::: columns
::: {.column width="60%"}

<center>
**Summary statistics**
</center>

::: {layout-ncol=2}

::: {#fig-anscombe-means}

```{python}
#| label: anscombe-means
# Compute dataset means
my_round = lambda x: round(x,2)
data_means = anscombe_df.groupby('dataset').agg(
  x_mean = ('x', np.mean),
  y_mean = ('y', np.mean)
).apply(my_round)
disp(data_means, floatfmt='.2f')
```

Column means for each dataset
:::

::: {#fig-anscombe-sds}

```{python}
#| label: anscombe-sds
# Compute dataset SDs
data_sds = anscombe_df.groupby('dataset').agg(
  x_mean = ('x', np.std),
  y_mean = ('y', np.std),
).apply(my_round)
disp(data_sds, floatfmt='.2f')
```

Column SDs for each dataset
:::

<!-- end anscombe SDs -->

:::

<!-- end layout-ncol=2 -->

:::

<!-- end sumstats column -->

::: {.column width="40%"}

<center>
**Correlations**
</center>

::: {#fig-anscombe-corrs}

```{python}
#| label: anscombe-corrs
#| output: asis
#| classes: corr-table
import tabulate
from IPython.display import HTML
corr_matrix = anscombe_df.groupby('dataset').corr().apply(my_round)
#Markdown(tabulate.tabulate(corr_matrix))
HTML(corr_matrix.to_html())
```

Correlation between $x$ and $y$ for each dataset
:::

<!-- end anscombe corrs fig -->

:::

<!-- end corr column -->

:::

<!-- end columns -->

## It Doesn't End There... {.smaller .smaller-table-text .nostretch}

```{python}
#| label: anscombe-yet-again
#| echo: false
anscombe_plot
```

::: {layout="[[1,1],[1,1]]"}

```{python}
#| label: anscombe-reg-0
#| classes: corr-table
import statsmodels.formula.api as smf
summary_dfs = []
for cur_ds in ['I','II','III','IV']:
  ds1_df = anscombe_df.loc[anscombe_df['dataset'] == "I"].copy()
  # Fit regression model (using the natural log of one of the regressors)
  results = smf.ols('y ~ x', data=ds1_df).fit()
  # Get R^2
  rsq = round(results.rsquared, 2)
  # Inspect the results
  summary = results.summary()
  summary.extra_txt = None
  summary_df = summary_to_df(summary, corner_col = f'Dataset {cur_ds}<br>R^2 = {rsq}')
  summary_dfs.append(summary_df)
disp(summary_dfs[0], include_index=False)
```

```{python}
#| label: anscombe-reg-1
disp(summary_dfs[1], include_index=False)
```

```{python}
#| label: anscombe-reg-2
disp(summary_dfs[2], include_index=False)
```

```{python}
#| label: anscombe-reg-3
disp(summary_dfs[3], include_index=False)
```

:::

## Normalization {.smaller}

::: columns
::: {.column width="42%"}

<center>
**Unnormalized World**
</center>

```{r}
#| label: r-gen-test-scores
#| code-fold: true
num_students <- 30
student_ids <- seq(from = 1, to = num_students)
# So we have the censored Normal pdf/cdf
library(crch)
gen_test_scores <- function(min_pts, max_pts) {
  score_vals_unif <- runif(num_students, min_pts, max_pts)
  unif_mean <- mean(score_vals_unif)
  unif_sd <- sd(score_vals_unif)
  # Resample, this time censored normal dist
  score_vals <- round(rcnorm(num_students, mean=unif_mean, sd=unif_sd, left=min_pts, right=max_pts), 2)
  return(score_vals)
}
# Test 1
t1_min <- 0
t1_max <- 268.3
t1_score_vals <- gen_test_scores(t1_min, t1_max)
t1_mean <- mean(t1_score_vals)
t1_sd <- sd(t1_score_vals)
get_t1_pctile <- function(s) round(100 * ecdf(t1_score_vals)(s), 1)
# Test 2
t2_min <- -1
t2_max <- 1.2
t2_score_vals <- gen_test_scores(t2_min, t2_max)
t2_mean <- mean(t2_score_vals)
t2_sd <- sd(t2_score_vals)
get_t2_pctile <- function(s) round(100 * ecdf(t2_score_vals)(s), 1)
score_df <- tibble(
  id=student_ids,
  t1_score=t1_score_vals,
  t2_score=t2_score_vals
)
score_df <- score_df |> arrange(desc(t1_score))
```

*"I got a **238.25** on the first test!"* ü§©

*"But only a **0.31** on the second"* üò≠

```{r}
#| label: r-test-scores-unnormalized
#| echo: false
score_df |> head()
```

:::
::: {.column width="58%"}

<center>
**Normalized World**
</center>

```{r}
#| label: r-normalize-test-scores
score_df <- score_df |>
  mutate(
    t1_z_score = round((t1_score - t1_mean) / t1_sd, 2),
    t2_z_score = round((t2_score - t2_mean) / t2_sd, 2),
    t1_pctile = get_t1_pctile(t1_score),
    t2_pctile = get_t2_pctile(t2_score)
  ) |>
  relocate(t1_pctile, .after = t1_score) |>
  relocate(t2_pctile, .after = t2_score)
```

*"I scored higher than **90%** of students on the first test!* ü§©

*"And higher than **60%** on the second!"* üòé

```{r}
#| label: r-normalized-scores
#| echo: false
score_df |> head()
```

:::
:::

## Scaling {.smaller .crunch-title .crunch-figures .crunch-images .crunch-code .crunch-p}

The percentile places everyone at **evenly-spaced intervals** from 0 to 100:

```{r}
#| label: pctile-numberline
#| fig-height: 2.5
# https://community.rstudio.com/t/number-line-in-ggplot/162894/4
# Add a binary indicator to track "me" (student #8)
whoami <- 29
score_df <- score_df |>
  mutate(is_me = as.numeric(id == whoami))
library(ggplot2)
t1_line_data <- tibble(
  x = score_df$t1_pctile,
  y = 0,
  me = score_df$is_me
)
ggplot(t1_line_data, aes(x, y, col=factor(me), shape=factor(me))) +
  geom_point(aes(size=g_pointsize)) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  scale_color_discrete(c(0,1)) +
  dsan_theme("half") +
  theme(
    legend.position="none", 
    #rect = element_blank(),
    #panel.grid = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y=element_blank(),
    panel.spacing = unit(0, "mm"),
    plot.margin = margin(-35, 0, 0, 0, "pt"),
  ) +
  labs(
    x = "Test 1 Percentile"
  ) +
  coord_fixed(ratio = 100)
```

![](images/foucault_mirror.jpg){.absolute top="2%" right=0 width="15%"}

But what if we want to see their **absolute** performance, on a 0 to 100 scale?

```{r}
#| label: scaled-score-numberline
#| fig-height: 2.5
library(scales)
score_df <- score_df |>
  mutate(
    t1_rescaled = rescale(
      t1_score,
      from = c(t1_min, t1_max),
      to = c(0, 100)
    ),
    t2_rescaled = rescale(
      t2_score,
      from = c(t2_min, t2_max),
      to = c(0, 100)
    )
  )
# Place "me" last so that it gets plotted last
t1_rescaled_line_data <- tibble(
  x = score_df$t1_rescaled,
  y = 0,
  me = score_df$is_me
) |> arrange(me)
ggplot(t1_rescaled_line_data, aes(x,y,col=factor(me), shape=factor(me))) +
  geom_point(size=g_pointsize) +
  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +
  dsan_theme("half") +
  expand_limits(x=c(0, 100)) +
  theme(
    legend.position="none", 
    #rect = element_blank(),
    #panel.grid = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y=element_blank(),
    #panel.spacing = unit(0, "mm"),
    #plot.margin = margin(-40, 0, 0, 0, "pt"),
  ) +
  labs(
    x = "Test 1 Score (Rescaled to 0-100)"
  ) +
  coord_fixed(ratio = 100)
```

![](images/foucault_happy.jpeg){.absolute top="56%" right=0 width="15%"}

## Shifting / Recentering  {.crunch-title .crunch-math}

* Percentiles tell us how the students did **in terms of relative rankings**
* Rescaling lets us reinterpret the **boundary points**
* What about with respect to some **absolute baseline**? For example, how well they did **relative to the mean $\mu$**?

$$
x'_i = x_i - \mu
$$

* But we're still "stuck" in units of the test: is $x'_i = 0.3$ (0.3 points above the mean) "good"? What about $x'_j = -2568$ (2568 points below the mean)? How "bad" is this case?

## Shifting and Scaling: The $z$-Score {.crunch-title .crunch-math .crunch-code .crunch-ul .crunch-images .nostretch}

* Enter the $z$-score!

$$
z_i = \frac{x_i - \mu}{\sigma}
$$

* Unit of original $x_i$ values: ?
* Unit of $z$-score: **standard deviations from the mean**!

```{r}
#| label: z-score-numberline
#| fig-height: 2.5
#| fig-width: 10
t1_z_score_line_data <- tibble(
  x = score_df$t1_z_score,
  y = 0,
  me = score_df$is_me
) |> arrange(me)
ggplot(t1_z_score_line_data, aes(x, y, col=factor(me), shape=factor(me))) +
  geom_point(aes(size=g_pointsize)) +
  scale_x_continuous(breaks=c(-2,-1,0,1,2)) +
  dsan_theme("half") +
  theme(
    legend.position="none", 
    #rect = element_blank(),
    #panel.grid = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y=element_blank(),
    plot.margin = margin(-20,0,0,0,"pt")
  ) +
  expand_limits(x=c(-2,2)) +
  labs(
    x = "Test 1 Z-Score"
  ) +
  coord_fixed(ratio = 3)
```

# Distance Metrics {data-stack-name="Distances" .smaller .small-title .not-title-slide .crunch-title .crunch-figures .crunch-images}

::: {layout="[[1,1],[1,1]]"}

![](images/line_vs_sphere.svg){fig-align="center"}

![](images/kozlowski_crop.jpeg){fig-align="center" width="550"}

![](images/culture_vs_geography.jpeg){fig-align="center" width="450"}

![](images/riemann_sphere.svg){fig-align="center" width="350"}

:::

## Why All The Worry About Units?

* Euclidean Distance
* Manhattan Distance
* Spherical Distance vs. Straight-Line Distance

## Why Should We Worry About This? {.smaller .small-math .crunch-ul .crunch-math .crunch-images .crunch-figures-less .small-footnotes .nostretch}

::: columns

::: {.column width="45%"}

::: {#plz}

Say you're training a **facial recognition** algorithm to detect criminals/terrorists

$$
\begin{align*}
&\Pr(\text{criminal}) \\
&= \textsf{dist}(\text{face}, \text{model of criminal face})
\end{align*}
$$

:::

![@pearson_life_1924[294]^[Good thing this guy isn't the father of modern statistics or anything like that üòÆ‚Äçüí®<br><br>(For more historical scariness take my DSAN 5450: Data Ethics and Policy course next semester! üòâ)]](images/wellcome_jewish_type_crop.jpeg){fig-align="center" width="100%"}



:::
::: {.column width="55%"}

![@galton_inquiries_1919, p. 8 (inset)](images/galton_criminality.jpeg){fig-align="center" width="60%"}

![@wang_deep_2018](images/deep_learning_gay_detection_crop.jpeg){fig-align="center" width="75%"}

:::
:::

## Distances Are *Metaphors* We Use To *Accomplish Something* {.smaller .small-captions .small-title}

![Image Credit: [Peter Dovak](https://www.behance.net/gallery/10965947/Washington-Metro-Map-to-Scale){target="_blank"}](images/dc_map_scaled_unscaled.png){fig-align="center"}

## Which Metric(s) Should We Use? {.smaller .crunch-title .crunch-math}

::: columns
::: {.column width="40%"}

<center>
**Ambulance Driver?**
</center>

![From @shahid_comparison_2009](images/medical_distances.jpg){fig-align="center" width="90%"}

:::
::: {.column width="60%"}

<center>
**Data Scientist?**
</center>

$L^p$-norm:

$$
|| \mathbf{x} - \mathbf{y} ||_p = \left(\sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}
$$

Edit Distance, e.g., Hamming distance:

$$
\begin{array}{c|c|c|c|c|c}
x & \green{1} & \green{1} & \red{0} & \red{1} & 1 \\ \hline
 & ‚úÖ & ‚úÖ & ‚ùå & ‚ùå & ‚úÖ \\\hline
y & \green{1} & \green{1} & \red{1} & \red{0} & 1 \\
\end{array} \; \leadsto d(x,y) = 2
$$

KL Divergence (Probability distributions):

$$
\begin{align*}
\kl(P \parallel Q) &= \sum_{x \in \mathcal{R}_X}P(x)\log\left[ \frac{P(x)}{Q(x)} \right] \\
&\neq \kl(Q \parallel P) \; (!)
\end{align*}
$$

:::
:::

## Onto the Math! $L^p$-Norms {.smaller .crunch-title .crunch-ul}

* Euclidean distance = $L^2$-norm:

$$
|| \mathbf{x} - \mathbf{y} ||_2 = \sqrt{\sum_{i=1}^n(x_i-y_i)^2}
$$

* Manhattan distance = $L^1$-norm:

$$
|| \mathbf{x} - \mathbf{y} ||_1 = \sum_{i=1}^n |x_i - y_i|
$$

* The **maximum**(!) = $L^\infty$-norm:

$$
|| \mathbf{x} - \mathbf{y} ||_{\infty} = \lim_{p \rightarrow \infty}\left[|| \mathbf{x} - \mathbf{y} ||_p\right] = \max\{|x_1-y_1|, \ldots, |x_n - y_n|\}
$$

## Top Secret Non-Well-Defined Yet Useful Norms {.smaller .small-title .crunch-ul .crunch-code .crunch-images .crunch-figures .crunch-title .nostretch}

::: {layout-ncol=2}

::: {#quasi-norms}

* The "$L^0$-norm"

$$
|| \mathbf{x} - \mathbf{y} ||_0 = \mathbf{1}\left[x_i \neq y_i\right]
$$

* The "$L^{1/2}$-norm"

$$
|| \mathbf{x} - \mathbf{y} ||_{1/2} = \left(\sum_{i=1}^n \sqrt{x_i - y_i} \right)^2
$$

::: {#triangle-ineq .crunch-math}

* What's wrong with these norms? (Re-)enter the **Triangle Inequality**! $d$ defines a norm iff

$$
\forall a, b, c \left[ d(a,c) \leq d(a,b) + d(b,c) \right]
$$

:::

:::

::: {#fig-lp-plots}

Visualizing "circles" in $L^p$ space:

```{python}
#| label: lp-norms
import matplotlib.pyplot as plt
import numpy as np

#p_values = [0., 0.5, 1, 1.5, 2, np.inf]
p_values = [0.5, 1, 2, np.inf]
x, y = np.meshgrid(np.linspace(-3, 3, num=101), np.linspace(-3, 3, num=101))
fig, axes = plt.subplots(ncols=(len(p_values) + 1)// 2,
                     nrows=2, figsize=(5, 5))
for p, ax in zip(p_values, axes.flat):
  if np.isinf(p):
    z = np.maximum(np.abs(x),np.abs(y))
  else:
    z = ((np.abs((x))**p) + (np.abs((y))**p))**(1./p)
  ax.contourf(x, y, z, 30, cmap='bwr')
  ax.contour(x, y, z, [1], colors='red', linewidths = 2)
  ax.title.set_text(f'p = {p}')
  ax.set_aspect('equal', 'box')
plt.tight_layout()
#plt.subplots_adjust(hspace=0.35, wspace=0.25)
plt.show()
```

Plots adapted from [this StackOverflow post](https://stackoverflow.com/questions/48723466/l0-and-inf-norms-plotting-with-contour-contourf){target='blank'}
:::

:::

::: {.aside}

To go beyond this, and explore how $L^p$ "quasi-norms" can be extremely useful, take DSAN 6100: Optimization!<br>One place they can be useful? You guessed it: **facial recognition algorithms** [@guo_enhancing_2013]

:::

# Missing Values {data-stack-name="DGPs"}

## The Value of Studying {.smaller .nostretch}

* You are a teacher trying to assess the **causal impact** of **studying** on **homework scores**
* Let $S$ = hours of studying, $H$ = homework score

![](images/pgm_homework.svg){fig-align="center" width="300"}

* So far so good: we could estimate the relationship via (e.g.) regression

$$
h_i = \beta_0 + \beta_1 s_i + \varepsilon_i
$$

## My Dog Ate My Homework {.smaller}

* The issue: for some students $h_i$ is **missing**, since **their dog ate their homework**
* Let $D = \begin{cases}1 &\text{if dog ate homework} \\ 0 &\text{otherwise}\end{cases}$
* This means we don't observe $H$ but $H^* = \begin{cases} H &\text{if }D = 0 \\ \texttt{NA} &\text{otherwise}\end{cases}$
* In the **easy case**, let's say that dogs eat homework **at random** (i.e., without reference to $S$ or $H$). Then we say $H$ is "missing at random".  Our PGM now looks like:

![](images/pgm_hw_missing_at_random.svg){fig-align="center"}

## My Dog Ate My Homework Because of Reasons {.smaller .small-title .nostretch}

![](images/reasons_crop.jpeg){.absolute top="-15" right="10%"}

There are scarier alternatives, though! What if...

::: {layout-ncol=3}

::: {#dog-study}

Dogs eat homework because their owner studied so much that the dog got ignored?

![](images/pgm_hw_missing_study.svg){fig-align="center" width="280"}

:::

::: {#dog-bad-hw}

Dogs hate sloppy work, and eat bad homework that would have gotten a low score

![](images/pgm_hw_missing_badhw.svg){fig-align="center" width="280"}

:::

::: {#dog-noisy}

Noisy homes ($Z = 1$) cause dogs to get agitated and eat homework more often, **and** students do worse

![](images/pgm_hw_missing_noisy.svg){fig-align="center" width="280"}

:::

:::

# Outlier Detection

## Tukey's Rule {.smaller .crunch-title .crunch-ul}

* Given the **first quartile** (25th percentile) $Q_1$, and the **third quartile** (75th percentile) $Q_2$, define the **Inter-Quartile Range** as

$$
\iqr = Q_3 - Q_1
$$

* Then an **outlier** is a point more than $1.5 \cdot \iqr$ away from $Q_1$ or $Q_3$; outside of

$$
[Q_1 - 1.5 \cdot \iqr, \; Q_3 + 1.5 \cdot \iqr]
$$

* This is the outlier rule used for **box-and-whisker plots**:

```{r}
#| label: boxplot-outliers
#| fig-height: 3
library(ggplot2)
library(tibble)
library(dplyr)
# Generate normal data
dist_df <- tibble(Score=rnorm(95), Distribution="N(0,1)")
# Add outliers
outlier_dist_sd <- 6
outlier_df <- tibble(Score=rnorm(5, 0, outlier_dist_sd), Distribution=paste0("N(0,",outlier_dist_sd,")"))
data_df <- bind_rows(dist_df, outlier_df)
# Compute iqr and outlier range
q1 <- quantile(data_df$Score, 0.25)
q3 <- quantile(data_df$Score, 0.75)
iqr <- q3 - q1
iqr_cutoff_lower <- q1 - 1.5 * iqr
iqr_cutoff_higher <- q3 + 1.5 * iqr
is_outlier <- function(x) (x < iqr_cutoff_lower) || (x > iqr_cutoff_higher)
data_df['Outlier'] <- sapply(data_df$Score, is_outlier)
#data_df
ggplot(data_df, aes(x=Score, y=factor(0))) + 
  geom_boxplot(outlier.color = NULL, linewidth = g_linewidth, outlier.size = g_pointsize / 1.5) +
  geom_jitter(data=data_df, aes(col = Distribution, shape=Outlier), size = g_pointsize / 1.5, height=0.15, alpha = 0.8, stroke = 1.5) +
  geom_vline(xintercept = iqr_cutoff_lower, linetype = "dashed") +
  geom_vline(xintercept = iqr_cutoff_higher, linetype = "dashed") +
  #coord_flip() +
  dsan_theme("half") +
  theme(
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) +
  scale_x_continuous(breaks=seq(from=-3, to=3, by=1)) +
  scale_shape_manual(values=c(16, 4))
```

## 3-Sigma Rule {.smaller}

* Recall the [**68-95-99.7 Rule**](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule){target="_blank"}
* The **3-Sigma Rule** says simply: throw away anything more than **3 standard deviations** away from the mean (beyond range that should contain 99.7% of data)

```{r}
mean_score <- mean(data_df$Score)
sd_score <- sd(data_df$Score)
lower_cutoff <- mean_score - 3 * sd_score
upper_cutoff <- mean_score + 3 * sd_score
# For printing / displaying
mean_score_str <- sprintf(mean_score, fmt='%.2f')
sd_score_str <- sprintf(sd_score, fmt='%.2f')
ggplot(data_df, aes(x=Score)) + 
  geom_density(linewidth = g_linewidth) +
  #geom_boxplot(outlier.color = NULL, linewidth = g_linewidth, outlier.size = g_pointsize / 1.5) +
  #geom_jitter(data=data_df, aes(y = factor(0), col = dist), size = g_pointsize / 1.5, height=0.25) +
  #coord_flip() +
  dsan_theme("half") +
  theme(
    axis.title.y = element_blank(),
    #axis.ticks.y = element_blank(),
    #axis.text.y = element_blank()
  ) +
  #geom_boxplot() +
  geom_vline(xintercept = mean_score, linetype = "dashed") +
  geom_vline(xintercept = lower_cutoff, linetype = "dashed") +
  geom_vline(xintercept = upper_cutoff, linetype = "dashed") +
  geom_jitter(data=data_df, aes(x = Score, y = 0, col = Distribution, shape = Outlier),
    size = g_pointsize / 1.5, height=0.025, alpha=0.8, stroke=1.5) +
  scale_x_continuous(breaks=seq(from=-3, to=3, by=1)) +
  scale_shape_manual(values=c(16, 4)) +
  labs(
    title = paste0("Six-Sigma Rule, mu = ",mean_score_str,", sd = ",sd_score_str),
    y = "Density"
  )
```

## Missing Data + Outliers: Most Important Takeaway!

* Always have a working hypothesis about the **Data-Generating Process**!
* Literally the solution to... 75% of all data-related headaches
* What **variables** explain **why** this data point is missing?
* What **variables** explain **why** this data point is an outlier?

## Driving the Point Home {.nostretch}

::: {layout-ncol=2}

![Figure (and DGP analysis) from @dignazio_numbers_2020](images/kidnappings.jpg){fig-align="center"}

::: {#kidnapping-dgp}

Presumed DGP:

![$K$ = Kidnappings, $C$ = Counts](images/dgp_kidnappings.svg){fig-align="center" width="300"}

Actual DGP:

![$K$ = Kidnappings, $R$ = **News reports** about kidnappings, $C$ = Counts](images/dgp_kidnappings_true.svg){fig-align="center" width="300"}

:::

:::

# Lab 5 {.not-title-slide .small-title data-stack-name="Lab 5"}

[Lab 5 Demo: Seaborn Link <i class='bi bi-box-arrow-up-right ps-1'></i>](https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/slides/seaborn-introduction/notes.html){target="_blank"}

[Lab 5 Assignment: EDA with Seaborn <i class='bi bi-box-arrow-up-right ps-1'></i>](https://jfh.georgetown.domains/dsan5000/slides-and-labs/_site/content/labs/assignments/lab-3.1-EDA/lab-3.1.html){target="_blank"}

## References

::: {#refs}
:::
