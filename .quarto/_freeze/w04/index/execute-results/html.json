{
  "hash": "80c023a2b3a912bcfa636868d0fe74b9",
  "result": {
    "markdown": "---\ntitle: \"Week 4: Data Gathering and APIs\"\nsubtitle: \"*DSAN 5100: Data Science and Analytics*<br>Section <span class='sec-num'>02</span>\"\ndate: 2023-09-12\ndate-format: full\nauthor: \"Jeff Jacobs\"\ninstitute: \"<a href=\\\"mailto:jj1088@georgetown.edu\\\" target=\\\"_blank\\\">jj1088@georgetown.edu</a>\"\n#date: last-modified\n#date-format: \"dddd MMM D, YYYY, HH:mm:ss\"\nlecnum: 4\ncategories:\n  - \"Class Sessions\"\nbibliography: \"../_DSAN5000.bib\"\nformat:\n  revealjs:\n    html-math-method: mathjax\n    # sadly, caching breaks this one -__-\n    # cache: true\n    cache: false\n    scrollable: false\n    slide-number: true\n    output-file: slides.html\n    section-divs: false\n    simplemenu:\n      flat: true\n      barhtml:\n        header: \"<div class='menubar'><span style='position: absolute; left: 8; padding-left: 8px;'><a href='./index.html'>&larr; Return to Notes</a></span><ul class='menu'></ul></div>\"\n      scale: 0.5\n    theme: [default, \"../_jjslides.scss\"]\n    revealjs-plugins:\n      - simplemenu\n  html:\n    output-file: index.html\n    html-math-method: mathjax\n    cache: false\nmetadata-files: \n  - \"../_slide-meta.yml\"\n---\n\n\n\n\n::: {.content-visible unless-format=\"revealjs\"}\n\n<center>\n<a class=\"h2\" href=\"./slides.html\" target=\"_blank\">Open slides in new window &rarr;</a>\n</center>\n\n:::\n\n# Schedule {.smaller .small-title .crunch-title .crunch-callout data-stack-name=\"Schedule\"}\n\nToday's Planned Schedule (Section <span class='sec-num'>02</span>):\n\n| | Start | End | Topic | Recording |\n|:- |:- |:- |:- |:-:|\n| **Lecture** | <span class='sec-start'>12:30pm</span> | <span class='sec-p10'>12:40pm</span> | <a href=\"#week-03-recap\">Week 03 Recap &rarr;</a> | <a href=\"../recordings/recording-w04s02-1.html\" target=\"_blank\"><i class=\"bi bi-film\"></i></a> |\n| | 12:40pm | 1:00pm | <a href=\"https://georgetown.instructure.com/courses/173310/quizzes/201388\">Quiz 2.1 <i class=\"bi bi-box-arrow-up-right\" style=\"font-size: 1.5rem;\"></i></a> | |\n| | 1:00pm | 1:30pm | <a href=\"#data-gathering\">Data Gathering &rarr;</a> | <a href=\"../recordings/recording-w04s02-2.html\" target=\"_blank\"><i class=\"bi bi-film\"></i></a> |\n| | 1:30pm | 1:45pm | <a href=\"#web-scraping\">Web Scraping &rarr;</a> | <a href=\"../recordings/recording-w04s02-3.html\" target=\"_blank\"><i class=\"bi bi-film\"></i></a> |\n| | 1:45pm | 2:00pm | <a href=\"#apis\">APIs &rarr;</a> | <a href=\"../recordings/recording-w04s02-4.html\" target=\"_blank\"><i class=\"bi bi-film\"></i></a> |\n| **Break!** | 2:00pm | 2:10pm | | |\n| **Lab** | 2:10pm | 2:50pm | <a href=\"#lab-demonstrations\">Lab Demonstrations &rarr;</a> | <a href=\"../recordings/recording-w04s02-5.html\" target=\"_blank\"><i class=\"bi bi-film\"></i></a> |\n| | 2:50pm | 3:00pm | <a href=\"#lab-assignment-overview\">Lab Assignment Overview &rarr;</a> | <a href=\"../recordings/recording-w04s02-6.html\" target=\"_blank\"><i class=\"bi bi-film\"></i></a> |\n\n: {tbl-colwidths=\"[14,12,12,50,12]\"} \n\n# Week 03 Recap {.smaller .smaller-title .not-title-slide data-stack-name=\"Recap\"}\n\n![Image from @menczer_first_2020 [p. 90]](images/client_server.jpeg){fig-align=\"center\"}\n\n## Git Commands {.smaller .crunch-title}\n\n| Command | What It Does |\n| - | - |\n| `git clone` | Downloads a repo from the web to our local computer |\n| `git init` | Creates a new, blank Git repository on our local computer (configuration/change-tracking stored in `.git` subfolder) |\n| `git add` | **Stages** a file(s): Git will now track changes in this file(s) |\n| `git reset` | Undoes a `git add` |\n| `git status` | Shows currently staged files and their status (created, modified, deleted) |\n| `git commit -m \"message\"` | \"Saves\" the current version of all staged files, ready to be pushed to a backup dir or remote server like GitHub |\n| `git push` | Transmits local commits to remote server |\n| `git pull` | Downloads commits from remote server to local computer |\n| `git merge` | Merges remote versions of files with local versions |\n\n: {tbl-colwidths=\"[30,70]\"}\n\n## Reproducible Docs/Literate Programming {.small-title .crunch-title}\n\n* 1980s: $\\LaTeX$ for $\\widehat{\\mathcal{T}}$ypesetting $\\sqrt{math}^2$\n* 1990s: **Python** and **R** as powerful **scripting** languages (no compilation required)\n* 2000s/2010s: Interactive Python via **Jupyter**, fancy IDE for R called **RStudio**\n* 2020s: **Quarto** (using **pandoc** under the hood) enables use of markdown for formatting, $\\LaTeX$ for math, and both Python and R in same document, with choice of output formats (HTML, presentations, Word docs, ...)\n\n# Quiz Time\n\n# Data Gathering {data-stack-name=\"Data Gathering\"}\n\n* Preexisting data sources\n* Web scraping\n* Converting between formats\n\n## Preexisting Data Sources {.crunch-title}\n\n* Depending on your field, or the type of data you're looking for, there may be a \"standard\" data source! For example:\n* *Economics*:\n    * US data: [FRED](https://fred.stlouisfed.org/){target=\"_blank\"}\n    * Global data: [World Bank Open Data](https://data.worldbank.org/){target=\"_blank\"}, [OECD Data](https://data.oecd.org/){target=\"_blank\"}, etc.\n* *Political Science*:\n    * [ICPSR](https://www.icpsr.umich.edu/web/pages/){target=\"_blank\"}\n* *Network Science*:\n    * [Stanford SNAP: Large Network Dataset Collection](https://snap.stanford.edu/data/){target=\"_blank\"}\n\n## Web Scraping\n\n* Fun fact: you can view a webpage's **HTML source code** by right-clicking on the page and selecting \"View Source\"\n  * On older websites, this means we can just request page and parse the returned HTML\n* Less fun fact: modern web frameworks (**React**, **Next.js**) generate pages dynamically using JS, meaning that what you see on the page will not be visible in the HTML source\n  * Data scraping still possible for these sites! Using browser automation tools like <a href=\"https://www.selenium.dev/\" target=\"_blank\">**Selenium**</a>\n\n## Scraping Difficulty {.crunch-title}\n\n::: {.smallish}\n\n| | | How is data loaded? | Solution | Example |\n|:-:|-|-|-|:-:|\n| üòä | **Easy** | Data in HTML source | \"View Source\" | [<i class=\"bi bi-box-arrow-up-right\"></i>](https://ivanstat.com/en/gdp/ao.html){target=\"_blank\"}\n| üòê | **Medium** | Data loaded dynamically via API | \"View Source\", find API call, scrape programmatically | [<i class=\"bi bi-box-arrow-up-right\"></i>](https://archive.org/details/killinghopeusmil0000blum/page/n3/mode/2up){target=\"_blank\"}\n| üò≥ | **Hard** | Data loaded dynamically [internally] via web framework | Use <a href=\"https://www.selenium.dev/\" target=\"_blank\">Selenium</a> | [<i class=\"bi bi-box-arrow-up-right\"></i>](https://www.google.com/books/edition/Killing_Hope/-IbQvd13uToC?hl=en&gbpv=1&dq=killing%20hope&pg=PA215&printsec=frontcover){target=\"_blank\"}\n\n: {tbl-colwidths=\"[5,10,45,35,5]\"}\n\n:::\n\n## Data Structures: Foundations\n\n* Could be ([is](https://www.cs.umd.edu/class/spring2022/cmsc420-0101/){target=\"_blank\"}) a whole class\n* Could be ([is](https://www.google.com/books/edition/The_Design_and_Analysis_of_Spatial_Data/LttQAAAAMAAJ?hl=en&gbpv=0&bsq=samet%20spatial%20data%20structures){target=\"_blank\"}) a whole class just for one type of data (geographic/spatial)\n* For this class: some **foundational** principles that should let you figure out fancier data structures you encounter\n\n## Opening Datasets With Your Terminator Glasses On\n\n::: {layout-ncol=2}\n\n::: {.smallish}\n* What does a **row** represent?\n* What does a **column** represent?\n* What does a value in a **cell** represent?\n* Are there **unique identifiers** for the objects you care about?\n:::\n\n![What you should see when you look at a new dataset](images/terminator.jpeg){#fig-terminator}\n\n:::\n\n## From Raw Data to Clean Data\n\n\n\n{{< video https://youtu.be/crVo8Cdo4x0 width=\"100%\" height=\"75%\" >}}\n\n\n\n\n\n## Data Structures: Simple $\\rightarrow$ Complex {.smaller .crunch-title .crunch-figures}\n\n::: {layout=\"[[1,1],[1,1]]\"}\n\n::: {#fig-record-data}\n\n| id | name | email |\n| - | - | - |\n| 0 | K. Desbrow | kd9@dailymail.com |\n| 1 | D. Minall | dminall1@wired.com |\n| 2 | C. Knight | ck2@microsoft.com |\n| 3 | M. McCaffrey | mccaf4@nhs.uk |\n\nRecord Data\n:::\n\n::: {#fig-time-series-data}\n\n| year | month | points |\n| - | - | - |\n| 2023 | Jan | 65 |\n| 2023 | Feb | |\n| 2023 | Mar | 42 |\n| 2023 | Apr | 11 |\n\nTime-Series Data\n:::\n\n::: {#fig-panel-data}\n\n| id | date | rating | num_rides |\n| - | - | - | - |\n| 0 | 2023-01 | 0.75 | 45 |\n| 0 | 2023-02 | 0.89 | 63 |\n| 0 | 2023-03 | 0.97 | 7 |\n| 1 | 2023-06 | 0.07 | 10 |\n\n\nPanel Data\n:::\n\n::: {#fig-network-data}\n\n| Source | Target | Weight |\n| - | - | - |\n| IGF2 | IGF1R | 1 |\n| IGF1R | TP53 | 2 |\n| TP53 | EGFR | 0.5 |\n\nNetwork Data\n:::\n\n:::\n\n::: {.aside}\n\nFake data via [Mockaroo](https://www.mockaroo.com/){target=\"_blank\"} and [Random.org](https://www.random.org/integers/){target=\"_blank\"}. Protein-protein interaction network from [@agrawal_largescale_2018](http://psb.stanford.edu/psb-online/proceedings/psb18/agrawal.pdf)\n\n:::\n\n## Tabular Data vs. Relational Data {.smaller}\n\n* All of the datasets on the previous slide are **tabular**\n* Databases like SQLite, MySQL, require us to think about relationships **within** and **between** tabular datasets\n* Imagine you're creating the **backend** for a social network. How would you record **users** and **friendships**? Your intuition may be **record data**:\n\n::: {layout-ncol=2}\n\n::: {#fig-user-table}\n\n\n\n```{=html}\n<style>\n  #user-table tr th:nth-child(3) {\n\tbackground-color: rgba(255,0,0,0.25);\n  border-left: 3px solid red;\n  border-top: 3px solid red;\n  border-right: 3px solid red;\n}  \n#user-table tr td:nth-child(3) {\n\tbackground-color: rgba(255,0,0,0.25);\n  border-left: 3px solid red;\n  border-right: 3px solid red;\n}\n#user-table tr:last-child td:last-child {\n  border-bottom: 3px solid red;\n}\n</style>\n<table id=\"user-table\">\n<colgroup>\n  <col>\n  <col>\n  <col style=\"border: 5px solid red !important;\">\n</colgroup>\n<thead>\n  <tr>\n    <th>id</th><th>name</th><th>friends</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>1</td><td>Purna</td><td>[2,3,4]</td>\n  </tr>\n  <tr>\n    <td>2</td><td>Jeff</td><td>[1,3,4,5,6]</td>\n  </tr>\n  <tr>\n    <td>3</td><td>James</td><td>[1,2,4,6]</td>\n  </tr>\n  <tr>\n    <td>4</td><td>Nakul</td><td>[1,2,3]</td>\n  </tr>\n  <tr>\n    <td>5</td><td>Dr. Fauci</td><td>[2,6]</td>\n  </tr>\n  <tr>\n    <td>6</td><td>Pitbull</td><td>[2,5]</td>\n  </tr>\n</tbody>\n</table>\n```\n\n\n\nOur first attempt at a **data structure** for our social network app's backend\n:::\n\n::: {#friend-table-desc}\n\nLong story short...\n\n* This doesn't scale\n* Extremely inefficient to find whether two users are friends\n* Redundant information: Have to store friendship between **A** and **B** in both **A**'s row and **B**'s row\n\n:::\n\n:::\n\n## A Better Approach {.smaller .crunch-title .crunch-figures .crunch-layout-panel}\n\n* Move the **friendship** data into its own table!\n* This table now represents **relational data**, (**user** table still corresponds to **records**):\n\n::: {layout=\"[50,50]\"}\n\n::: {#fig-user-table}\n\n\n\n```{=html}\n<style>\n  #user-table-rel tr th:nth-child(3) {\n\tbackground-color: rgba(255,0,0,0.25);\n  border-left: 3px solid red;\n  border-top: 3px solid red;\n  border-right: 3px solid red;\n}  \n#user-table-rel tr td:nth-child(3) {\n\tbackground-color: rgba(255,0,0,0.25);\n  border-left: 3px solid red;\n  border-right: 3px solid red;\n}\n</style>\n<table id=\"user-table-rel\">\n<thead>\n  <tr>\n    <th>user_id</th><th>name</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>1</td><td>Purna</td>\n  </tr>\n  <tr>\n    <td>2</td><td>Jeff</td>\n  </tr>\n  <tr>\n    <td>3</td><td>James</td>\n  </tr>\n  <tr>\n    <td>4</td><td>Nakul</td>\n  </tr>\n  <tr>\n    <td>5</td><td>Dr. Fauci</td>\n  </tr>\n  <tr>\n    <td>6</td><td>Pitbull</td>\n  </tr>\n</tbody>\n</table>\n```\n\n\n\nThe **user** table in our relational structure\n:::\n\n::: {#fig-relational-friendships}\n\n\n\n\n```{=html}\n<style>\n#friend-table-rel tr th:nth-child(3) {\n  border-right: 3px solid black;\n}  \n#friend-table-rel tr td:nth-child(3) {\n  border-right: 3px solid black;\n}\n</style>\n<table id=\"friend-table-rel\">\n<thead>\n  <tr>\n    <th>id</th><th>friend_1</th><th>friend_2</th><th>id</th><th>friend_1</th><th>friend_2</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>1</td><td>1</td><td>2</td>\n    <td>6</td><td>2</td><td>5</td>\n  </tr>\n  <tr>\n    <td>2</td><td>1</td><td>3</td>\n    <td>7</td><td>2</td><td>6</td>\n  </tr>\n  <tr>\n    <td>3</td><td>1</td><td>4</td>\n    <td>8</td><td>3</td><td>4</td>\n  </tr>\n  <tr>\n    <td>4</td><td>2</td><td>3</td>\n    <td>9</td><td>3</td><td>6</td>\n  </tr>\n  <tr>\n    <td>5</td><td>2</td><td>4</td>\n    <td>10</td><td>5</td><td>6</td>\n  </tr>\n</tbody>\n</table>\n```\n\n\n\nThe **friendships** table in our relational structure\n:::\n:::\n\n* May seem weird in terms of human readability, but think in terms of **memory/computational efficiency**: **(a)** Scalable, **(b)** Easy to find if two users are friends (via sorting/searching algorithms), **(c)** No redundant info\n\n## DBs: Relational or Otherwise {.smaller .crunch-title .crunch-ul .shift-footnotes}\n\n* For rest of lecture we zoom in on cases where data comes as individual **files**\n* But on top of the **relational** format from previous slide, there are also **non-relational database** formats, like the **document-based** format used by e.g. MongoDB[^prisma]\n* In either case, data is spread over **many files**, so that to obtain a single dataset we use **queries**.\n\n::: {layout=\"[1,2]\"}\n\n::: {#fig-data-files}\n\n\n\n```{dot}\n//| fig-width: 5\n//| fig-height: 2\ndigraph G {\n  rankdir=LR\n  file[label=\"File (.csv/.json/etc.)\"];\n  load[label=\"read_csv()\"];\n  dataset[label=\"Dataset\"];\n  file -> load;\n  load -> dataset;\n}\n```\n\n\n\n**Statically** datasets (as **individual files** on disk)\n:::\n\n::: {#fig-data-queries}\n\n\n\n```{dot}\n//| fig-width: 6\n//| fig-height: 3\ndigraph G {\n  rankdir=LR\n\n  subgraph cluster_00 {\n    label=\"Database\"\n    tab1[label=\"Table 1\"];\n    tab2[label=\"Table 2\"];\n    tabdots[label=\"...\", penwidth=0];\n    tabN[label=\"Table N\"];\n  }\n\n  query[label=\"Query\", style=\"dashed\"];\n  tab1 -> query;\n  tab2 -> query;\n  tabdots -> query;\n  tabN -> query\n\n  dataset[label=\"Dataset\"];\n\n  query -> dataset;\n}\n```\n\n\n\nDatasets formed **dynamically** via **database queries**\n:::\n\n:::\n\n[^prisma]: For (much) more on this topic, see [this page](https://www.prisma.io/dataguide/types/relational-vs-document-databases){target=\"_blank\"} from **Prisma**, a high-level \"wrapper\" that auto-syncs your DB structure with a TypeScript **schema**, so your code knows exactly \"what's inside\" a variable whose content was retrieved from the DB...\n\n## Data Formats\n\n* The most common formats, for most fields:\n  * `.csv`: Comma-Separated Values\n  * `.tsv`: Tab-Separated Values\n  * `.json`: JavaScript Object Notation\n  * `.xls`/`.xlsx`: Excel format\n  * `.dta`: Stata format\n\n## `.csv` / `.tsv` {.smaller}\n\n::: columns\n::: {.column width=\"47%\"}\n\nüëç\n\n```csv {filename=\"my_data.csv\"}\nindex,var_1,var_2,var_3\nA,val_A1,val_A2,val_A3\nB,val_B1,val_B2,val_B3\nC,val_C1,val_C2,val_C3\nD,val_D1,val_D2,val_D3\n```\n\n::: {.small-codeblock}\n\n(üëé)\n\n```csv {filename=\"my_data.tsv\"}\nindex var_1 var_2 var_3\nA val_A1  val_A2  val_A3\nB val_B1  val_B2  val_B3\nC val_C1  val_C2  val_C3\nD val_D1  val_D2  val_D3\n```\n\n:::\n\n:::\n::: {.column width=\"6%\"}\n&rarr;\n:::\n\n::: {.column width=\"47%\"}\n\n::: {.fw-table .small-table .r-stretch}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../_globals.r\")\nlibrary(readr)\ndata <- read_csv(\"assets/my_data.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 3 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): index, var_1, var_2, var_3\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ndisp(data)\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"datatables html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-be2784eab43122e063f8\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-be2784eab43122e063f8\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"extensions\":[\"FixedColumns\",\"FixedHeader\"],\"data\":[[\"1\",\"2\",\"3\"],[\"A\",\"B\",\"C\"],[\"val_A1\",\"val_B1\",\"val_C1\"],[\"val_A2\",\"val_B2\",\"val_C2\"],[\"val_A3\",\"val_B3\",\"val_C3\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>index<\\/th>\\n      <th>var_1<\\/th>\\n      <th>var_2<\\/th>\\n      <th>var_3<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":6,\"scrollX\":true,\"paging\":true,\"dom\":\"t\",\"fixedHeader\":true,\"filter\":false,\"ordering\":false,\"language\":{\"paginate\":{\"previous\":\"<i class='bi bi-chevron-left'><\\/i>\",\"next\":\"<i class='bi bi-chevron-right'><\\/i>\"}},\"columnDefs\":[{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[6,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n\n```{.r .cell-code}\n# | index | var_1 | var_2 | var_3 |\n# | - | - | - | - |\n# | A | val_A1 | val_A2 | val_A3 |\n# | B | val_B1 | val_B2 | val_B3 |\n# | C | val_C1 | val_C2 | val_C3 |\n# | D | val_D1 | val_D2 | val_D3 | \n```\n:::\n\n\n\n:::\n\n:::\n:::\n\n* Python: `pd.read_csv()` (from Pandas library)\n* R: `read_csv()` (from `readr` library)\n\n## `.json` {.smaller}\n\n::: {.smallish-codeblock}\n\n```json {filename=\"courses.json\"}\n{\n  \"dsan5000\": {\n    \"title\": \"Data Science and Analytics\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Tools and Workflow\"\n    ]\n  },\n  \"dsan5100\": {\n    \"title\": \"Probabilistic Modeling and Statistical Computing\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Conditional Probability\"\n    ]\n  }\n}\n```\n:::\n\n* Python: <a href=\"https://docs.python.org/3/library/json.html\" target=\"_blank\">`json`</a> (built-in library</a>, `import json`)\n* R: <a href=\"https://cran.r-project.org/web/packages/jsonlite/index.html\" target=\"_blank\">`jsonlite`</a> (`install.packages(jsonlite)`)\n* <a href=\"https://jsonlint.com/\" target=\"_blank\">Helpful validator</a> (for when `.json` file won't load)\n\n## Other Formats\n\n* `.xls`/`.xlsx`: Requires special libraries in Python/R\n  * Python: <a href=\"https://openpyxl.readthedocs.io\" target=\"_blank\">`openpyxl`</a>\n  * R: <a href=\"https://readxl.tidyverse.org/\" target=\"_blank\">`readxl`</a> (part of tidyverse)\n* `.dta`: Stata format, but can be read/written to in Python/R\n  * Python: Pandas has built-in `pd.read_stata()` and `pd.to_stata()`\n  * R: `read_dta()` from <a href=\"https://haven.tidyverse.org/reference/read_dta.html\" target=\"_blank\">Haven</a> library (part of tidyverse)\n\n\n# Web Scraping {.smaller .small-title .not-title-slide data-stack-name=\"Web Scraping\"}\n\n| | How is data loaded? | Solution | Example |\n|-|-|-|-|:-:|\n| **This section &rarr;** | üòä **Easy** | Data in HTML source | \"View Source\" | [<i class=\"bi bi-box-arrow-up-right\"></i>](https://ivanstat.com/en/gdp/ao.html){target=\"_blank\"}\n| **Next section &rarr;** | üòê **Medium** | Data loaded dynamically via API | \"View Source\", find API call, scrape programmatically | [<i class=\"bi bi-box-arrow-up-right\"></i>](https://archive.org/details/killinghopeusmil0000blum/page/n3/mode/2up){target=\"_blank\"}\n| **Future weeks &rarr;** | üò≥ **Hard** | Data loaded dynamically [internally] via web framework | Use <a href=\"https://www.selenium.dev/\" target=\"_blank\">Selenium</a> | [<i class=\"bi bi-box-arrow-up-right\"></i>](https://www.google.com/books/edition/Killing_Hope/-IbQvd13uToC?hl=en&gbpv=1&dq=killing%20hope&pg=PA215&printsec=frontcover){target=\"_blank\"}\n\n: {tbl-colwidths=\"[30,20,30,30,5]\"}\n\n## Scraping HTML with `requests` and BeautifulSoup {.smaller .small-title}\n\n<a href=\"https://requests.readthedocs.io/en/latest/\" target=\"_blank\">`requests` Documentation</a> | <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=\"_blank\">BeautifulSoup Documentation</a>\n\n\n\n::: {.cell layout-align=\"center\" reticulate='false'}\n\n```{.python .cell-code  code-fold=\"show\"}\n# Get HTML\nimport requests\n# Perform request\nresponse = requests.get(\"https://en.wikipedia.org/wiki/Data_science\")\n# Parse HTML\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.text, 'html.parser')\nall_headers = soup.find_all(\"h2\")\nsection_headers = [h.find(\"span\", {'class': 'mw-headline'}).text for h in all_headers[1:]]\nsection_headers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Foundations', 'Etymology', 'Data Science and Data Analysis', 'History', 'See also', 'References']\n```\n:::\n:::\n\n\n\n\n## Navigating HTML with BeautifulSoup\n\n* Let's focus on this line from the previous slide:\n\n```python\nall_headers = soup.find_all(\"h2\")\n```\n\n* `find_all()` is the **key function** for scraping!\n* If the HTML has a **repeating structure** (like rows in a table), `find_all()` can instantly parse this structure into a Python list.\n\n## The Power of `find_all()` {.smaller}\n\n::: {layout=\"[[1,1],[0.75,1]]\"}\n\n::: {#fig-page-html}\n\n```html {filename=\"data_page.html\"}\n<div class=\"all-the-data\">\n    <h4>First Dataset</h4>\n    <div class=\"data-1\">\n        <div class=\"dataval\">1</div>\n        <div class=\"dataval\">2</div>\n        <div class=\"dataval\">3</div>\n    </div>\n    <h4>Second Dataset</h4>\n    <div class=\"data-2\">\n        <ul>\n            <li>4.0</li>\n            <li>5.5</li>\n            <li>6.7</li>\n        </ul>\n    </div>\n</div>\n```\n\nData in page elements (`<div>`, `<li>`)\n:::\n\n::: {#fig-page-rendered fig-align=\"left\"}\n\n\n\n```{=html}\n<div class=\"all-the-data\" style=\"margin-left: 16px !important; font-size: 1.3rem !important;\">\n    <h4>First Dataset</h4>\n    <div class=\"data-1\">\n        <div class=\"datapoint\">1</div>\n        <div class=\"datapoint\">2</div>\n        <div class=\"datapoint\">3</div>\n    </div>\n    <h4>Second Dataset</h4>\n        <ul>\n            <li>4.0</li>\n            <li>5.5</li>\n            <li>6.7</li>\n        </ul>\n</div>\n```\n\n\n\nThe code from @fig-page-html, rendered by your browser\n:::\n\n::: {#fig-page-parse-code}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"left\"}\n\n```{.python .cell-code .code-overflow-wrap}\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(page_html, 'html.parser')\nds1_elt = soup.find(\"div\", class_='data-1')\nds1 = [e.text for e in ds1_elt.find_all(\"div\")]\nds2_elt = soup.find(\"div\", {'class': 'data-2'})\nds2 = [e.text for e in ds2_elt.find_all(\"li\")]\n```\n:::\n\n\n\nThe BeautifulSoup code used to parse the HTML\n:::\n\n::: {#fig-page-parsed}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nprint(f\"dataset-1: {ds1}\\ndataset-2: {ds2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndataset-1: ['1', '2', '3']\ndataset-2: ['4.0', '5.5', '6.7']\n```\n:::\n:::\n\n\n\nContents of the Python variables holding the parsed data\n:::\n\n:::\n\n## Parsing HTML Tables {.smaller}\n\n::: {layout=\"[[1,1],[1,1]]\"}\n\n::: {#fig-table-html}\n\n```html {filename=\"table_data.html\"}\n<table>\n<thead>\n    <tr>\n        <th>X1</th><th>X2</th><th>X3</th>\n    </tr>\n</thead>\n<tbody>\n    <tr>\n        <td>1</td><td>3</td><td>5</td>\n    </tr>\n    <tr>\n        <td>2</td><td>4</td><td>6</td>\n    </tr>\n</tbody>\n</table>\n```\n\n\nData in HTML table format\n:::\n\n::: {#fig-table-rendered}\n\n\n\n```{=html}\n<table>\n<thead>\n    <tr>\n        <th>X1</th><th>X2</th><th>X3</th>\n    </tr>\n</thead>\n<tbody>\n    <tr>\n        <td>1</td>\n        <td>3</td>\n        <td>5</td>\n    </tr>\n    <tr>\n        <td>2</td>\n        <td>4</td>\n        <td>6</td>\n    </tr>\n</tbody>\n</table>\n```\n\n\n\nThe HTML table code, as rendered by your browser\n:::\n\n::: {#fig-table-parse-code}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(table_html, 'html.parser')\nthead = soup.find(\"thead\")\nheaders = [e.text for e in thead.find_all(\"th\")]\ntbody = soup.find(\"tbody\")\nrows = tbody.find_all(\"tr\")\ndata = [[e.text for e in r.find_all(\"td\")]\n            for r in rows]\n```\n:::\n\n\n\nThe BeautifulSoup code used to parse the table HTML\n:::\n\n::: {#fig-table-parsed}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nprint(f\"headers: {headers}\\ndata: {data}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nheaders: ['X1', 'X2', 'X3']\ndata: [['1', '3', '5'], ['2', '4', '6']]\n```\n:::\n:::\n\n\n\nContents of the Python variables holding the parsed data\n:::\n\n:::\n\n# APIs {.small-title .not-title-slide data-stack-name=\"APIs\"}\n\n**A**pplication **P**rogramming **I**nterfaces: **developer-facing** part of data pipeline/service. **Abstracts unnecessary details**:\n\n| Example | Care about | Don't care about |\n| - | - | - |\n| Electrical outlet |  **Electricity** | Details of Alternating/Direct Currents |\n| Water fountain | **Water** | Details of how it's pumped into the fountain |\n| Car | **Accelerate**, **brake**, **reverse** | Details of combustion engine |\n\n: {tbl-colwidths=\"[25,25,50]\"}\n\n<!-- * Can accept **parameters** for more fine-tuned usage:\n\n| Example | Default | Options |\n| - | - | - |\n| Electrical outlet |  -->\n\n## What Does an API Do?\n\nExposes **endpoints** for use by developers, without requiring them to know the nuts and bolts of your pipeline/service:\n\n| Example | Endpoint | Not Exposed |\n| - | - | - |\n| Electrical outlet | **Socket** | Internal wiring |\n| Water fountain | **Aerator** | Water pump |\n| Car | **Pedals**, **Steering wheel**, etc. |Engine |\n\n: {tbl-colwidths=\"[25,50,25]\"}\n\n::: {.notes}\n\nWhen I'm teaching programming to students in refugee camps who may have never used a computer before, I try to use the idea of \"robots\": a program is a robot trained to sit there and wait for inputs, then process them in some way and spit out some output. APIs really capture this notion, honestly.\n\n:::\n\n## Example: Math API\n\n* Base URL: <a href=\"https://newton.vercel.app/api/v2/\" target=\"_blank\">`https://newton.vercel.app/api/v2/`</a>\n* The **endpoint**: `factor`\n* The **argument**: `\"x^2 - 1\"`\n* The **request**: <a href=\"https://newton.vercel.app/api/v2/factor/x^2-1\" target=\"_blank\">`https://newton.vercel.app/api/v2/factor/x^2-1`</a>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code  code-fold=\"show\"}\nimport requests\nresponse = requests.get(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nprint(response.json())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'operation': 'factor', 'expression': 'x^2-1', 'result': '(x - 1) (x + 1)'}\n```\n:::\n:::\n\n\n\n## Math API Endpoints {.smaller}\n\n| Operation | API Endpoint | Result |\n| - | - | - |\n| Simplify | `/simplify/2^2+2(2)` | `8` |\n| Factor | `/factor/x^2 + 2x` | `x (x + 2)` |\n| Derive | `/derive/x^2+2x` | `2 x + 2` |\n| Integrate | `/integrate/x^2+2x` | `1/3 x^3 + x^2 + C` |\n| Find 0's | `/zeroes/x^2+2x` | `[-2, 0]` |\n| Find Tangent | `/tangent/2|x^3` | `12 x + -16` |\n| Area Under Curve | `/area/2:4|x^3` | `60` |\n| Cosine | `/cos/pi` | `-1` |\n| Sine | `/sin/0` | `0` |\n| Tangent | `/tan/0` | `0` |\n\n<!-- | Inverse Cosine | `/arccos/1` | `0` |\nInverse Sine \t/arcsin/0 \t0\nInverse Tangent \t/arctan/0 \t0\nAbsolute Value \t/abs/-1 \t1\nLogarithm \t/log/2l8 \t3 -->\n\n\n## Authentication\n\n* Unlike the math API, most APIs do not allow requests to be made by **anonymous** requesters, and require **authentication**.\n* For example, you can access **public GitHub repos** anonymously, but to access **private GitHub repos** using GitHub's API, you'll need to authenticate that you are in fact the one making the request\n\n\n## Authentication via `PyGithub` {.smaller}\n\n::: {.callout-tip icon=\"false\" title=\"`PyGithub` Installation\"}\n\nInstall using the following terminal/shell command [[Documentation]](https://github.com/PyGithub/PyGithub){target=\"_blank\" style=\"margin-left: 8px;\"}\n\n```bash\npip install PyGithub\n```\n\n:::\n\n`PyGithub` can handle authentication for you. Example: <a href=\"https://github.com/jpowerj/private-repo-test/\" target=\"_blank\">this private repo</a> in my account does **not** show up unless the request is **authenticated** (via a Personal Access Token)[^security]:\n\n::: {layout-ncol=2}\n\n\n::: {#fig-github-noauth}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# import github\n# g = github.Github()\n# try:\n#   g.get_repo(\"jpowerj/private-repo-test\")\n# except Exception as e:\n#   print(e)\n```\n:::\n\n\n\nUsing the GitHub API **without** authentication\n:::\n\n::: {#fig-gh-auth}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Load the access token securely\n# import os\n# my_access_token = os.getenv('GITHUB_TOKEN')\n# import github\n# # Use the access token to make an API request\n# auth = github.Auth.Token(my_access_token)\n# g = github.Github(auth=auth)\n# g.get_user().get_repo(\"private-repo-test\")\n```\n:::\n\n\n\nUsing the GitHub API **with** authentication\n:::\n:::\n\n[^security]: Your code should **üö®neverüö®** contain authentication info, especially when using GitHub. In this case, I created an OS **environment variable** called `GITHUB_TOKEN` containing my Personal Access Token, which I then loaded using `os.getenv()` and provided to `PyGithub`.\n\n## References {.smaller}\n\n::: {#refs}\n:::\n\n<!-- R APPENDIX -->\n\n# Appendix: Code Examples in R\n\n## Scraping HTML with `httr2` and `xml2`\n\n<a href=\"https://httr2.r-lib.org/\" target=\"_blank\">`httr2` Documentation</a> | <a href=\"https://xml2.r-lib.org/\" target=\"_blank\">`xml2` Documentation</a>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\n# Get HTML\nlibrary(httr2)\nrequest_obj <- request(\"https://en.wikipedia.org/wiki/Data_science\")\nresponse_obj <- req_perform(request_obj)\n# Parse HTML\nlibrary(xml2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'xml2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:httr2':\n\n    url_parse\n```\n:::\n\n```{.r .cell-code  code-fold=\"show\"}\nhtml_obj <- response_obj %>% resp_body_html()\nhtml_obj %>% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (6)}\n[1] <span class=\"mw-headline\" id=\"Foundations\">Foundations</span>\n[2] <span class=\"mw-headline\" id=\"Etymology\">Etymology</span>\n[3] <span class=\"mw-headline\" id=\"Data_Science_and_Data_Analysis\">Data Scienc ...\n[4] <span class=\"mw-headline\" id=\"History\">History</span>\n[5] <span class=\"mw-headline\" id=\"See_also\">See also</span>\n[6] <span class=\"mw-headline\" id=\"References\">References</span>\n```\n:::\n:::\n\n\n\n::: {.aside}\n\nNote: `httr2` is a re-written version of the original `httr` package, which is now deprecated. You'll still see lots of code using `httr`, however, so it's good to know how both versions work. <a href=\"https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html\" target=\"_blank\">Click here for a helpful vignette</a> on the original `httr` library.\n\n:::\n\n## Navigating HTML with XPath\n\n<a href=\"https://devhints.io/xpath\" target=\"_blank\">XPath Cheatsheet</a>\n\n* Notice the last line on the previous slide:\n\n```r\nhtml_obj %>% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n```\n\n* The string passed to `xml_find_all()` is an **XPath selector**\n\n::: {.aside}\n\nXPath selectors are used by many different libraries, including **Selenium** (which we'll look at very soon) and **jQuery** (a standard extension to plain JavaScript allowing easy searching/manipulation of the DOM), so it's good to learn it now!\n\n:::\n\n## XPath I: Selecting Elements\n\n```html {filename=\"mypage.html\"}\n<div class=\"container\">\n  <h1>Header</h1>\n  <p id=\"page-content\">Content</p>\n  <img class=\"footer-image m-5\" src=\"footer.png\">\n</div>\n```\n\n* `'//div'` matches all elements `<div>` in the document:\n\n    ```html\n    <div class=\"container\">\n      <h1>Header</h1>\n      <p id=\"page-content\">Content</p>\n      <img class=\"footer-image m-5\" src=\"footer.png\">\n    </div>\n    ```\n* `'//div//img'` matches `<img>` elements which are **children of** `<div>` elements:\n\n    ```html\n    <img class=\"footer-image m-5\" src=\"footer.png\">\n    ```\n\n## XPath II: Filtering by Attributes {.smaller}\n\n\n\n```html {filename=\"mypage.html\"}\n<div class=\"container\">\n  <h1>Header</h1>\n  <p id=\"page-content\">Content</p>\n  <img class=\"footer-image m-5\" src=\"footer.png\">\n</div>\n```\n\n* `'//p[id=\"page-content\"]'` matches all `<p>` elements with id `page-content`[^unique-id]:\n\n    ```html\n    <p id=\"page-content\">Content</p>\n    ```\n* Matching **classes** is a bit trickier:\n\n    [`'//img[contains(concat(\" \", normalize-space(@class), \" \"), \" footer-image \")]'`]{.small-codeblock}\n\n    matches all `<img>` elements with `page-content` as one of their classes[^multi-class]\n\n    ```html\n    <img class=\"footer-image m-5\" src=\"footer.png\">\n    ```\n\n[^unique-id]: In HTML, `id`s are required to be **unique** to particular elements (and elements cannot have more than one `id`), meaning that this should only return a **single** element, for valid HTML code (not followed by all webpages!). Also note the **double-quotes** after `id=`, which are required in XPath.\n\n[^multi-class]: Your intuition may be to just use `'//img[@class=\"footer-image\"]'`. Sadly, however, this will match only elements with `footer-image` as their **only** class. i.e., it will match `<img class=\"footer-image\">` but not `<img class=\"footer-image another-class\">`. This will usually fail, since most elements on modern webpages have several classes. For example, if the site is using <a href=\"https://getbootstrap.com/docs/5.3/getting-started/introduction/\" target=\"_blank\">Bootstrap</a>, `<p class=\"p-5 m-3\"></p>` creates a paragraph element with a padding of 5 pixels and a margin of 3 pixels.\n\n## Example: Math API\n\n* Base URL: <a href=\"https://newton.vercel.app/api/v2/\" target=\"_blank\">`https://newton.vercel.app/api/v2/`</a>\n* The **endpoint**: `factor`\n* The **argument**: `\"x^2 - 1\"`\n* The **request**: <a href=\"https://newton.vercel.app/api/v2/factor/x^2-1\" target=\"_blank\">`https://newton.vercel.app/api/v2/factor/x^2-1`</a>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(httr2)\nrequest_obj <- request(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nresponse_obj <- req_perform(request_obj)\nwriteLines(response_obj %>% resp_body_string())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{\"operation\":\"factor\",\"expression\":\"x^2-1\",\"result\":\"(x - 1) (x + 1)\"}\n```\n:::\n:::\n\n\n\n## Authentication\n\n* Most APIs don't allow requests to be made by anonymous requesters, and require **authentication**.\n* For example, to access private GitHub repos using GitHub's API, you'll need to authenticate that you are in fact the one making the request\n\n## Authentication via `GH`\n\n* The `GH` library for `R` can handle this authentication process for you. For example, <a href=\"https://github.com/jpowerj/private-repo-test/\" target=\"_blank\">this private repo</a> in my account does not show up if requested anonymously, but does show up when requested using `GH` with a Personal Access Token[^security-r]:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nlibrary(gh)\nresult <- gh(\"GET /repos/jpowerj/private-repo-test\")\nwriteLines(paste0(result$name, \": \",result$description))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nprivate-repo-test: Private repo example for DSAN5000\n```\n:::\n:::\n\n\n\n&nbsp;\n\n[^security-r]: Your code should **never** contain authentication info, especially when using GitHub. In this case, I created an OS environment variable called `GITHUB_TOKEN` containing my Personal Access Token, which `GH` then uses to make authenticated requests.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.2/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.29/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.4/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.4/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.4/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/dt-ext-fixedcolumns-1.13.4/css/fixedColumns.dataTables.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-ext-fixedcolumns-1.13.4/js/dataTables.fixedColumns.min.js\"></script>\n<link href=\"../site_libs/dt-ext-fixedheader-1.13.4/css/fixedHeader.dataTables.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-ext-fixedheader-1.13.4/js/dataTables.fixedHeader.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}