[
  {
    "objectID": "w06/slides.html#nlp-recap",
    "href": "w06/slides.html#nlp-recap",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "NLP Recap",
    "text": "NLP Recap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Excerpts from two data science textbooks, plus another book\n\n\n\n\n\n\n\n\n\n\ndoc_id\ntext\ntexts\nKékkek\nvoice\n\n\n\n\n\n0\n0\n6\n0\n1\n\n\n\n1\n0\n0\n3\n1\n\n\n\n2\n6\n0\n0\n0\n\n\n\n\nFigure 2: The Document-Term Matrix (DTM)\n\n\n\n\n\n\n\n\n\n \n\n\n\ndoc_id\ntext\nkekkek\nvoice\n\n\n\n\n\n\n0\n6\n0\n1\n\n\n\n\n1\n0\n3\n1\n\n\n\n\n2\n6\n0\n0\n\n\n\n\n\nFigure 3: The cleaned DTM, after lowercasing, lemmatization, and unicode standardization"
  },
  {
    "objectID": "w06/slides.html#your-nlp-toolbox",
    "href": "w06/slides.html#your-nlp-toolbox",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Your NLP Toolbox",
    "text": "Your NLP Toolbox\n\nProcesses like lowercasing and stemming allowed the computer to recognize that text and texts should be counted together in this context, since they refer to the same semantic concept.\nAs we learn NLP, we’ll develop a “toolbox” of ideas, algorithms, and tasks allowing us to quantify, clean, and analyzing text data, where each tool will help us at some level/stage of this analysis:\n\nGathering texts\nPreprocessing\nLearning (e.g., estimating parameters for a model) about the texts\nApplying what we learned to downstream tasks we’d like to solve"
  },
  {
    "objectID": "w06/slides.html#the-items-in-our-toolbox",
    "href": "w06/slides.html#the-items-in-our-toolbox",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Items In Our Toolbox",
    "text": "The Items In Our Toolbox\n\n\n\n\n\nCorpus Construction\n\n• Corpus: The collection of documents you’re hoping to analyze\n• Books, articles, posts, emails, tweets, etc.\n\n\n\n\nCorpus/Document Level NLP\n\n• Vocabulary: The collection of unique tokens across all documents in your corpus\n• Segmentation: Breaking a document into parts (paragraphs and/or sentences)\n\n↓Sentence/Word Level NLP\n\n• Tokenization: Break sentence into tokens\n• Stopword Removal: Removing non-semantic (syntactic) tokens like “the”, “and”\n• Stemming: Naïvely (but quickly) “chopping off” ends of tokens (e.g., plural → singular)\n• Lemmatization: Algorithmically map tokens to linguistic roots (slower than stemming)\n\n\n\n\nVectorization\n\nTransform textual representation into numeric representation, like the DTM\n\n\n↓\n\n\n\nDownstream Tasks\n\n• Text classification\n• Named entity recognition\n• Sentiment analysis"
  },
  {
    "objectID": "w06/slides.html#tidyverse",
    "href": "w06/slides.html#tidyverse",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nThink of data science tasks as involving pipelines:\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_00\n\n Data-Processing Pipeline 1   \n\nraw\n\n Raw Data   \n\ntr1\n\n Transformation A (select(), filter())   \n\nraw-&gt;tr1\n\n    \n\ntr2\n\n Transformation B (mutate(), summarize())   \n\ntr1-&gt;tr2\n\n    \n\nviz\n\n Visualization   \n\ntr2-&gt;viz\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_00\n\n Data-Processing Pipeline 2   \n\nraw\n\n Raw Data   \n\ntr1\n\n Transformation C (select(), filter())   \n\nraw-&gt;tr1\n\n    \n\ntr2\n\n Transformation D (mutate(), summarize())   \n\ntr1-&gt;tr2\n\n    \n\nviz\n\n       Result        \n\ntr2-&gt;viz\n\n   \n\n\n\n\n\n\nTidyverse lets you pipe output from one transformation as the input to another:\n\nraw_data |&gt; select() |&gt; mutate() |&gt; visualize()\nraw_data |&gt; filter() |&gt; summarize() |&gt; check_result()"
  },
  {
    "objectID": "w06/slides.html#selecting-columns",
    "href": "w06/slides.html#selecting-columns",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nselect() lets you keep only the columns you care about in your current analysis:\n\n\n\n\nCode\nlibrary(tidyverse)\ntable1\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\nCode\ntable1 |&gt; select(country, year, population)\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nAfghanistan\n1999\n19987071\n\n\nAfghanistan\n2000\n20595360\n\n\nBrazil\n1999\n172006362\n\n\nBrazil\n2000\n174504898\n\n\nChina\n1999\n1272915272\n\n\nChina\n2000\n1280428583"
  },
  {
    "objectID": "w06/slides.html#filtering-rows",
    "href": "w06/slides.html#filtering-rows",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Filtering Rows",
    "text": "Filtering Rows\nfilter() lets you keep only the rows you care about in your current analysis:\n\n\n\n\nCode\ntable1 |&gt; filter(year == 2000)\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\nCode\ntable1 |&gt; filter(country == \"Afghanistan\")\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360"
  },
  {
    "objectID": "w06/slides.html#merging-data",
    "href": "w06/slides.html#merging-data",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Merging Data",
    "text": "Merging Data\n\nThe task: Analyze relationship between population and GDP (in 2000)\nThe data: One dataset on population in 2000, another on GDP in 2000\nLet’s get the data ready for merging using R\n\n\n\n\n\nCode\ndf &lt;- table1 |&gt;\n  select(country, year, population) |&gt;\n  filter(year == 2000)\ndf |&gt; write_csv(\"assets/pop_2000.csv\")\ndf\n\n\n\n\n\n\ncountry\nyear\npopulation\n\n\n\n\nAfghanistan\n2000\n20595360\n\n\nBrazil\n2000\n174504898\n\n\nChina\n2000\n1280428583\n\n\n\n\n\n\n\n\n\nCode\ngdp_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/c83e87f61c166dea8ba7e4453f08a404/raw/29b03e6320bc3ffc9f528c2ac497a21f2d801c00/gdp_2000_2010.csv\")\ngdp_df |&gt; head(5)\n\n\n\n\n\n\nCountry Name\nCountry Code\nYear\nValue\n\n\n\n\nAfghanistan\nAFG\n2010\n15936800636\n\n\nAlbania\nALB\n2000\n3632043908\n\n\nAlbania\nALB\n2010\n11926953259\n\n\nAlgeria\nDZA\n2000\n54790245601\n\n\nAlgeria\nDZA\n2010\n161207268655"
  },
  {
    "objectID": "w06/slides.html#selectingfiltering-in-action",
    "href": "w06/slides.html#selectingfiltering-in-action",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Selecting/Filtering in Action",
    "text": "Selecting/Filtering in Action\n\n\nCode\ngdp_2000_df &lt;- gdp_df |&gt;\n  select(`Country Name`,Year,Value) |&gt;\n  filter(Year == \"2000\") |&gt;\n  rename(country=`Country Name`, year=`Year`, gdp=`Value`)\ngdp_2000_df |&gt; write_csv(\"assets/gdp_2000.csv\")\ngdp_2000_df |&gt; head()\n\n\n\n\n\n\ncountry\nyear\ngdp\n\n\n\n\nAlbania\n2000\n3632043908\n\n\nAlgeria\n2000\n54790245601\n\n\nAndorra\n2000\n1434429703\n\n\nAngola\n2000\n9129594819\n\n\nAntigua and Barbuda\n2000\n830158769\n\n\nArgentina\n2000\n284203750000"
  },
  {
    "objectID": "w06/slides.html#recommended-language-python",
    "href": "w06/slides.html#recommended-language-python",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Recommended Language: Python",
    "text": "Recommended Language: Python\nPandas provides an easy-to-use df.merge(other_df)!\n\n\n\nLeft Join\n\n\n\nCode\nmerged_df = pop_df.merge(gdp_df,\n  on='country', how='left', indicator=True\n)\nMarkdown(merged_df.to_markdown())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear_x\npopulation\nyear_y\ngdp\n_merge\n\n\n\n\n0\nAfghanistan\n2000\n20595360\nnan\nnan\nleft_only\n\n\n1\nBrazil\n2000\n174504898\n2000\n6.55421e+11\nboth\n\n\n2\nChina\n2000\n1280428583\n2000\n1.21135e+12\nboth\n\n\n\n\n\n\n\nInner join (≈ Intersection (\\(\\cap\\)))\n\n\n\nCode\nmerged_df = pop_df.merge(gdp_df,\n  on='country', how='inner', indicator=True\n)\nMarkdown(merged_df.to_markdown())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear_x\npopulation\nyear_y\ngdp\n_merge\n\n\n\n\n0\nBrazil\n2000\n174504898\n2000\n6.55421e+11\nboth\n\n\n1\nChina\n2000\n1280428583\n2000\n1.21135e+12\nboth"
  },
  {
    "objectID": "w06/slides.html#reshaping-data",
    "href": "w06/slides.html#reshaping-data",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Reshaping Data",
    "text": "Reshaping Data\nSometimes you can’t merge because one of the datasets looks like the table on the left, but we want it to look like the table on the right\n\n\nIn data-cleaning jargon, this dataset is long (more than one row per observation)\n\n\nCode\ntable2 |&gt; write_csv(\"assets/long_data.csv\")\ntable2 |&gt; head()\n\n\n\n\n\n\ncountry\nyear\ntype\ncount\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\n\n\n\n\n\nIn data-cleaning jargon, this dataset is wide (one row per obs; usually tidy)\n\n\nCode\ntable1 |&gt; write_csv(\"assets/wide_data.csv\")\ntable1 |&gt; head()\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583"
  },
  {
    "objectID": "w06/slides.html#reshaping-long-to-wide-in-python-pd.pivot",
    "href": "w06/slides.html#reshaping-long-to-wide-in-python-pd.pivot",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Reshaping Long-to-Wide in Python: pd.pivot()",
    "text": "Reshaping Long-to-Wide in Python: pd.pivot()\n\n\nCreate unique ID for wide version:\n\n\nCode\nlong_df['id'] = long_df['country'] + '_' + long_df['year'].apply(str)\n# Reorder the columns, so it shows the id first\nlong_df = long_df[['id','country','year','type','count']]\ndisp(long_df.head(6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry\nyear\ntype\ncount\n\n\n\n\n0\nAfghanistan_1999\nAfghanistan\n1999\ncases\n745\n\n\n1\nAfghanistan_1999\nAfghanistan\n1999\npopulation\n19987071\n\n\n2\nAfghanistan_2000\nAfghanistan\n2000\ncases\n2666\n\n\n3\nAfghanistan_2000\nAfghanistan\n2000\npopulation\n20595360\n\n\n4\nBrazil_1999\nBrazil\n1999\ncases\n37737\n\n\n5\nBrazil_1999\nBrazil\n1999\npopulation\n172006362\n\n\n\n\n\n\n\n\nCode\nreshaped_df = pd.pivot(long_df,\n  index='id',\n  columns='type',\n  values='count'\n)\ndisp(reshaped_df)\n\n\n\n\n\nid\ncases\npopulation\n\n\n\n\nAfghanistan_1999\n745\n1.99871e+07\n\n\nAfghanistan_2000\n2666\n2.05954e+07\n\n\nBrazil_1999\n37737\n1.72006e+08\n\n\nBrazil_2000\n80488\n1.74505e+08\n\n\nChina_1999\n212258\n1.27292e+09\n\n\nChina_2000\n213766\n1.28043e+09"
  },
  {
    "objectID": "w06/slides.html#the-other-direction-wide-to-long-pd.melt",
    "href": "w06/slides.html#the-other-direction-wide-to-long-pd.melt",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Other Direction (Wide-to-Long): pd.melt()",
    "text": "The Other Direction (Wide-to-Long): pd.melt()\n\n\n\n\nCode\nwide_df = pd.read_csv(\"assets/wide_data.csv\")\ndisp(wide_df)\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\n0\nAfghanistan\n1999\n745\n19987071\n\n\n1\nAfghanistan\n2000\n2666\n20595360\n\n\n2\nBrazil\n1999\n37737\n172006362\n\n\n3\nBrazil\n2000\n80488\n174504898\n\n\n4\nChina\n1999\n212258\n1272915272\n\n\n5\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\nCode\nlong_df = pd.melt(wide_df,\n  id_vars=['country','year'],\n  value_vars=['cases','population']\n)\ndisp(long_df.head(6))\n\n\n\n\n\n\ncountry\nyear\nvariable\nvalue\n\n\n\n\n0\nAfghanistan\n1999\ncases\n745\n\n\n1\nAfghanistan\n2000\ncases\n2666\n\n\n2\nBrazil\n1999\ncases\n37737\n\n\n3\nBrazil\n2000\ncases\n80488\n\n\n4\nChina\n1999\ncases\n212258\n\n\n5\nChina\n2000\ncases\n213766"
  },
  {
    "objectID": "w06/slides.html#wide-to-long-in-r-gather",
    "href": "w06/slides.html#wide-to-long-in-r-gather",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Wide-to-Long in R: gather()",
    "text": "Wide-to-Long in R: gather()\n\n\n\n\nCode\ntable1\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\n\nCode\nlong_df &lt;- gather(table1,\n  key = \"variable\",\n  value = cases,\n  -c(country, year)\n)\nlong_df |&gt; head()\n\n\n\n\n\n\ncountry\nyear\nvariable\ncases\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n2000\ncases\n80488\n\n\nChina\n1999\ncases\n212258\n\n\nChina\n2000\ncases\n213766"
  },
  {
    "objectID": "w06/slides.html#exploratory-data-analysis-eda",
    "href": "w06/slides.html#exploratory-data-analysis-eda",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nIn contrast to confirmatory data analysis\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_understand\n\n Understand  \n\ncluster_expl\n\n Exploratory  \n\ncluster_conf\n\n Confirmatory  \n\ncluster_clean\n\n Clean  \n\ncluster_tidy\n\n    \n\ncluster_import\n\n     \n\nImport\n\n Import   \n\nTidy\n\n Tidy   \n\nImport-&gt;Tidy\n\n    \n\nimportLibs\n\n R:  read_csv() Python:  pd.read_csv()   \n\nV\n\n Visualize   \n\nTidy-&gt;V\n\n    \n\ntidyLibraries\n\n R:  tidyverse Python:  Pandas   \n\nModel\n\n Model   \n\nV-&gt;Model\n\n    \n\nvizLibraries\n\n R:  ggplot2 Python:  seaborn   \n\nModel-&gt;V\n\n    \n\nCommunicate\n\n Communicate   \n\nModel-&gt;Communicate\n\n    \n\nconfLibraries\n\n R:  e1071 Python:  scikit-learn"
  },
  {
    "objectID": "w06/slides.html#exploratory-data-analysis-eda-1",
    "href": "w06/slides.html#exploratory-data-analysis-eda-1",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nSo you have reasonably clean data… now what?\n\n\n(Image source: Oldies but Goldies: Statistical Graphics Books)"
  },
  {
    "objectID": "w06/slides.html#what-is-eda",
    "href": "w06/slides.html#what-is-eda",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "What is EDA?",
    "text": "What is EDA?\nFrom IBM:\n\nLook at data before making any assumptions1\nScreen data and identify obvious errors\nBetter understand patterns within the data\nDetect outliers or anomalous events\nFind interesting relations among the variables.\n\nOverarching goal to keep in mind: does this data have what I need to address my research question?\n(See next slide)"
  },
  {
    "objectID": "w06/slides.html#assumption-free-analysis",
    "href": "w06/slides.html#assumption-free-analysis",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Assumption-Free Analysis?",
    "text": "Assumption-Free Analysis?\n\n\nImportant question for EDA: Is it actually possible to analyze data without assumptions?1\n\nEmpirical results are laden with values and theoretical commitments.(Boyd and Bogen 2021)\n\n\nEx: Estimates of optimal tax rates in Econ journals vs. Economist ideology scores\n\n\n\n\n\nComputed based on Jelveh, Kogut, and Naidu (2023)\n\n\n\n\n\n\n(When I started as a PhD student in Computer Science, I would have answered “yes”. Now that my brain has been warped by social science, I’m not so sure.)"
  },
  {
    "objectID": "w06/slides.html#statistical-eda",
    "href": "w06/slides.html#statistical-eda",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Statistical EDA",
    "text": "Statistical EDA\n\nIterative process: Ask questions of the data, find answers, generate more questions\nYou’re probably already used to Mean and Variance: Fancier EDA/robustness methods build upon these two!\nWhy do we need to visualize? Can’t we just use mean, \\(R^2\\)?\n…Enter Anscombe’s Quartet\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\")\n# https://towardsdatascience.com/how-to-use-your-own-color-palettes-with-seaborn-a45bf5175146\nsns.set_palette(sns.color_palette(cb_palette))\n\n# Load the example dataset for Anscombe's quartet\nanscombe_df = sns.load_dataset(\"anscombe\")\n#print(anscombe_df)\n\n# Show the results of a linear regression within each dataset\nanscombe_plot = sns.lmplot(\n    data=anscombe_df, x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\",\n    col_wrap=4, palette=\"muted\", ci=None,\n    scatter_kws={\"s\": 50, \"alpha\": 1},\n    height=3\n);\nanscombe_plot;"
  },
  {
    "objectID": "w06/slides.html#the-scariest-dataset-of-all-time",
    "href": "w06/slides.html#the-scariest-dataset-of-all-time",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Scariest Dataset of All Time",
    "text": "The Scariest Dataset of All Time\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\n\n\nCode\n# Compute dataset means\nmy_round = lambda x: round(x,2)\ndata_means = anscombe_df.groupby('dataset').agg(\n  x_mean = ('x', np.mean),\n  y_mean = ('y', np.mean)\n).apply(my_round)\ndisp(data_means, floatfmt='.2f')\n\n\n\n\n\ndataset\nx_mean\ny_mean\n\n\n\n\nI\n9.00\n7.50\n\n\nII\n9.00\n7.50\n\n\nIII\n9.00\n7.50\n\n\nIV\n9.00\n7.50\n\n\n\n\n\nFigure 4: Column means for each dataset\n\n\n\n\n\n\nCode\n# Compute dataset SDs\ndata_sds = anscombe_df.groupby('dataset').agg(\n  x_mean = ('x', np.std),\n  y_mean = ('y', np.std),\n).apply(my_round)\ndisp(data_sds, floatfmt='.2f')\n\n\n\n\n\ndataset\nx_mean\ny_mean\n\n\n\n\nI\n3.32\n2.03\n\n\nII\n3.32\n2.03\n\n\nIII\n3.32\n2.03\n\n\nIV\n3.32\n2.03\n\n\n\n\n\nFigure 5: Column SDs for each dataset\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelations\n\n\n\n\n\nCode\nimport tabulate\nfrom IPython.display import HTML\ncorr_matrix = anscombe_df.groupby('dataset').corr().apply(my_round)\n#Markdown(tabulate.tabulate(corr_matrix))\nHTML(corr_matrix.to_html())\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\n\nI\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\nII\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\nIII\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\nIV\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\n\n\n\nFigure 6: Correlation between \\(x\\) and \\(y\\) for each dataset"
  },
  {
    "objectID": "w06/slides.html#it-doesnt-end-there",
    "href": "w06/slides.html#it-doesnt-end-there",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "It Doesn’t End There…",
    "text": "It Doesn’t End There…\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.formula.api as smf\nsummary_dfs = []\nfor cur_ds in ['I','II','III','IV']:\n  ds1_df = anscombe_df.loc[anscombe_df['dataset'] == \"I\"].copy()\n  # Fit regression model (using the natural log of one of the regressors)\n  results = smf.ols('y ~ x', data=ds1_df).fit()\n  # Get R^2\n  rsq = round(results.rsquared, 2)\n  # Inspect the results\n  summary = results.summary()\n  summary.extra_txt = None\n  summary_df = summary_to_df(summary, corner_col = f'Dataset {cur_ds}&lt;br&gt;R^2 = {rsq}')\n  summary_dfs.append(summary_df)\ndisp(summary_dfs[0], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77\n\n\n\n\n\n\n\nCode\ndisp(summary_dfs[1], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IIR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77\n\n\n\n\n\n\n\n\n\nCode\ndisp(summary_dfs[2], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IIIR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77\n\n\n\n\n\n\n\nCode\ndisp(summary_dfs[3], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IVR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77"
  },
  {
    "objectID": "w06/slides.html#normalization",
    "href": "w06/slides.html#normalization",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Normalization",
    "text": "Normalization\n\n\n\nUnnormalized World\n\n\n\nCode\nnum_students &lt;- 30\nstudent_ids &lt;- seq(from = 1, to = num_students)\n# So we have the censored Normal pdf/cdf\nlibrary(crch)\ngen_test_scores &lt;- function(min_pts, max_pts) {\n  score_vals_unif &lt;- runif(num_students, min_pts, max_pts)\n  unif_mean &lt;- mean(score_vals_unif)\n  unif_sd &lt;- sd(score_vals_unif)\n  # Resample, this time censored normal dist\n  score_vals &lt;- round(rcnorm(num_students, mean=unif_mean, sd=unif_sd, left=min_pts, right=max_pts), 2)\n  return(score_vals)\n}\n# Test 1\nt1_min &lt;- 0\nt1_max &lt;- 268.3\nt1_score_vals &lt;- gen_test_scores(t1_min, t1_max)\nt1_mean &lt;- mean(t1_score_vals)\nt1_sd &lt;- sd(t1_score_vals)\nget_t1_pctile &lt;- function(s) round(100 * ecdf(t1_score_vals)(s), 1)\n# Test 2\nt2_min &lt;- -1\nt2_max &lt;- 1.2\nt2_score_vals &lt;- gen_test_scores(t2_min, t2_max)\nt2_mean &lt;- mean(t2_score_vals)\nt2_sd &lt;- sd(t2_score_vals)\nget_t2_pctile &lt;- function(s) round(100 * ecdf(t2_score_vals)(s), 1)\nscore_df &lt;- tibble(\n  id=student_ids,\n  t1_score=t1_score_vals,\n  t2_score=t2_score_vals\n)\nscore_df &lt;- score_df |&gt; arrange(desc(t1_score))\n\n\n“I got a 238.25 on the first test!” 🤩\n“But only a 0.31 on the second” 😭\n\n\n\n\n\n\nid\nt1_score\nt2_score\n\n\n\n\n17\n268.30\n-0.54\n\n\n27\n258.44\n-0.33\n\n\n26\n245.86\n-0.55\n\n\n5\n238.25\n0.31\n\n\n11\n206.54\n-0.02\n\n\n16\n205.49\n-0.06\n\n\n\n\n\n\n\n\nNormalized World\n\n\n\nCode\nscore_df &lt;- score_df |&gt;\n  mutate(\n    t1_z_score = round((t1_score - t1_mean) / t1_sd, 2),\n    t2_z_score = round((t2_score - t2_mean) / t2_sd, 2),\n    t1_pctile = get_t1_pctile(t1_score),\n    t2_pctile = get_t2_pctile(t2_score)\n  ) |&gt;\n  relocate(t1_pctile, .after = t1_score) |&gt;\n  relocate(t2_pctile, .after = t2_score)\n\n\n“I scored higher than 90% of students on the first test! 🤩\n“And higher than 60% on the second!” 😎\n\n\n\n\n\n\nid\nt1_score\nt1_pctile\nt2_score\nt2_pctile\nt1_z_score\nt2_z_score\n\n\n\n\n17\n268.30\n100.0\n-0.54\n30.0\n1.87\n-0.82\n\n\n27\n258.44\n96.7\n-0.33\n46.7\n1.73\n-0.52\n\n\n26\n245.86\n93.3\n-0.55\n26.7\n1.54\n-0.83\n\n\n5\n238.25\n90.0\n0.31\n60.0\n1.44\n0.39\n\n\n11\n206.54\n86.7\n-0.02\n56.7\n0.98\n-0.08\n\n\n16\n205.49\n83.3\n-0.06\n50.0\n0.96\n-0.14"
  },
  {
    "objectID": "w06/slides.html#scaling",
    "href": "w06/slides.html#scaling",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Scaling",
    "text": "Scaling\nThe percentile places everyone at evenly-spaced intervals from 0 to 100:\n\n\nCode\n# https://community.rstudio.com/t/number-line-in-ggplot/162894/4\n# Add a binary indicator to track \"me\" (student #8)\nwhoami &lt;- 29\nscore_df &lt;- score_df |&gt;\n  mutate(is_me = as.numeric(id == whoami))\nlibrary(ggplot2)\nt1_line_data &lt;- tibble(\n  x = score_df$t1_pctile,\n  y = 0,\n  me = score_df$is_me\n)\nggplot(t1_line_data, aes(x, y, col=factor(me), shape=factor(me))) +\n  geom_point(aes(size=g_pointsize)) +\n  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +\n  scale_color_discrete(c(0,1)) +\n  dsan_theme(\"half\") +\n  theme(\n    legend.position=\"none\", \n    #rect = element_blank(),\n    #panel.grid = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y=element_blank(),\n    panel.spacing = unit(0, \"mm\"),\n    plot.margin = margin(-35, 0, 0, 0, \"pt\"),\n  ) +\n  labs(\n    x = \"Test 1 Percentile\"\n  ) +\n  coord_fixed(ratio = 100)\n\n\n\n\n\n\n\n\n\n\nBut what if we want to see their absolute performance, on a 0 to 100 scale?\n\n\nCode\nlibrary(scales)\nscore_df &lt;- score_df |&gt;\n  mutate(\n    t1_rescaled = rescale(\n      t1_score,\n      from = c(t1_min, t1_max),\n      to = c(0, 100)\n    ),\n    t2_rescaled = rescale(\n      t2_score,\n      from = c(t2_min, t2_max),\n      to = c(0, 100)\n    )\n  )\n# Place \"me\" last so that it gets plotted last\nt1_rescaled_line_data &lt;- tibble(\n  x = score_df$t1_rescaled,\n  y = 0,\n  me = score_df$is_me\n) |&gt; arrange(me)\nggplot(t1_rescaled_line_data, aes(x,y,col=factor(me), shape=factor(me))) +\n  geom_point(size=g_pointsize) +\n  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(0, 100)) +\n  theme(\n    legend.position=\"none\", \n    #rect = element_blank(),\n    #panel.grid = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y=element_blank(),\n    #panel.spacing = unit(0, \"mm\"),\n    #plot.margin = margin(-40, 0, 0, 0, \"pt\"),\n  ) +\n  labs(\n    x = \"Test 1 Score (Rescaled to 0-100)\"\n  ) +\n  coord_fixed(ratio = 100)"
  },
  {
    "objectID": "w06/slides.html#shifting-recentering",
    "href": "w06/slides.html#shifting-recentering",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Shifting / Recentering",
    "text": "Shifting / Recentering\n\nPercentiles tell us how the students did in terms of relative rankings\nRescaling lets us reinterpret the boundary points\nWhat about with respect to some absolute baseline? For example, how well they did relative to the mean \\(\\mu\\)?\n\n\\[\nx'_i = x_i - \\mu\n\\]\n\nBut we’re still “stuck” in units of the test: is \\(x'_i = 0.3\\) (0.3 points above the mean) “good”? What about \\(x'_j = -2568\\) (2568 points below the mean)? How “bad” is this case?"
  },
  {
    "objectID": "w06/slides.html#shifting-and-scaling-the-z-score",
    "href": "w06/slides.html#shifting-and-scaling-the-z-score",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Shifting and Scaling: The \\(z\\)-Score",
    "text": "Shifting and Scaling: The \\(z\\)-Score\n\nEnter the \\(z\\)-score!\n\n\\[\nz_i = \\frac{x_i - \\mu}{\\sigma}\n\\]\n\nUnit of original \\(x_i\\) values: ?\nUnit of \\(z\\)-score: standard deviations from the mean!\n\n\n\nCode\nt1_z_score_line_data &lt;- tibble(\n  x = score_df$t1_z_score,\n  y = 0,\n  me = score_df$is_me\n) |&gt; arrange(me)\nggplot(t1_z_score_line_data, aes(x, y, col=factor(me), shape=factor(me))) +\n  geom_point(aes(size=g_pointsize)) +\n  scale_x_continuous(breaks=c(-2,-1,0,1,2)) +\n  dsan_theme(\"half\") +\n  theme(\n    legend.position=\"none\", \n    #rect = element_blank(),\n    #panel.grid = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y=element_blank(),\n    plot.margin = margin(-20,0,0,0,\"pt\")\n  ) +\n  expand_limits(x=c(-2,2)) +\n  labs(\n    x = \"Test 1 Z-Score\"\n  ) +\n  coord_fixed(ratio = 3)"
  },
  {
    "objectID": "w06/slides.html#why-all-the-worry-about-units",
    "href": "w06/slides.html#why-all-the-worry-about-units",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Why All The Worry About Units?",
    "text": "Why All The Worry About Units?\n\nEuclidean Distance\nManhattan Distance\nSpherical Distance vs. Straight-Line Distance"
  },
  {
    "objectID": "w06/slides.html#why-should-we-worry-about-this",
    "href": "w06/slides.html#why-should-we-worry-about-this",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Why Should We Worry About This?",
    "text": "Why Should We Worry About This?\n\n\n\nSay you’re training a facial recognition algorithm to detect criminals/terrorists\n\\[\n\\begin{align*}\n&\\Pr(\\text{criminal}) \\\\\n&= \\textsf{dist}(\\text{face}, \\text{model of criminal face})\n\\end{align*}\n\\]\n\n\n\n\nPearson (1924, 294)1\n\n\n\n\n\n\nGalton (1919), p. 8 (inset)\n\n\n\n\n\nWang and Kosinski (2018)\n\n\n\n\nGood thing this guy isn’t the father of modern statistics or anything like that 😮‍💨(For more historical scariness take my DSAN 5450: Data Ethics and Policy course next semester! 😉)"
  },
  {
    "objectID": "w06/slides.html#distances-are-metaphors-we-use-to-accomplish-something",
    "href": "w06/slides.html#distances-are-metaphors-we-use-to-accomplish-something",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Distances Are Metaphors We Use To Accomplish Something",
    "text": "Distances Are Metaphors We Use To Accomplish Something\n\nImage Credit: Peter Dovak"
  },
  {
    "objectID": "w06/slides.html#which-metrics-should-we-use",
    "href": "w06/slides.html#which-metrics-should-we-use",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Which Metric(s) Should We Use?",
    "text": "Which Metric(s) Should We Use?\n\n\n\nAmbulance Driver?\n\n\n\n\nFrom Shahid et al. (2009)\n\n\n\n\nData Scientist?\n\n\\(L^p\\)-norm:\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_p = \\left(\\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p}\n\\]\nEdit Distance, e.g., Hamming distance:\n\\[\n\\begin{array}{c|c|c|c|c|c}\nx & \\green{1} & \\green{1} & \\red{0} & \\red{1} & 1 \\\\ \\hline\n& ✅ & ✅ & ❌ & ❌ & ✅ \\\\\\hline\ny & \\green{1} & \\green{1} & \\red{1} & \\red{0} & 1 \\\\\n\\end{array} \\; \\leadsto d(x,y) = 2\n\\]\nKL Divergence (Probability distributions):\n\\[\n\\begin{align*}\n\\kl(P \\parallel Q) &= \\sum_{x \\in \\mathcal{R}_X}P(x)\\log\\left[ \\frac{P(x)}{Q(x)} \\right] \\\\\n&\\neq \\kl(Q \\parallel P) \\; (!)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w06/slides.html#onto-the-math-lp-norms",
    "href": "w06/slides.html#onto-the-math-lp-norms",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Onto the Math! \\(L^p\\)-Norms",
    "text": "Onto the Math! \\(L^p\\)-Norms\n\nEuclidean distance = \\(L^2\\)-norm:\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_2 = \\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}\n\\]\n\nManhattan distance = \\(L^1\\)-norm:\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_1 = \\sum_{i=1}^n |x_i - y_i|\n\\]\n\nThe maximum(!) = \\(L^\\infty\\)-norm:\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_{\\infty} = \\lim_{p \\rightarrow \\infty}\\left[|| \\mathbf{x} - \\mathbf{y} ||_p\\right] = \\max\\{|x_1-y_1|, \\ldots, |x_n - y_n|\\}\n\\]"
  },
  {
    "objectID": "w06/slides.html#top-secret-non-well-defined-yet-useful-norms",
    "href": "w06/slides.html#top-secret-non-well-defined-yet-useful-norms",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Top Secret Non-Well-Defined Yet Useful Norms",
    "text": "Top Secret Non-Well-Defined Yet Useful Norms\n\n\n\n\nThe “\\(L^0\\)-norm”\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_0 = \\mathbf{1}\\left[x_i \\neq y_i\\right]\n\\]\n\nThe “\\(L^{1/2}\\)-norm”\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_{1/2} = \\left(\\sum_{i=1}^n \\sqrt{x_i - y_i} \\right)^2\n\\]\n\n\nWhat’s wrong with these norms? (Re-)enter the Triangle Inequality! \\(d\\) defines a norm iff\n\n\\[\n\\forall a, b, c \\left[ d(a,c) \\leq d(a,b) + d(b,c) \\right]\n\\]\n\n\n\n\nVisualizing “circles” in \\(L^p\\) space:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#p_values = [0., 0.5, 1, 1.5, 2, np.inf]\np_values = [0.5, 1, 2, np.inf]\nx, y = np.meshgrid(np.linspace(-3, 3, num=101), np.linspace(-3, 3, num=101))\nfig, axes = plt.subplots(ncols=(len(p_values) + 1)// 2,\n                     nrows=2, figsize=(5, 5))\nfor p, ax in zip(p_values, axes.flat):\n  if np.isinf(p):\n    z = np.maximum(np.abs(x),np.abs(y))\n  else:\n    z = ((np.abs((x))**p) + (np.abs((y))**p))**(1./p)\n  ax.contourf(x, y, z, 30, cmap='bwr')\n  ax.contour(x, y, z, [1], colors='red', linewidths = 2)\n  ax.title.set_text(f'p = {p}')\n  ax.set_aspect('equal', 'box')\nplt.tight_layout()\n#plt.subplots_adjust(hspace=0.35, wspace=0.25)\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 7: Plots adapted from this StackOverflow post\n\n\n\n\n\n\nTo go beyond this, and explore how \\(L^p\\) “quasi-norms” can be extremely useful, take DSAN 6100: Optimization!One place they can be useful? You guessed it: facial recognition algorithms (Guo, Wang, and Ruan 2013)"
  },
  {
    "objectID": "w06/slides.html#the-value-of-studying",
    "href": "w06/slides.html#the-value-of-studying",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Value of Studying",
    "text": "The Value of Studying\n\nYou are a teacher trying to assess the causal impact of studying on homework scores\nLet \\(S\\) = hours of studying, \\(H\\) = homework score\n\n\n\n\n\n\n\nSo far so good: we could estimate the relationship via (e.g.) regression\n\n\\[\nh_i = \\beta_0 + \\beta_1 s_i + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "w06/slides.html#my-dog-ate-my-homework",
    "href": "w06/slides.html#my-dog-ate-my-homework",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "My Dog Ate My Homework",
    "text": "My Dog Ate My Homework\n\nThe issue: for some students \\(h_i\\) is missing, since their dog ate their homework\nLet \\(D = \\begin{cases}1 &\\text{if dog ate homework} \\\\ 0 &\\text{otherwise}\\end{cases}\\)\nThis means we don’t observe \\(H\\) but \\(H^* = \\begin{cases} H &\\text{if }D = 0 \\\\ \\texttt{NA} &\\text{otherwise}\\end{cases}\\)\nIn the easy case, let’s say that dogs eat homework at random (i.e., without reference to \\(S\\) or \\(H\\)). Then we say \\(H\\) is “missing at random”. Our PGM now looks like:"
  },
  {
    "objectID": "w06/slides.html#my-dog-ate-my-homework-because-of-reasons",
    "href": "w06/slides.html#my-dog-ate-my-homework-because-of-reasons",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "My Dog Ate My Homework Because of Reasons",
    "text": "My Dog Ate My Homework Because of Reasons\n\nThere are scarier alternatives, though! What if…\n\n\n\nDogs eat homework because their owner studied so much that the dog got ignored?\n\n\n\n\n\n\n\nDogs hate sloppy work, and eat bad homework that would have gotten a low score\n\n\n\n\n\n\n\nNoisy homes (\\(Z = 1\\)) cause dogs to get agitated and eat homework more often, and students do worse"
  },
  {
    "objectID": "w06/slides.html#tukeys-rule",
    "href": "w06/slides.html#tukeys-rule",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Tukey’s Rule",
    "text": "Tukey’s Rule\n\nGiven the first quartile (25th percentile) \\(Q_1\\), and the third quartile (75th percentile) \\(Q_2\\), define the Inter-Quartile Range as\n\n\\[\n\\iqr = Q_3 - Q_1\n\\]\n\nThen an outlier is a point more than \\(1.5 \\cdot \\iqr\\) away from \\(Q_1\\) or \\(Q_3\\); outside of\n\n\\[\n[Q_1 - 1.5 \\cdot \\iqr, \\; Q_3 + 1.5 \\cdot \\iqr]\n\\]\n\nThis is the outlier rule used for box-and-whisker plots:\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n# Generate normal data\ndist_df &lt;- tibble(Score=rnorm(95), Distribution=\"N(0,1)\")\n# Add outliers\noutlier_dist_sd &lt;- 6\noutlier_df &lt;- tibble(Score=rnorm(5, 0, outlier_dist_sd), Distribution=paste0(\"N(0,\",outlier_dist_sd,\")\"))\ndata_df &lt;- bind_rows(dist_df, outlier_df)\n# Compute iqr and outlier range\nq1 &lt;- quantile(data_df$Score, 0.25)\nq3 &lt;- quantile(data_df$Score, 0.75)\niqr &lt;- q3 - q1\niqr_cutoff_lower &lt;- q1 - 1.5 * iqr\niqr_cutoff_higher &lt;- q3 + 1.5 * iqr\nis_outlier &lt;- function(x) (x &lt; iqr_cutoff_lower) || (x &gt; iqr_cutoff_higher)\ndata_df['Outlier'] &lt;- sapply(data_df$Score, is_outlier)\n#data_df\nggplot(data_df, aes(x=Score, y=factor(0))) + \n  geom_boxplot(outlier.color = NULL, linewidth = g_linewidth, outlier.size = g_pointsize / 1.5) +\n  geom_jitter(data=data_df, aes(col = Distribution, shape=Outlier), size = g_pointsize / 1.5, height=0.15, alpha = 0.8, stroke = 1.5) +\n  geom_vline(xintercept = iqr_cutoff_lower, linetype = \"dashed\") +\n  geom_vline(xintercept = iqr_cutoff_higher, linetype = \"dashed\") +\n  #coord_flip() +\n  dsan_theme(\"half\") +\n  theme(\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  scale_x_continuous(breaks=seq(from=-3, to=3, by=1)) +\n  scale_shape_manual(values=c(16, 4))"
  },
  {
    "objectID": "w06/slides.html#sigma-rule",
    "href": "w06/slides.html#sigma-rule",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "3-Sigma Rule",
    "text": "3-Sigma Rule\n\nRecall the 68-95-99.7 Rule\nThe 3-Sigma Rule says simply: throw away anything more than 3 standard deviations away from the mean (beyond range that should contain 99.7% of data)\n\n\n\nCode\nmean_score &lt;- mean(data_df$Score)\nsd_score &lt;- sd(data_df$Score)\nlower_cutoff &lt;- mean_score - 3 * sd_score\nupper_cutoff &lt;- mean_score + 3 * sd_score\n# For printing / displaying\nmean_score_str &lt;- sprintf(mean_score, fmt='%.2f')\nsd_score_str &lt;- sprintf(sd_score, fmt='%.2f')\nggplot(data_df, aes(x=Score)) + \n  geom_density(linewidth = g_linewidth) +\n  #geom_boxplot(outlier.color = NULL, linewidth = g_linewidth, outlier.size = g_pointsize / 1.5) +\n  #geom_jitter(data=data_df, aes(y = factor(0), col = dist), size = g_pointsize / 1.5, height=0.25) +\n  #coord_flip() +\n  dsan_theme(\"half\") +\n  theme(\n    axis.title.y = element_blank(),\n    #axis.ticks.y = element_blank(),\n    #axis.text.y = element_blank()\n  ) +\n  #geom_boxplot() +\n  geom_vline(xintercept = mean_score, linetype = \"dashed\") +\n  geom_vline(xintercept = lower_cutoff, linetype = \"dashed\") +\n  geom_vline(xintercept = upper_cutoff, linetype = \"dashed\") +\n  geom_jitter(data=data_df, aes(x = Score, y = 0, col = Distribution, shape = Outlier),\n    size = g_pointsize / 1.5, height=0.025, alpha=0.8, stroke=1.5) +\n  scale_x_continuous(breaks=seq(from=-3, to=3, by=1)) +\n  scale_shape_manual(values=c(16, 4)) +\n  labs(\n    title = paste0(\"Six-Sigma Rule, mu = \",mean_score_str,\", sd = \",sd_score_str),\n    y = \"Density\"\n  )"
  },
  {
    "objectID": "w06/slides.html#missing-data-outliers-most-important-takeaway",
    "href": "w06/slides.html#missing-data-outliers-most-important-takeaway",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Missing Data + Outliers: Most Important Takeaway!",
    "text": "Missing Data + Outliers: Most Important Takeaway!\n\nAlways have a working hypothesis about the Data-Generating Process!\nLiterally the solution to… 75% of all data-related headaches\nWhat variables explain why this data point is missing?\nWhat variables explain why this data point is an outlier?"
  },
  {
    "objectID": "w06/slides.html#driving-the-point-home",
    "href": "w06/slides.html#driving-the-point-home",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Driving the Point Home",
    "text": "Driving the Point Home\n\n\n\n\n\n\nFigure (and DGP analysis) from D’Ignazio and Klein (2020)\n\n\n\n\nPresumed DGP:\n\n\n\n\\(K\\) = Kidnappings, \\(C\\) = Counts\n\n\nActual DGP:\n\n\n\n\\(K\\) = Kidnappings, \\(R\\) = News reports about kidnappings, \\(C\\) = Counts"
  },
  {
    "objectID": "w06/slides.html#references",
    "href": "w06/slides.html#references",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "References",
    "text": "References\n\n\nBoyd, Nora Mills, and James Bogen. 2021. “Theory and Observation in Science.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Winter 2021. Metaphysics Research Lab, Stanford University.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. “6. The Numbers Don’t Speak for Themselves.” Data Feminism, March.\n\n\nGalton, Francis. 1919. Inquiries Into Human Faculty and Its Development. J.M. Dent & Company.\n\n\nGuo, Song, Zhan Wang, and Qiuqi Ruan. 2013. “Enhancing Sparsity via \\(\\mathscr{l}\\)p (0.” Neurocomputing 99 (January): 592–602. https://doi.org/10.1016/j.neucom.2012.05.028.\n\n\nJelveh, Zubin, Bruce Kogut, and Suresh Naidu. 2023. “Political Language in Economics.” Economic Journal (Forthcoming). https://doi.org/10.2139/ssrn.2535453.\n\n\nPearson, Karl. 1924. The Life, Letters and Labours of Francis Galton. University Press.\n\n\nShahid, Rizwan, Stefania Bertazzon, Merril L. Knudtson, and William A. Ghali. 2009. “Comparison of Distance Measures in Spatial Analytical Modeling for Health Service Planning.” BMC Health Services Research 9 (1): 200. https://doi.org/10.1186/1472-6963-9-200.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098.\n\n\n\n\nDSAN 5000-02 Week 6: Exploratory Data Analysis"
  },
  {
    "objectID": "w06/index.html",
    "href": "w06/index.html",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w06/index.html#nlp-recap",
    "href": "w06/index.html#nlp-recap",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "NLP Recap",
    "text": "NLP Recap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Excerpts from two data science textbooks, plus another book\n\n\n\n\n\n\n\n\n\n\ndoc_id\ntext\ntexts\nKékkek\nvoice\n\n\n\n\n\n0\n0\n6\n0\n1\n\n\n\n1\n0\n0\n3\n1\n\n\n\n2\n6\n0\n0\n0\n\n\n\n\nFigure 2: The Document-Term Matrix (DTM)\n\n\n\n\n\n\n\n\n\n \n\n\n\ndoc_id\ntext\nkekkek\nvoice\n\n\n\n\n\n\n0\n6\n0\n1\n\n\n\n\n1\n0\n3\n1\n\n\n\n\n2\n6\n0\n0\n\n\n\n\n\nFigure 3: The cleaned DTM, after lowercasing, lemmatization, and unicode standardization"
  },
  {
    "objectID": "w06/index.html#your-nlp-toolbox",
    "href": "w06/index.html#your-nlp-toolbox",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Your NLP Toolbox",
    "text": "Your NLP Toolbox\n\nProcesses like lowercasing and stemming allowed the computer to recognize that text and texts should be counted together in this context, since they refer to the same semantic concept.\nAs we learn NLP, we’ll develop a “toolbox” of ideas, algorithms, and tasks allowing us to quantify, clean, and analyzing text data, where each tool will help us at some level/stage of this analysis:\n\nGathering texts\nPreprocessing\nLearning (e.g., estimating parameters for a model) about the texts\nApplying what we learned to downstream tasks we’d like to solve"
  },
  {
    "objectID": "w06/index.html#the-items-in-our-toolbox",
    "href": "w06/index.html#the-items-in-our-toolbox",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Items In Our Toolbox",
    "text": "The Items In Our Toolbox\n\n\n\n\n\nCorpus Construction\n\n• Corpus: The collection of documents you’re hoping to analyze\n• Books, articles, posts, emails, tweets, etc.\n\n\n\n\nCorpus/Document Level NLP\n\n• Vocabulary: The collection of unique tokens across all documents in your corpus\n• Segmentation: Breaking a document into parts (paragraphs and/or sentences)\n\n↓Sentence/Word Level NLP\n\n• Tokenization: Break sentence into tokens\n• Stopword Removal: Removing non-semantic (syntactic) tokens like “the”, “and”\n• Stemming: Naïvely (but quickly) “chopping off” ends of tokens (e.g., plural → singular)\n• Lemmatization: Algorithmically map tokens to linguistic roots (slower than stemming)\n\n\n\n\nVectorization\n\nTransform textual representation into numeric representation, like the DTM\n\n\n↓\n\n\n\nDownstream Tasks\n\n• Text classification\n• Named entity recognition\n• Sentiment analysis"
  },
  {
    "objectID": "w06/index.html#tidyverse",
    "href": "w06/index.html#tidyverse",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nThink of data science tasks as involving pipelines:\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_00\n\n Data-Processing Pipeline 1   \n\nraw\n\n Raw Data   \n\ntr1\n\n Transformation A (select(), filter())   \n\nraw-&gt;tr1\n\n    \n\ntr2\n\n Transformation B (mutate(), summarize())   \n\ntr1-&gt;tr2\n\n    \n\nviz\n\n Visualization   \n\ntr2-&gt;viz\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_00\n\n Data-Processing Pipeline 2   \n\nraw\n\n Raw Data   \n\ntr1\n\n Transformation C (select(), filter())   \n\nraw-&gt;tr1\n\n    \n\ntr2\n\n Transformation D (mutate(), summarize())   \n\ntr1-&gt;tr2\n\n    \n\nviz\n\n       Result        \n\ntr2-&gt;viz\n\n   \n\n\n\n\n\n\nTidyverse lets you pipe output from one transformation as the input to another:\n\nraw_data |&gt; select() |&gt; mutate() |&gt; visualize()\nraw_data |&gt; filter() |&gt; summarize() |&gt; check_result()"
  },
  {
    "objectID": "w06/index.html#selecting-columns",
    "href": "w06/index.html#selecting-columns",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nselect() lets you keep only the columns you care about in your current analysis:\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\nCode\ntable1 |&gt; select(country, year, population)\n\n\n# A tibble: 6 × 3\n  country      year population\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999   19987071\n2 Afghanistan  2000   20595360\n3 Brazil       1999  172006362\n4 Brazil       2000  174504898\n5 China        1999 1272915272\n6 China        2000 1280428583"
  },
  {
    "objectID": "w06/index.html#filtering-rows",
    "href": "w06/index.html#filtering-rows",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Filtering Rows",
    "text": "Filtering Rows\nfilter() lets you keep only the rows you care about in your current analysis:\n\n\n\n\nCode\ntable1 |&gt; filter(year == 2000)\n\n\n# A tibble: 3 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  2000   2666   20595360\n2 Brazil       2000  80488  174504898\n3 China        2000 213766 1280428583\n\n\n\n\nCode\ntable1 |&gt; filter(country == \"Afghanistan\")\n\n\n# A tibble: 2 × 4\n  country      year cases population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999   745   19987071\n2 Afghanistan  2000  2666   20595360"
  },
  {
    "objectID": "w06/index.html#merging-data",
    "href": "w06/index.html#merging-data",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Merging Data",
    "text": "Merging Data\n\nThe task: Analyze relationship between population and GDP (in 2000)\nThe data: One dataset on population in 2000, another on GDP in 2000\nLet’s get the data ready for merging using R\n\n\n\n\n\nCode\ndf &lt;- table1 |&gt;\n  select(country, year, population) |&gt;\n  filter(year == 2000)\ndf |&gt; write_csv(\"assets/pop_2000.csv\")\ndf\n\n\n# A tibble: 3 × 3\n  country      year population\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  2000   20595360\n2 Brazil       2000  174504898\n3 China        2000 1280428583\n\n\n\n\n\nCode\ngdp_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/c83e87f61c166dea8ba7e4453f08a404/raw/29b03e6320bc3ffc9f528c2ac497a21f2d801c00/gdp_2000_2010.csv\")\n\n\nRows: 403 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Country Name, Country Code\ndbl (2): Year, Value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ngdp_df |&gt; head(5)\n\n\n# A tibble: 5 × 4\n  `Country Name` `Country Code`  Year         Value\n  &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1 Afghanistan    AFG             2010  15936800636.\n2 Albania        ALB             2000   3632043908.\n3 Albania        ALB             2010  11926953259.\n4 Algeria        DZA             2000  54790245601.\n5 Algeria        DZA             2010 161207268655."
  },
  {
    "objectID": "w06/index.html#selectingfiltering-in-action",
    "href": "w06/index.html#selectingfiltering-in-action",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Selecting/Filtering in Action",
    "text": "Selecting/Filtering in Action\n\n\nCode\ngdp_2000_df &lt;- gdp_df |&gt;\n  select(`Country Name`,Year,Value) |&gt;\n  filter(Year == \"2000\") |&gt;\n  rename(country=`Country Name`, year=`Year`, gdp=`Value`)\ngdp_2000_df |&gt; write_csv(\"assets/gdp_2000.csv\")\ngdp_2000_df |&gt; head()\n\n\n# A tibble: 6 × 3\n  country              year           gdp\n  &lt;chr&gt;               &lt;dbl&gt;         &lt;dbl&gt;\n1 Albania              2000   3632043908.\n2 Algeria              2000  54790245601.\n3 Andorra              2000   1434429703.\n4 Angola               2000   9129594819.\n5 Antigua and Barbuda  2000    830158769.\n6 Argentina            2000 284203750000"
  },
  {
    "objectID": "w06/index.html#recommended-language-python",
    "href": "w06/index.html#recommended-language-python",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Recommended Language: Python",
    "text": "Recommended Language: Python\nPandas provides an easy-to-use df.merge(other_df)!\n\n\n\nLeft Join\n\n\n\nCode\nmerged_df = pop_df.merge(gdp_df,\n  on='country', how='left', indicator=True\n)\nMarkdown(merged_df.to_markdown())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear_x\npopulation\nyear_y\ngdp\n_merge\n\n\n\n\n0\nAfghanistan\n2000\n20595360\nnan\nnan\nleft_only\n\n\n1\nBrazil\n2000\n174504898\n2000\n6.55421e+11\nboth\n\n\n2\nChina\n2000\n1280428583\n2000\n1.21135e+12\nboth\n\n\n\n\n\n\n\nInner join (≈ Intersection (\\(\\cap\\)))\n\n\n\nCode\nmerged_df = pop_df.merge(gdp_df,\n  on='country', how='inner', indicator=True\n)\nMarkdown(merged_df.to_markdown())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nyear_x\npopulation\nyear_y\ngdp\n_merge\n\n\n\n\n0\nBrazil\n2000\n174504898\n2000\n6.55421e+11\nboth\n\n\n1\nChina\n2000\n1280428583\n2000\n1.21135e+12\nboth"
  },
  {
    "objectID": "w06/index.html#reshaping-data",
    "href": "w06/index.html#reshaping-data",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Reshaping Data",
    "text": "Reshaping Data\nSometimes you can’t merge because one of the datasets looks like the table on the left, but we want it to look like the table on the right\n\n\nIn data-cleaning jargon, this dataset is long (more than one row per observation)\n\ntable2 |&gt; write_csv(\"assets/long_data.csv\")\ntable2 |&gt; head()\n\n# A tibble: 6 × 4\n  country      year type           count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Afghanistan  1999 cases            745\n2 Afghanistan  1999 population  19987071\n3 Afghanistan  2000 cases           2666\n4 Afghanistan  2000 population  20595360\n5 Brazil       1999 cases          37737\n6 Brazil       1999 population 172006362\n\n\n\nIn data-cleaning jargon, this dataset is wide (one row per obs; usually tidy)\n\ntable1 |&gt; write_csv(\"assets/wide_data.csv\")\ntable1 |&gt; head()\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "w06/index.html#reshaping-long-to-wide-in-python-pd.pivot",
    "href": "w06/index.html#reshaping-long-to-wide-in-python-pd.pivot",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Reshaping Long-to-Wide in Python: pd.pivot()",
    "text": "Reshaping Long-to-Wide in Python: pd.pivot()\n\n\nCreate unique ID for wide version:\n\n\nCode\nlong_df['id'] = long_df['country'] + '_' + long_df['year'].apply(str)\n# Reorder the columns, so it shows the id first\nlong_df = long_df[['id','country','year','type','count']]\ndisp(long_df.head(6))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry\nyear\ntype\ncount\n\n\n\n\n0\nAfghanistan_1999\nAfghanistan\n1999\ncases\n745\n\n\n1\nAfghanistan_1999\nAfghanistan\n1999\npopulation\n19987071\n\n\n2\nAfghanistan_2000\nAfghanistan\n2000\ncases\n2666\n\n\n3\nAfghanistan_2000\nAfghanistan\n2000\npopulation\n20595360\n\n\n4\nBrazil_1999\nBrazil\n1999\ncases\n37737\n\n\n5\nBrazil_1999\nBrazil\n1999\npopulation\n172006362\n\n\n\n\n\n\n\n\nCode\nreshaped_df = pd.pivot(long_df,\n  index='id',\n  columns='type',\n  values='count'\n)\ndisp(reshaped_df)\n\n\n\n\n\nid\ncases\npopulation\n\n\n\n\nAfghanistan_1999\n745\n1.99871e+07\n\n\nAfghanistan_2000\n2666\n2.05954e+07\n\n\nBrazil_1999\n37737\n1.72006e+08\n\n\nBrazil_2000\n80488\n1.74505e+08\n\n\nChina_1999\n212258\n1.27292e+09\n\n\nChina_2000\n213766\n1.28043e+09"
  },
  {
    "objectID": "w06/index.html#the-other-direction-wide-to-long-pd.melt",
    "href": "w06/index.html#the-other-direction-wide-to-long-pd.melt",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Other Direction (Wide-to-Long): pd.melt()",
    "text": "The Other Direction (Wide-to-Long): pd.melt()\n\n\n\n\nCode\nwide_df = pd.read_csv(\"assets/wide_data.csv\")\ndisp(wide_df)\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\n0\nAfghanistan\n1999\n745\n19987071\n\n\n1\nAfghanistan\n2000\n2666\n20595360\n\n\n2\nBrazil\n1999\n37737\n172006362\n\n\n3\nBrazil\n2000\n80488\n174504898\n\n\n4\nChina\n1999\n212258\n1272915272\n\n\n5\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\n\n\nCode\nlong_df = pd.melt(wide_df,\n  id_vars=['country','year'],\n  value_vars=['cases','population']\n)\ndisp(long_df.head(6))\n\n\n\n\n\n\ncountry\nyear\nvariable\nvalue\n\n\n\n\n0\nAfghanistan\n1999\ncases\n745\n\n\n1\nAfghanistan\n2000\ncases\n2666\n\n\n2\nBrazil\n1999\ncases\n37737\n\n\n3\nBrazil\n2000\ncases\n80488\n\n\n4\nChina\n1999\ncases\n212258\n\n\n5\nChina\n2000\ncases\n213766"
  },
  {
    "objectID": "w06/index.html#wide-to-long-in-r-gather",
    "href": "w06/index.html#wide-to-long-in-r-gather",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Wide-to-Long in R: gather()",
    "text": "Wide-to-Long in R: gather()\n\n\n\n\nCode\ntable1\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n\nCode\nlong_df &lt;- gather(table1,\n  key = \"variable\",\n  value = cases,\n  -c(country, year)\n)\nlong_df |&gt; head()\n\n\n# A tibble: 6 × 4\n  country      year variable  cases\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 Afghanistan  1999 cases       745\n2 Afghanistan  2000 cases      2666\n3 Brazil       1999 cases     37737\n4 Brazil       2000 cases     80488\n5 China        1999 cases    212258\n6 China        2000 cases    213766"
  },
  {
    "objectID": "w06/index.html#exploratory-data-analysis-eda",
    "href": "w06/index.html#exploratory-data-analysis-eda",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nIn contrast to confirmatory data analysis\n\n\n\n\n\n\n\n\nG\n\n \n\ncluster_understand\n\n Understand  \n\ncluster_conf\n\n Confirmatory  \n\ncluster_expl\n\n Exploratory  \n\ncluster_clean\n\n Clean  \n\ncluster_import\n\n    \n\ncluster_tidy\n\n     \n\nImport\n\n Import   \n\nTidy\n\n Tidy   \n\nImport-&gt;Tidy\n\n    \n\nimportLibs\n\n R:  read_csv() Python:  pd.read_csv()   \n\nV\n\n Visualize   \n\nTidy-&gt;V\n\n    \n\ntidyLibraries\n\n R:  tidyverse Python:  Pandas   \n\nModel\n\n Model   \n\nV-&gt;Model\n\n    \n\nvizLibraries\n\n R:  ggplot2 Python:  seaborn   \n\nModel-&gt;V\n\n    \n\nCommunicate\n\n Communicate   \n\nModel-&gt;Communicate\n\n    \n\nconfLibraries\n\n R:  e1071 Python:  scikit-learn"
  },
  {
    "objectID": "w06/index.html#exploratory-data-analysis-eda-1",
    "href": "w06/index.html#exploratory-data-analysis-eda-1",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nSo you have reasonably clean data… now what?\n\n\n\n\n(Image source: Oldies but Goldies: Statistical Graphics Books)"
  },
  {
    "objectID": "w06/index.html#what-is-eda",
    "href": "w06/index.html#what-is-eda",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "What is EDA?",
    "text": "What is EDA?\nFrom IBM:\n\nLook at data before making any assumptions1\nScreen data and identify obvious errors\nBetter understand patterns within the data\nDetect outliers or anomalous events\nFind interesting relations among the variables.\n\nOverarching goal to keep in mind: does this data have what I need to address my research question?"
  },
  {
    "objectID": "w06/index.html#assumption-free-analysis",
    "href": "w06/index.html#assumption-free-analysis",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Assumption-Free Analysis?",
    "text": "Assumption-Free Analysis?\n\n\nImportant question for EDA: Is it actually possible to analyze data without assumptions?2\n\nEmpirical results are laden with values and theoretical commitments.(Boyd and Bogen 2021)\n\n\nEx: Estimates of optimal tax rates in Econ journals vs. Economist ideology scores\n\n\nRows: 21 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): id, ideology, elast, tr_est\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nComputed based on Jelveh, Kogut, and Naidu (2023)"
  },
  {
    "objectID": "w06/index.html#statistical-eda",
    "href": "w06/index.html#statistical-eda",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Statistical EDA",
    "text": "Statistical EDA\n\nIterative process: Ask questions of the data, find answers, generate more questions\nYou’re probably already used to Mean and Variance: Fancier EDA/robustness methods build upon these two!\nWhy do we need to visualize? Can’t we just use mean, \\(R^2\\)?\n…Enter Anscombe’s Quartet\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"ticks\")\n# https://towardsdatascience.com/how-to-use-your-own-color-palettes-with-seaborn-a45bf5175146\nsns.set_palette(sns.color_palette(cb_palette))\n\n# Load the example dataset for Anscombe's quartet\nanscombe_df = sns.load_dataset(\"anscombe\")\n#print(anscombe_df)\n\n# Show the results of a linear regression within each dataset\nanscombe_plot = sns.lmplot(\n    data=anscombe_df, x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\",\n    col_wrap=4, palette=\"muted\", ci=None,\n    scatter_kws={\"s\": 50, \"alpha\": 1},\n    height=3\n);\n\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\nanscombe_plot;"
  },
  {
    "objectID": "w06/index.html#the-scariest-dataset-of-all-time",
    "href": "w06/index.html#the-scariest-dataset-of-all-time",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Scariest Dataset of All Time",
    "text": "The Scariest Dataset of All Time\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\n\n# Compute dataset means\nmy_round = lambda x: round(x,2)\ndata_means = anscombe_df.groupby('dataset').agg(\n  x_mean = ('x', np.mean),\n  y_mean = ('y', np.mean)\n).apply(my_round)\n\n&lt;string&gt;:1: FutureWarning: The provided callable &lt;function mean at 0x11eccd120&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n&lt;string&gt;:1: FutureWarning: The provided callable &lt;function mean at 0x11eccd120&gt; is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n\ndisp(data_means, floatfmt='.2f')\n\n\n\n\ndataset\nx_mean\ny_mean\n\n\n\n\nI\n9.00\n7.50\n\n\nII\n9.00\n7.50\n\n\nIII\n9.00\n7.50\n\n\nIV\n9.00\n7.50\n\n\n\n\n\nFigure 4: Column means for each dataset\n\n\n\n\n\n# Compute dataset SDs\ndata_sds = anscombe_df.groupby('dataset').agg(\n  x_mean = ('x', np.std),\n  y_mean = ('y', np.std),\n).apply(my_round)\n\n&lt;string&gt;:2: FutureWarning: The provided callable &lt;function std at 0x11eccd260&gt; is currently using SeriesGroupBy.std. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"std\" instead.\n&lt;string&gt;:2: FutureWarning: The provided callable &lt;function std at 0x11eccd260&gt; is currently using SeriesGroupBy.std. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"std\" instead.\n\ndisp(data_sds, floatfmt='.2f')\n\n\n\n\ndataset\nx_mean\ny_mean\n\n\n\n\nI\n3.32\n2.03\n\n\nII\n3.32\n2.03\n\n\nIII\n3.32\n2.03\n\n\nIV\n3.32\n2.03\n\n\n\n\n\nFigure 5: Column SDs for each dataset\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelations\n\n\n\n\nimport tabulate\nfrom IPython.display import HTML\ncorr_matrix = anscombe_df.groupby('dataset').corr().apply(my_round)\n#Markdown(tabulate.tabulate(corr_matrix))\nHTML(corr_matrix.to_html())\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\n\nI\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\nII\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\nIII\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\nIV\nx\n1.00\n0.82\n\n\ny\n0.82\n1.00\n\n\n\n\n\nFigure 6: Correlation between \\(x\\) and \\(y\\) for each dataset"
  },
  {
    "objectID": "w06/index.html#it-doesnt-end-there",
    "href": "w06/index.html#it-doesnt-end-there",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "It Doesn’t End There…",
    "text": "It Doesn’t End There…\n\n\n\n\n\n\n\n\n\n\n\n\nimport statsmodels.formula.api as smf\nsummary_dfs = []\nfor cur_ds in ['I','II','III','IV']:\n  ds1_df = anscombe_df.loc[anscombe_df['dataset'] == \"I\"].copy()\n  # Fit regression model (using the natural log of one of the regressors)\n  results = smf.ols('y ~ x', data=ds1_df).fit()\n  # Get R^2\n  rsq = round(results.rsquared, 2)\n  # Inspect the results\n  summary = results.summary()\n  summary.extra_txt = None\n  summary_df = summary_to_df(summary, corner_col = f'Dataset {cur_ds}&lt;br&gt;R^2 = {rsq}')\n  summary_dfs.append(summary_df)\n\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/scipy/stats/_stats_py.py:1806: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=11\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/scipy/stats/_stats_py.py:1806: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=11\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/scipy/stats/_stats_py.py:1806: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=11\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n/Users/jpj/.virtualenvs/r-reticulate/lib/python3.11/site-packages/scipy/stats/_stats_py.py:1806: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=11\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n\ndisp(summary_dfs[0], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77\n\n\n\n\n\n\ndisp(summary_dfs[1], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IIR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77\n\n\n\n\n\n\n\n\ndisp(summary_dfs[2], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IIIR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77\n\n\n\n\n\n\ndisp(summary_dfs[3], include_index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset IVR^2 = 0.67\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n3\n1.12\n2.67\n0.03\n0.46\n5.54\n\n\nx\n0.5\n0.12\n4.24\n0\n0.23\n0.77"
  },
  {
    "objectID": "w06/index.html#normalization",
    "href": "w06/index.html#normalization",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Normalization",
    "text": "Normalization\n\n\n\nUnnormalized World\n\n\n\nCode\nnum_students &lt;- 30\nstudent_ids &lt;- seq(from = 1, to = num_students)\n# So we have the censored Normal pdf/cdf\nlibrary(crch)\ngen_test_scores &lt;- function(min_pts, max_pts) {\n  score_vals_unif &lt;- runif(num_students, min_pts, max_pts)\n  unif_mean &lt;- mean(score_vals_unif)\n  unif_sd &lt;- sd(score_vals_unif)\n  # Resample, this time censored normal dist\n  score_vals &lt;- round(rcnorm(num_students, mean=unif_mean, sd=unif_sd, left=min_pts, right=max_pts), 2)\n  return(score_vals)\n}\n# Test 1\nt1_min &lt;- 0\nt1_max &lt;- 268.3\nt1_score_vals &lt;- gen_test_scores(t1_min, t1_max)\nt1_mean &lt;- mean(t1_score_vals)\nt1_sd &lt;- sd(t1_score_vals)\nget_t1_pctile &lt;- function(s) round(100 * ecdf(t1_score_vals)(s), 1)\n# Test 2\nt2_min &lt;- -1\nt2_max &lt;- 1.2\nt2_score_vals &lt;- gen_test_scores(t2_min, t2_max)\nt2_mean &lt;- mean(t2_score_vals)\nt2_sd &lt;- sd(t2_score_vals)\nget_t2_pctile &lt;- function(s) round(100 * ecdf(t2_score_vals)(s), 1)\nscore_df &lt;- tibble(\n  id=student_ids,\n  t1_score=t1_score_vals,\n  t2_score=t2_score_vals\n)\nscore_df &lt;- score_df |&gt; arrange(desc(t1_score))\n\n\n“I got a 238.25 on the first test!” 🤩\n“But only a 0.31 on the second” 😭\n\n\n# A tibble: 6 × 3\n     id t1_score t2_score\n  &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    17     268.    -0.54\n2    27     258.    -0.33\n3    26     246.    -0.55\n4     5     238.     0.31\n5    11     207.    -0.02\n6    16     205.    -0.06\n\n\n\n\nNormalized World\n\n\nscore_df &lt;- score_df |&gt;\n  mutate(\n    t1_z_score = round((t1_score - t1_mean) / t1_sd, 2),\n    t2_z_score = round((t2_score - t2_mean) / t2_sd, 2),\n    t1_pctile = get_t1_pctile(t1_score),\n    t2_pctile = get_t2_pctile(t2_score)\n  ) |&gt;\n  relocate(t1_pctile, .after = t1_score) |&gt;\n  relocate(t2_pctile, .after = t2_score)\n\n“I scored higher than 90% of students on the first test! 🤩\n“And higher than 60% on the second!” 😎\n\n\n# A tibble: 6 × 7\n     id t1_score t1_pctile t2_score t2_pctile t1_z_score t2_z_score\n  &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1    17     268.     100      -0.54      30         1.87      -0.82\n2    27     258.      96.7    -0.33      46.7       1.73      -0.52\n3    26     246.      93.3    -0.55      26.7       1.54      -0.83\n4     5     238.      90       0.31      60         1.44       0.39\n5    11     207.      86.7    -0.02      56.7       0.98      -0.08\n6    16     205.      83.3    -0.06      50         0.96      -0.14"
  },
  {
    "objectID": "w06/index.html#scaling",
    "href": "w06/index.html#scaling",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Scaling",
    "text": "Scaling\nThe percentile places everyone at evenly-spaced intervals from 0 to 100:\n\n# https://community.rstudio.com/t/number-line-in-ggplot/162894/4\n# Add a binary indicator to track \"me\" (student #8)\nwhoami &lt;- 29\nscore_df &lt;- score_df |&gt;\n  mutate(is_me = as.numeric(id == whoami))\nlibrary(ggplot2)\nt1_line_data &lt;- tibble(\n  x = score_df$t1_pctile,\n  y = 0,\n  me = score_df$is_me\n)\nggplot(t1_line_data, aes(x, y, col=factor(me), shape=factor(me))) +\n  geom_point(aes(size=g_pointsize)) +\n  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +\n  scale_color_discrete(c(0,1)) +\n  dsan_theme(\"half\") +\n  theme(\n    legend.position=\"none\", \n    #rect = element_blank(),\n    #panel.grid = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y=element_blank(),\n    panel.spacing = unit(0, \"mm\"),\n    plot.margin = margin(-35, 0, 0, 0, \"pt\"),\n  ) +\n  labs(\n    x = \"Test 1 Percentile\"\n  ) +\n  coord_fixed(ratio = 100)\n\n\n\n\n\n\n\n\n\nBut what if we want to see their absolute performance, on a 0 to 100 scale?\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nscore_df &lt;- score_df |&gt;\n  mutate(\n    t1_rescaled = rescale(\n      t1_score,\n      from = c(t1_min, t1_max),\n      to = c(0, 100)\n    ),\n    t2_rescaled = rescale(\n      t2_score,\n      from = c(t2_min, t2_max),\n      to = c(0, 100)\n    )\n  )\n# Place \"me\" last so that it gets plotted last\nt1_rescaled_line_data &lt;- tibble(\n  x = score_df$t1_rescaled,\n  y = 0,\n  me = score_df$is_me\n) |&gt; arrange(me)\nggplot(t1_rescaled_line_data, aes(x,y,col=factor(me), shape=factor(me))) +\n  geom_point(size=g_pointsize) +\n  scale_x_continuous(breaks=seq(from=0, to=100, by=10)) +\n  dsan_theme(\"half\") +\n  expand_limits(x=c(0, 100)) +\n  theme(\n    legend.position=\"none\", \n    #rect = element_blank(),\n    #panel.grid = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y=element_blank(),\n    #panel.spacing = unit(0, \"mm\"),\n    #plot.margin = margin(-40, 0, 0, 0, \"pt\"),\n  ) +\n  labs(\n    x = \"Test 1 Score (Rescaled to 0-100)\"\n  ) +\n  coord_fixed(ratio = 100)"
  },
  {
    "objectID": "w06/index.html#shifting-recentering",
    "href": "w06/index.html#shifting-recentering",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Shifting / Recentering",
    "text": "Shifting / Recentering\n\nPercentiles tell us how the students did in terms of relative rankings\nRescaling lets us reinterpret the boundary points\nWhat about with respect to some absolute baseline? For example, how well they did relative to the mean \\(\\mu\\)?\n\n\\[\nx'_i = x_i - \\mu\n\\]\n\nBut we’re still “stuck” in units of the test: is \\(x'_i = 0.3\\) (0.3 points above the mean) “good”? What about \\(x'_j = -2568\\) (2568 points below the mean)? How “bad” is this case?"
  },
  {
    "objectID": "w06/index.html#shifting-and-scaling-the-z-score",
    "href": "w06/index.html#shifting-and-scaling-the-z-score",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Shifting and Scaling: The \\(z\\)-Score",
    "text": "Shifting and Scaling: The \\(z\\)-Score\n\nEnter the \\(z\\)-score!\n\n\\[\nz_i = \\frac{x_i - \\mu}{\\sigma}\n\\]\n\nUnit of original \\(x_i\\) values: ?\nUnit of \\(z\\)-score: standard deviations from the mean!\n\n\nt1_z_score_line_data &lt;- tibble(\n  x = score_df$t1_z_score,\n  y = 0,\n  me = score_df$is_me\n) |&gt; arrange(me)\nggplot(t1_z_score_line_data, aes(x, y, col=factor(me), shape=factor(me))) +\n  geom_point(aes(size=g_pointsize)) +\n  scale_x_continuous(breaks=c(-2,-1,0,1,2)) +\n  dsan_theme(\"half\") +\n  theme(\n    legend.position=\"none\", \n    #rect = element_blank(),\n    #panel.grid = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.line.y = element_blank(),\n    axis.ticks.y=element_blank(),\n    plot.margin = margin(-20,0,0,0,\"pt\")\n  ) +\n  expand_limits(x=c(-2,2)) +\n  labs(\n    x = \"Test 1 Z-Score\"\n  ) +\n  coord_fixed(ratio = 3)"
  },
  {
    "objectID": "w06/index.html#why-all-the-worry-about-units",
    "href": "w06/index.html#why-all-the-worry-about-units",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Why All The Worry About Units?",
    "text": "Why All The Worry About Units?\n\nEuclidean Distance\nManhattan Distance\nSpherical Distance vs. Straight-Line Distance"
  },
  {
    "objectID": "w06/index.html#why-should-we-worry-about-this",
    "href": "w06/index.html#why-should-we-worry-about-this",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Why Should We Worry About This?",
    "text": "Why Should We Worry About This?\n\n\n\nSay you’re training a facial recognition algorithm to detect criminals/terrorists\n\\[\n\\begin{align*}\n&\\Pr(\\text{criminal}) \\\\\n&= \\textsf{dist}(\\text{face}, \\text{model of criminal face})\n\\end{align*}\n\\]\n\n\n\n\nPearson (1924, 294)3\n\n\n\n\n\n\nGalton (1919), p. 8 (inset)\n\n\n\n\n\nWang and Kosinski (2018)"
  },
  {
    "objectID": "w06/index.html#distances-are-metaphors-we-use-to-accomplish-something",
    "href": "w06/index.html#distances-are-metaphors-we-use-to-accomplish-something",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Distances Are Metaphors We Use To Accomplish Something",
    "text": "Distances Are Metaphors We Use To Accomplish Something\n\n\n\nImage Credit: Peter Dovak"
  },
  {
    "objectID": "w06/index.html#which-metrics-should-we-use",
    "href": "w06/index.html#which-metrics-should-we-use",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Which Metric(s) Should We Use?",
    "text": "Which Metric(s) Should We Use?\n\n\n\nAmbulance Driver?\n\n\n\n\nFrom Shahid et al. (2009)\n\n\n\n\nData Scientist?\n\n\\(L^p\\)-norm:\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_p = \\left(\\sum_{i=1}^n |x_i - y_i|^p \\right)^{1/p}\n\\]\nEdit Distance, e.g., Hamming distance:\n\\[\n\\begin{array}{c|c|c|c|c|c}\nx & \\green{1} & \\green{1} & \\red{0} & \\red{1} & 1 \\\\ \\hline\n& ✅ & ✅ & ❌ & ❌ & ✅ \\\\\\hline\ny & \\green{1} & \\green{1} & \\red{1} & \\red{0} & 1 \\\\\n\\end{array} \\; \\leadsto d(x,y) = 2\n\\]\nKL Divergence (Probability distributions):\n\\[\n\\begin{align*}\n\\kl(P \\parallel Q) &= \\sum_{x \\in \\mathcal{R}_X}P(x)\\log\\left[ \\frac{P(x)}{Q(x)} \\right] \\\\\n&\\neq \\kl(Q \\parallel P) \\; (!)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "w06/index.html#onto-the-math-lp-norms",
    "href": "w06/index.html#onto-the-math-lp-norms",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Onto the Math! \\(L^p\\)-Norms",
    "text": "Onto the Math! \\(L^p\\)-Norms\n\nEuclidean distance = \\(L^2\\)-norm:\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_2 = \\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}\n\\]\n\nManhattan distance = \\(L^1\\)-norm:\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_1 = \\sum_{i=1}^n |x_i - y_i|\n\\]\n\nThe maximum(!) = \\(L^\\infty\\)-norm:\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_{\\infty} = \\lim_{p \\rightarrow \\infty}\\left[|| \\mathbf{x} - \\mathbf{y} ||_p\\right] = \\max\\{|x_1-y_1|, \\ldots, |x_n - y_n|\\}\n\\]"
  },
  {
    "objectID": "w06/index.html#top-secret-non-well-defined-yet-useful-norms",
    "href": "w06/index.html#top-secret-non-well-defined-yet-useful-norms",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Top Secret Non-Well-Defined Yet Useful Norms",
    "text": "Top Secret Non-Well-Defined Yet Useful Norms\n\n\n\n\nThe “\\(L^0\\)-norm”\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_0 = \\mathbf{1}\\left[x_i \\neq y_i\\right]\n\\]\n\nThe “\\(L^{1/2}\\)-norm”\n\n\\[\n|| \\mathbf{x} - \\mathbf{y} ||_{1/2} = \\left(\\sum_{i=1}^n \\sqrt{x_i - y_i} \\right)^2\n\\]\n\n\nWhat’s wrong with these norms? (Re-)enter the Triangle Inequality! \\(d\\) defines a norm iff\n\n\\[\n\\forall a, b, c \\left[ d(a,c) \\leq d(a,b) + d(b,c) \\right]\n\\]\n\n\n\n\nVisualizing “circles” in \\(L^p\\) space:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#p_values = [0., 0.5, 1, 1.5, 2, np.inf]\np_values = [0.5, 1, 2, np.inf]\nx, y = np.meshgrid(np.linspace(-3, 3, num=101), np.linspace(-3, 3, num=101))\nfig, axes = plt.subplots(ncols=(len(p_values) + 1)// 2,\n                     nrows=2, figsize=(5, 5))\nfor p, ax in zip(p_values, axes.flat):\n  if np.isinf(p):\n    z = np.maximum(np.abs(x),np.abs(y))\n  else:\n    z = ((np.abs((x))**p) + (np.abs((y))**p))**(1./p)\n  ax.contourf(x, y, z, 30, cmap='bwr')\n  ax.contour(x, y, z, [1], colors='red', linewidths = 2)\n  ax.title.set_text(f'p = {p}')\n  ax.set_aspect('equal', 'box')\nplt.tight_layout()\n#plt.subplots_adjust(hspace=0.35, wspace=0.25)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Plots adapted from this StackOverflow post\n\n\n\n\n\n\nTo go beyond this, and explore how \\(L^p\\) “quasi-norms” can be extremely useful, take DSAN 6100: Optimization!One place they can be useful? You guessed it: facial recognition algorithms (Guo, Wang, and Ruan 2013)"
  },
  {
    "objectID": "w06/index.html#the-value-of-studying",
    "href": "w06/index.html#the-value-of-studying",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "The Value of Studying",
    "text": "The Value of Studying\n\nYou are a teacher trying to assess the causal impact of studying on homework scores\nLet \\(S\\) = hours of studying, \\(H\\) = homework score\n\n\n\n\n\n\n\nSo far so good: we could estimate the relationship via (e.g.) regression\n\n\\[\nh_i = \\beta_0 + \\beta_1 s_i + \\varepsilon_i\n\\]"
  },
  {
    "objectID": "w06/index.html#my-dog-ate-my-homework",
    "href": "w06/index.html#my-dog-ate-my-homework",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "My Dog Ate My Homework",
    "text": "My Dog Ate My Homework\n\nThe issue: for some students \\(h_i\\) is missing, since their dog ate their homework\nLet \\(D = \\begin{cases}1 &\\text{if dog ate homework} \\\\ 0 &\\text{otherwise}\\end{cases}\\)\nThis means we don’t observe \\(H\\) but \\(H^* = \\begin{cases} H &\\text{if }D = 0 \\\\ \\texttt{NA} &\\text{otherwise}\\end{cases}\\)\nIn the easy case, let’s say that dogs eat homework at random (i.e., without reference to \\(S\\) or \\(H\\)). Then we say \\(H\\) is “missing at random”. Our PGM now looks like:"
  },
  {
    "objectID": "w06/index.html#my-dog-ate-my-homework-because-of-reasons",
    "href": "w06/index.html#my-dog-ate-my-homework-because-of-reasons",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "My Dog Ate My Homework Because of Reasons",
    "text": "My Dog Ate My Homework Because of Reasons\n\nThere are scarier alternatives, though! What if…\n\n\n\nDogs eat homework because their owner studied so much that the dog got ignored?\n\n\n\n\n\n\n\nDogs hate sloppy work, and eat bad homework that would have gotten a low score\n\n\n\n\n\n\n\nNoisy homes (\\(Z = 1\\)) cause dogs to get agitated and eat homework more often, and students do worse"
  },
  {
    "objectID": "w06/index.html#tukeys-rule",
    "href": "w06/index.html#tukeys-rule",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Tukey’s Rule",
    "text": "Tukey’s Rule\n\nGiven the first quartile (25th percentile) \\(Q_1\\), and the third quartile (75th percentile) \\(Q_2\\), define the Inter-Quartile Range as\n\n\\[\n\\iqr = Q_3 - Q_1\n\\]\n\nThen an outlier is a point more than \\(1.5 \\cdot \\iqr\\) away from \\(Q_1\\) or \\(Q_3\\); outside of\n\n\\[\n[Q_1 - 1.5 \\cdot \\iqr, \\; Q_3 + 1.5 \\cdot \\iqr]\n\\]\n\nThis is the outlier rule used for box-and-whisker plots:\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n# Generate normal data\ndist_df &lt;- tibble(Score=rnorm(95), Distribution=\"N(0,1)\")\n# Add outliers\noutlier_dist_sd &lt;- 6\noutlier_df &lt;- tibble(Score=rnorm(5, 0, outlier_dist_sd), Distribution=paste0(\"N(0,\",outlier_dist_sd,\")\"))\ndata_df &lt;- bind_rows(dist_df, outlier_df)\n# Compute iqr and outlier range\nq1 &lt;- quantile(data_df$Score, 0.25)\nq3 &lt;- quantile(data_df$Score, 0.75)\niqr &lt;- q3 - q1\niqr_cutoff_lower &lt;- q1 - 1.5 * iqr\niqr_cutoff_higher &lt;- q3 + 1.5 * iqr\nis_outlier &lt;- function(x) (x &lt; iqr_cutoff_lower) || (x &gt; iqr_cutoff_higher)\ndata_df['Outlier'] &lt;- sapply(data_df$Score, is_outlier)\n#data_df\nggplot(data_df, aes(x=Score, y=factor(0))) + \n  geom_boxplot(outlier.color = NULL, linewidth = g_linewidth, outlier.size = g_pointsize / 1.5) +\n  geom_jitter(data=data_df, aes(col = Distribution, shape=Outlier), size = g_pointsize / 1.5, height=0.15, alpha = 0.8, stroke = 1.5) +\n  geom_vline(xintercept = iqr_cutoff_lower, linetype = \"dashed\") +\n  geom_vline(xintercept = iqr_cutoff_higher, linetype = \"dashed\") +\n  #coord_flip() +\n  dsan_theme(\"half\") +\n  theme(\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  scale_x_continuous(breaks=seq(from=-3, to=3, by=1)) +\n  scale_shape_manual(values=c(16, 4))"
  },
  {
    "objectID": "w06/index.html#sigma-rule",
    "href": "w06/index.html#sigma-rule",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "3-Sigma Rule",
    "text": "3-Sigma Rule\n\nRecall the 68-95-99.7 Rule\nThe 3-Sigma Rule says simply: throw away anything more than 3 standard deviations away from the mean (beyond range that should contain 99.7% of data)\n\n\nmean_score &lt;- mean(data_df$Score)\nsd_score &lt;- sd(data_df$Score)\nlower_cutoff &lt;- mean_score - 3 * sd_score\nupper_cutoff &lt;- mean_score + 3 * sd_score\n# For printing / displaying\nmean_score_str &lt;- sprintf(mean_score, fmt='%.2f')\nsd_score_str &lt;- sprintf(sd_score, fmt='%.2f')\nggplot(data_df, aes(x=Score)) + \n  geom_density(linewidth = g_linewidth) +\n  #geom_boxplot(outlier.color = NULL, linewidth = g_linewidth, outlier.size = g_pointsize / 1.5) +\n  #geom_jitter(data=data_df, aes(y = factor(0), col = dist), size = g_pointsize / 1.5, height=0.25) +\n  #coord_flip() +\n  dsan_theme(\"half\") +\n  theme(\n    axis.title.y = element_blank(),\n    #axis.ticks.y = element_blank(),\n    #axis.text.y = element_blank()\n  ) +\n  #geom_boxplot() +\n  geom_vline(xintercept = mean_score, linetype = \"dashed\") +\n  geom_vline(xintercept = lower_cutoff, linetype = \"dashed\") +\n  geom_vline(xintercept = upper_cutoff, linetype = \"dashed\") +\n  geom_jitter(data=data_df, aes(x = Score, y = 0, col = Distribution, shape = Outlier),\n    size = g_pointsize / 1.5, height=0.025, alpha=0.8, stroke=1.5) +\n  scale_x_continuous(breaks=seq(from=-3, to=3, by=1)) +\n  scale_shape_manual(values=c(16, 4)) +\n  labs(\n    title = paste0(\"Six-Sigma Rule, mu = \",mean_score_str,\", sd = \",sd_score_str),\n    y = \"Density\"\n  )"
  },
  {
    "objectID": "w06/index.html#missing-data-outliers-most-important-takeaway",
    "href": "w06/index.html#missing-data-outliers-most-important-takeaway",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Missing Data + Outliers: Most Important Takeaway!",
    "text": "Missing Data + Outliers: Most Important Takeaway!\n\nAlways have a working hypothesis about the Data-Generating Process!\nLiterally the solution to… 75% of all data-related headaches\nWhat variables explain why this data point is missing?\nWhat variables explain why this data point is an outlier?"
  },
  {
    "objectID": "w06/index.html#driving-the-point-home",
    "href": "w06/index.html#driving-the-point-home",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Driving the Point Home",
    "text": "Driving the Point Home\n\n\n\n\n\n\nFigure (and DGP analysis) from D’Ignazio and Klein (2020)\n\n\n\n\nPresumed DGP:\n\n\n\n\\(K\\) = Kidnappings, \\(C\\) = Counts\n\n\nActual DGP:\n\n\n\n\\(K\\) = Kidnappings, \\(R\\) = News reports about kidnappings, \\(C\\) = Counts"
  },
  {
    "objectID": "w06/index.html#references",
    "href": "w06/index.html#references",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "References",
    "text": "References\n\n\nBoyd, Nora Mills, and James Bogen. 2021. “Theory and Observation in Science.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Winter 2021. Metaphysics Research Lab, Stanford University.\n\n\nD’Ignazio, Catherine, and Lauren Klein. 2020. “6. The Numbers Don’t Speak for Themselves.” Data Feminism, March.\n\n\nGalton, Francis. 1919. Inquiries Into Human Faculty and Its Development. J.M. Dent & Company.\n\n\nGuo, Song, Zhan Wang, and Qiuqi Ruan. 2013. “Enhancing Sparsity via \\(\\mathscr{l}\\)p (0.” Neurocomputing 99 (January): 592–602. https://doi.org/10.1016/j.neucom.2012.05.028.\n\n\nJelveh, Zubin, Bruce Kogut, and Suresh Naidu. 2023. “Political Language in Economics.” Economic Journal (Forthcoming). https://doi.org/10.2139/ssrn.2535453.\n\n\nPearson, Karl. 1924. The Life, Letters and Labours of Francis Galton. University Press.\n\n\nShahid, Rizwan, Stefania Bertazzon, Merril L. Knudtson, and William A. Ghali. 2009. “Comparison of Distance Measures in Spatial Analytical Modeling for Health Service Planning.” BMC Health Services Research 9 (1): 200. https://doi.org/10.1186/1472-6963-9-200.\n\n\nWang, Yilun, and Michal Kosinski. 2018. “Deep Neural Networks Are More Accurate Than Humans at Detecting Sexual Orientation from Facial Images.” Journal of Personality and Social Psychology 114 (2): 246–57. https://doi.org/10.1037/pspa0000098."
  },
  {
    "objectID": "w06/index.html#footnotes",
    "href": "w06/index.html#footnotes",
    "title": "Week 6: Exploratory Data Analysis (EDA)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(See next slide)↩︎\n(When I started as a PhD student in Computer Science, I would have answered “yes”. Now that my brain has been warped by social science, I’m not so sure.)↩︎\nGood thing this guy isn’t the father of modern statistics or anything like that 😮‍💨(For more historical scariness take my DSAN 5450: Data Ethics and Policy course next semester! 😉)↩︎"
  },
  {
    "objectID": "extra-videos/recording-w03-ssh-windows.html",
    "href": "extra-videos/recording-w03-ssh-windows.html",
    "title": "Video Series: Using SSH and SCP on Windows",
    "section": "",
    "text": "Since this video had to be recorded in multiple parts, I’m linking each part here:\n\nPart 1: Installing Necessary Apps+Addons on Windows\nPart 2: Setting Up a Linux Terminal on Windows\nPart 3: Using SSH and SCP in the Terminal on Windows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN 5000, Section 02",
    "section": "",
    "text": "This is a “hub” collecting relevant links for each week, for students in the Tuesday section (Section 02) of DSAN 5000: Data Science and Analytics, Fall 2023 at Georgetown University.\nIt is not a replacement for the Main Course Page or the Canvas Page, which are shared across all sections!\nUse the menu on the left, or the table below, to view the resources for a specific week.\n\n\n\n\n\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nWeek 1: Welcome to DSAN 5000!\n\n\nWednesday, August 23, 2023\n\n\n\n\nWeek 2: Data Science Fundamentals and Workflow\n\n\nTuesday, August 29, 2023\n\n\n\n\nWeek 3: Data Science Workflow\n\n\nWednesday, September 6, 2023\n\n\n\n\nWeek 4: Data Gathering and APIs\n\n\nTuesday, September 12, 2023\n\n\n\n\nWeek 5: Data Cleaning\n\n\nTuesday, September 19, 2023\n\n\n\n\nWeek 6: Exploratory Data Analysis (EDA)\n\n\nTuesday, September 26, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/empty-files-error/index.html",
    "href": "writeups/empty-files-error/index.html",
    "title": "The Final Boss of Quarto Errors",
    "section": "",
    "text": "While working on the DSAN 5000 homework, more than one student has run into an extremely opaque issue with Quarto producing the following error:\nSyntaxError: Unexpected end of JSON input\n    at JSON.parse (&lt;anonymous&gt;)\n    at Object.target (file:///Applications/quarto/bin/quarto.js:55201:27)\n    at fileExecutionEngineAndTarget (file:///Applications/quarto/bin/quarto.is:58033:33)\n    at renderContexts (file:///Applications/quarto/bin/quarto.js:82635:40)\n    at Obiect.renderFormats (file:///Applications/quarto/bin/quarto.is: 82688:32)\n    at inputTargetIndex (file:///Applications/quarto/bin/quarto.js: 83504:35)\n    at resolveInputTarget (file:///Applications/quarto/bin/quarto.is:83583:25)\n    at resolveItem (file:///Applications/quarto/bin/quarto.is:93627:32)\n    at navigationItem (file:///Applications/quarto/bin/quarto.js:93598:22)\n    at navbarEisData (file:///Applications/quarto/bin/quarto.is: 93517:34)\nAfter spending kind of a long time bashing my head against this, and eventually giving up for the day, I discovered that some absolute champion GitHub user had already traced the reason for this error on a thread within Quarto’s GitHub page:\nhttps://github.com/quarto-dev/quarto-cli/issues/6392#issuecomment-1660262686\nAnd, their writeup is immensely helpful in general, but I can also provide a quick summary here:\n\n\n\n\n\n\nFixing the Unexpected end of JSON input Error\n\n\n\nIf you encounter this error or an error that looks like this (SyntaxError: Unexpected end of JSON input), and it’s not due to actual JSON code that you’re writing, it’s quite likely that you have an\n\n⚠️ Empty .ipynb file somewhere within your project ⚠️\n\nSo, to fix this, make sure you go through every .ipynb file anywhere in your project and make sure it has at least one cell with some content inside it.\n\n\nFor example, you can go through and make sure that every .ipynb file at least has a first cell that is either a Markdown or Raw cell type, with some YAML-formatted metadata inside this cell, like\n\n\nmy-page.ipynb\n\n---\ntitle: \"My Page\"\nformat: html\n---\n\nQuarto needs this information anyways, to know what to title the file and what format to render it to, so it’s worth doing this even if you’re not seeing the Unexpected end of JSON input error.\nAlso note that this error seems to only happen because of empty .ipynb-format files: empty .qmd files seem to be fine. So, if you have a lot of files in your project and don’t want to go through all of them, you can just focus on the .ipynb files. Note that this error occurs even if you are not referencing or using the ipynb files anywhere in your project! I pulled my non-existent hair out for a long, long time before finding out (thanks to the GitHub post) that the .ipynb files were the issue, despite the fact that I had removed/commented out all references to any .ipynb file (in my _quarto.yml file, for example) anywhere in the project."
  },
  {
    "objectID": "writeups/refs-and-citations/index.html",
    "href": "writeups/refs-and-citations/index.html",
    "title": "Using Quarto’s Reference Manager with Google Scholar",
    "section": "",
    "text": "For your homework and your project, we ask you to integrate references into your Quarto documents, a process which invovles the following steps:\nThis walkthrough will show you the details of each step.\nFirst things first, there are many ways to obtain references, and many formats that the references can come in, but the format used by Quarto is called BibTeX. Long story short, there are two ways to obtain BibTeX-style reference info for a given paper or book. The first way (using Google Scholar) is simple, and will let you immediately generate a references.bib file. The second way (using a reference manager like Zotero or Mendeley) takes a bit longer to get set up, but will save you SO much time in the long run. In this writeup we’ll use the Google Scholar approach, but I highly recommend reading the writeup on using Zotero to learn the Zotero/Mendeley approach as well."
  },
  {
    "objectID": "writeups/refs-and-citations/index.html#step-1-obtaining-references-and-creating-a-.bib-file",
    "href": "writeups/refs-and-citations/index.html#step-1-obtaining-references-and-creating-a-.bib-file",
    "title": "Using Quarto’s Reference Manager with Google Scholar",
    "section": "Step 1: Obtaining References and Creating a .bib File",
    "text": "Step 1: Obtaining References and Creating a .bib File\n\nStep 1.1: Obtaining Your First Reference\nAn easy way to quickly obtain references in a format where you can immediately cite them in your Quarto documents is to use Google Scholar. If you open that link, and search for a topic you’re interested in, it should show you a big list of results that looks like:\n\n\n\nFigure 1: Results from searching for “Einstein” on Google Scholar\n\n\nNow, if you choose one of these references and click the “Cite” button at the bottom of the result, it should display a popup (modal) dialog that looks like the following:\n\n\n\nFigure 2: The popup (modal) dialog which comes up if you click the “Cite” link at the bottom of the Google Scholar result\n\n\nNow, if you click on the “BibTeX” link, the first link at the bottom of this popup (modal) window, it should immediately open a new tab containing a plaintext version of the BibTeX data for this reference, like:\n\n\n\nThe BibTeX data for the selected reference, obtained by clicking the “BibTeX” link at the bottom of the “Cite” popup\n\n\n\n\n\n\n\n\nIf you don’t see this plaintext data, that probably means that your browser is set to download rather than just display .bib files, so just look in your Downloads folder for the downloaded file (it might have some weird name, and it might not have a .bib extension, but that’s ok, it will still open in a text editor like VSCode), open it in a text editor, and it should contain exactly the above data.\n\n\n\nHowever you end up with this plaintext reference data, copy and paste it into a new text file (in VSCode, you can create a new plaintext file using Cmd+N), and save this file as references.bib.\nThat’s it! You now have a fully-fledged references.bib file, that looks as follows:\n\n\nreferences.bib\n\n@book{barnett2005universe,\n  title={The Universe and Dr. Einstein},\n  author={Barnett, Lincoln and Einstein, Albert},\n  year={2005},\n  publisher={Courier Corporation}\n}\n\n\n\nStep 1.2: Adding Additional References\nOur references.bib file only contains one reference at the moment, but we can quickly add more by repeating the above process: find another reference you’d like to cite, click the “Cite” link underneath that reference, click the “BibTeX” link at the bottom of the “Cite” dialog box, and copy the resulting (plaintext) data into your references.bib file. Make sure to separate each reference within the .bib file by at least one space, so that for example it may look like the following:\n\n\nreferences.bib\n\n@book{barnett2005universe,\n  title={The Universe and Dr. Einstein},\n  author={Barnett, Lincoln and Einstein, Albert},\n  year={2005},\n  publisher={Courier Corporation}\n}\n\n@book{einstein2017einstein,\n  title={Einstein on peace},\n  author={Einstein, Albert},\n  year={2017},\n  publisher={Pickle Partners Publishing}\n}"
  },
  {
    "objectID": "writeups/refs-and-citations/index.html#step-2-citing-the-references.bib-file-from-within-quarto",
    "href": "writeups/refs-and-citations/index.html#step-2-citing-the-references.bib-file-from-within-quarto",
    "title": "Using Quarto’s Reference Manager with Google Scholar",
    "section": "Step 2: Citing the references.bib File From Within Quarto",
    "text": "Step 2: Citing the references.bib File From Within Quarto\nNow that you’ve created your references.bib file, and filled it with two references, we need to carry out two final steps if we want to cite references from this file when creating content in a .qmd file.\n\nStep 2.1: Telling Quarto About Your .bib File\nLet’s say you are modifying a file called introduction.qmd, and in this file you’d like to cite one or more of the references contained in references.bib. Hopefully it’s becoming a bit of a habit now, that if we want to give Quarto some information to use when it goes to render the .qmd, we need to include this information within the metadata block at the top of the page. So, if you are editing introduction.qmd, and its metadata block currently looks like\n---\ntitle: \"Introduction\"\nauthor: \"DSAN 5000 Student\"\n---\nYou’ll need to add an additional line to this metadata block telling Quarto where it can find the .bib file—that is, you’ll need to give it a relative path telling it how to get from the introduction.qmd file to the references.bib file. The syntax for this metadata entry looks like bibliography: &lt;path-to-file&gt;. So, if our references.bib file happened to be in the same exact folder as the introduction.qmd file, we could simply add\nbibliography: references.bib\nto the metadata block. Or, if you placed the references.bib file in a subdirectory of the directory containing introduction.qmd (say, a subdirectory called assets), you would need to tell Quarto to look for the .bib file within this subdirectory:\nbibliography: assets/references.bib\nIf the .bib file instead was placed one level above the location of the introduction.qmd file in your directory tree, then the following would tell Quarto to look there:\nbibliography: ../references.bib\nOnce you have pointed Quarto to the location of your references.bib file, all that’s left is to actually cite one or more of the references within the content of introduction.qmd.\n\n\nStep 2.2: Citing references.bib Entries Within a Quarto Document\nFor this final step, recall what the contents of the .bib file you created look like:\n\n\nreferences.bib\n\n@book{barnett2005universe,\n  title={The Universe and Dr. Einstein},\n  author={Barnett, Lincoln and Einstein, Albert},\n  year={2005},\n  publisher={Courier Corporation}\n}\n\n@book{einstein2017einstein,\n  title={Einstein on peace},\n  author={Einstein, Albert},\n  year={2017},\n  publisher={Pickle Partners Publishing}\n}\n\nFrom this structure, we can see that @book tells Quarto that this entry is a book (this could, alternatively, be @article or @conference-proceedings, for example), and then after the line containing this entry type information we see key-value pairs containing the relevant information: the title, author, year and publisher of each book.\nThe key thing to notice in our case, however, is the “code” which is given immediately after the entry type: barnett2005universe and einstein2017einstein. These are called citation keys, and they are what we use to refer to entries in our .bib file from within our .qmd documents!\nSo, for example, if we started writing the content of our introduction.qmd page before we created the references.bib file, it may have looked like:\n\n\n\n\n\n\nintroduction.qmd Before Setting Up References\n\n\n\nIn this project we are studying the life and work of Albert Einstein. Barnett (2005) is a key work about Einstein's scientific contributions, while Einstein (2017) contains a collection of Einstein's writings on peace in the years before and after World War II.\n\n\nThe issue is: If we kept it like this and started writing dozens and dozens of paragraphs, but our boss came along and said “Hey! We need you to switch to a different citation format!”, you would have to go back through your document and manually update each in-text citation to match the new format, plus you would need to (a) manually create a “References” section at the end, and (b) manually update it if your boss requests a change in how the end-of-article references should be formatted.\nSo, instead, we use the citation keys mentioned above (which should be present for each entry in your references.bib file) to indicate to Quarto when we’d like to cite something, and Quarto will handle the rest! If your boss comes along and requests a change, you no longer need to manually update each citation/reference: Quarto allows you to specify formatting as a set of global options, which it then automatically applies to each citation and reference. For example, to utilize this system, we can now update our introduction.qmd file to look as follows:\n\n\n\n\n\n\nintroduction.qmd After Setting Up References\n\n\n\nIn this project we are studying the life and work of Albert Einstein. @barnett2005universe is a key work about Einstein's scientific contributions, while @einstein2017einstein contains a collection of Einstein's writings on peace in the years before and after World War II.\n\n\nAnd when Quarto goes to render this introduction.qmd file (for example, after you run quarto render in the root folder containing your website files), it will see these citation keys indicated by the @ symbol followed by the key from the references.bib file, and style the citation by filling in all of the relevant information for this item from references.bib. To see the difference, the following two blocks show the rendered version of this paragraph without and then with Quarto’s reference-management features set up:\n\n\n\n\n\n\nRendered introduction.qmd Content Without Citation Keys\n\n\n\nIn this project we are studying the life and work of Albert Einstein. Barnett and Einstein (2005) is a key work about Einstein’s scientific contributions, while Einstein (2017) contains a collection of Einstein’s writings on peace in the years before and after World War II.\n\n\n\n\n\n\n\n\nRendered introduction.qmd Content With Citation Keys\n\n\n\nIn this project we are studying the life and work of Albert Einstein. Barnett and Einstein (2005) is a key work about Einstein’s scientific contributions, while Einstein (2017) contains a collection of Einstein’s writings on peace in the years before and after World War II.\n\n\nNotice how, in the rendered version with citation keys:\n\nThe parenthesized year in each citation is now colored blue, and when you hover your mouse over these years it should show a popup containing more information about the cited book.\nQuarto also auto-generated a “References” section for our article, which you should see at the bottom of this page."
  },
  {
    "objectID": "writeups/zotero/index.html",
    "href": "writeups/zotero/index.html",
    "title": "Using Quarto’s Reference Manager with Zotero",
    "section": "",
    "text": "This writeup builds on the more basic approach described in the previous writeup: Using Quarto’s Reference Manager with Google Scholar"
  },
  {
    "objectID": "writeups/zotero/index.html#intro-why-should-i-use-zotero-when-its-more-complicated-than-google-scholar",
    "href": "writeups/zotero/index.html#intro-why-should-i-use-zotero-when-its-more-complicated-than-google-scholar",
    "title": "Using Quarto’s Reference Manager with Zotero",
    "section": "Intro: Why Should I Use Zotero When It’s More Complicated Than Google Scholar?",
    "text": "Intro: Why Should I Use Zotero When It’s More Complicated Than Google Scholar?\nHere, I basically want to convince you to adopt Zotero, with an absolutely crucial extension called Better BibTeX, as your go-to reference manager for the rest of the semester/rest of your academic career, since I can 110% say that it made my life 1000 times easier as I used it more and more throughout the PhD (and, I wish I had started using it sooner!). Not as some sort of awkward self-aggrandizement, but just to show you the power of Zotero, my dissertation ended up having 707 references, and I got literally 5 or 6 emails from Columbia asking me to change the formatting of the references before they would accept it, meaning that without Zotero I would have had to manually update my references\n\\[\n\\underbrace{(707 \\times 6)}_{\\text{Updating in-text}\\atop\\text{citations}} + \\underbrace{(707 \\times 6)}_{\\text{Updating references}\\atop\\text{section}} = 8,484\\text{ times}\n\\]\nwhereas with Zotero it just meant updating a single global setting 6 times.\nHowever, you’ll notice that this is basically the selling point I already gave for Quarto’s references manager (which integrates with Zotero) in the previous writeup. The real selling point of Zotero over manually downloading BibTeX entries using Google Scholar comes from:\n\nThe Zotero Connector, which is a browser extension that lets you click a single button to instantly add any article you’re reading to your Zotero library, and\nThe Better BibTex Extension for Zotero, which truly transforms you into a citation master by automatically syncing your Zotero library with different .bib files across your hard drive.\n\nLong story short, let’s first think about the pipeline of steps you need to perform when using Google Scholar, if you are browsing the web and happen to find an interesting article/book somewhere that you’d like to integrate into your Quarto website:\n\nRemember the name of the article\nOpen up Google Scholar\nSearch the article on Google Scholar\nFind the search result corresponding to the article you are interested in citing (if the article has some generic title like “Postmodernism” or something, it may be many many pages down in the list)\nClick the “Cite” link\nClick the “BibTeX” link\nCopy the BibTeX entry\nManually look through your hard drive for any .bib files that you’d like to update to contain this entry, open them, and paste the entry at the end of the file.\n\nWith Zotero plus these two addons installed, these steps literally become:\n\nClick the Zotero Connector button\n\nAutomatically, immediately after you click this button, Zotero\n\nAdds an entry to your library for this article/book\nScrapes metadata from the page to autofill the various fields: title, authors, date, etc.\nAssigns a citation key to the article/book, based on a template you give it1.\nAutomatically goes out to every .bib file on your computer that you asked it to keep in sync with your library, and adds this entry to the end."
  },
  {
    "objectID": "writeups/zotero/index.html#downloading-zotero-zotero-connector-and-better-bibtex-extension",
    "href": "writeups/zotero/index.html#downloading-zotero-zotero-connector-and-better-bibtex-extension",
    "title": "Using Quarto’s Reference Manager with Zotero",
    "section": "Downloading Zotero, Zotero Connector, and Better BibTeX Extension",
    "text": "Downloading Zotero, Zotero Connector, and Better BibTeX Extension\nSo, if you’re sold on Zotero, you can download the main Zotero program here (there are both Windows and Mac versions).\nOnce you’ve download and installed it, like I mentioned, there is a crucial second piece of the puzzle: kind of like how we had to install both Quarto and the Quarto VSCode Extension to fully Quartify our lives, to fully Zoterify our lives we need both the main Zotero app as well as the Zotero Connector. This is literally a magic button that takes whatever article/book you’re reading, figures out its title/author/year/etc., and adds it to your library.\nThe third and final piece of the puzzle is the Better BibTeX Extension for Zotero. This is the piece that lets you:\n\nSpecify templates for how you would like Zotero to create the citation keys for any article you add via the magic button (see previous footnote), and\nAutomatically update .bib files across your hard drive, to stay in sync with your Zotero library."
  },
  {
    "objectID": "writeups/zotero/index.html#adding-quarto-back-into-the-mix",
    "href": "writeups/zotero/index.html#adding-quarto-back-into-the-mix",
    "title": "Using Quarto’s Reference Manager with Zotero",
    "section": "Adding Quarto Back Into the Mix",
    "text": "Adding Quarto Back Into the Mix\nIf you’ve downloaded Zotero, Zotero Connector, and Better BibTeX, there are really only four remaining steps:\n\nCreate a collection within Zotero, which you’ll use to keep track of all the references that you want to include in a given Quarto project. For example, I have a Zotero collection called “DSAN5000”, where I add (using the magic button) any DSAN5000-related articles. Then,\nRight-click on this collection, select “Export Collection…”, and instead of the standard choices built into Zotero, choose the “Better BibTeX” format. Make sure you check the “Keep Updated” checkbox, and then click “OK”:\n\n\n\nChoose where you’d like it to export the .bib file to (for example, in the same directory as some Quarto file you’re working on, like my_article.qmd), and click “Save”.\nGo into your Quarto document, like my_article.qmd, and add the following line to the YAML metadata block at the top of the file:\n\nbibliography: references.bib\n(if you chose a different name from references.bib when exporting from within Zotero just now, use that name instead)\nThat’s it, you’re done! Better BibTeX keeps track of all the .bib files that you’ve chosen as export locations for this collection, and every time you use the Magic Button to save a book/article to this collection, all of these .bib files will automagically update to contain the new book/article, so that you can immediately cite it in my_article.qmd."
  },
  {
    "objectID": "writeups/zotero/index.html#footnotes",
    "href": "writeups/zotero/index.html#footnotes",
    "title": "Using Quarto’s Reference Manager with Zotero",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, my template is something like lower(author(1).last)_year_lower(title(1)), which tells Zotero to automatically set the key to be the last name of the first author, then the year, and then the first word of the title, all lowercased.↩︎"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Extra Writeups",
    "section": "",
    "text": "Order By\n       Default\n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\nCategory\n\n\n\n\n\n\nExploratory Data Analysis of the THOR Dataset in Seaborn\n\n\nSunday, October 1, 2023\n\n\nExtra Writeups\n\n\n\n\nSplitting a String Column in Tidyverse\n\n\nFriday, September 29, 2023\n\n\nExtra Writeups\n\n\n\n\nMultiple-Column Layout in Quarto\n\n\nThursday, September 28, 2023\n\n\nExtra Writeups\n\n\n\n\nDSAN 5000 Quiz Status Sheet\n\n\nThursday, September 28, 2023\n\n\nExtra Writeups\n\n\n\n\nManaging Python Installations\n\n\nWednesday, September 27, 2023\n\n\nExtra Writeups\n\n\n\n\nRegular Expressions for Data Cleaning\n\n\nTuesday, September 26, 2023\n\n\nExtra Writeups\n\n\n\n\nData Cleaning with Python\n\n\nWednesday, September 20, 2023\n\n\nExtra Writeups\n\n\n\n\nThe Final Boss of Quarto Errors\n\n\nTuesday, September 19, 2023\n\n\nExtra Writeups\n\n\n\n\nLab 1.2 Clarifications\n\n\nSaturday, September 16, 2023\n\n\nClarifications\n\n\n\n\nUsing Quarto’s Reference Manager with Zotero\n\n\nFriday, September 15, 2023\n\n\nExtra Writeups\n\n\n\n\nTroubleshooting SSH/SCP/rsync on Georgetown Domains\n\n\nThursday, September 14, 2023\n\n\nExtra Writeups\n\n\n\n\nHomework 1 Clarifications\n\n\nWednesday, September 13, 2023\n\n\nClarifications\n\n\n\n\nUsing Quarto’s Reference Manager with Google Scholar\n\n\nWednesday, September 13, 2023\n\n\nExtra Writeups\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/tidyverse-split-column/index.html",
    "href": "writeups/tidyverse-split-column/index.html",
    "title": "Splitting a String Column in Tidyverse",
    "section": "",
    "text": "In this quick writeup, I will show you how to take a string-format column in R and split each of this single column’s values into multiple columns, using the separate_wider_*() functions in tidyverse.\nSay we start out with a dataset that looks like the following (where I’m using the tibble library’s ultra-helpful tribble() function, to quickly make a “fake” dataset to demonstrate the approach here):\n\nlibrary(tibble)\ndf &lt;- tribble(\n    ~id, ~name,\n    0, \"Gamage, Purna\",\n    1, \"Hickman, James\",\n    2, \"Jacobs, Jeff\",\n    3, \"Padalkar, Nakul\"\n)\ndf\n\n\n\n\n\nid\nname\n\n\n\n\n0\nGamage, Purna\n\n\n1\nHickman, James\n\n\n2\nJacobs, Jeff\n\n\n3\nPadalkar, Nakul\n\n\n\n\n\n\nWe see that, although the name column really contains two pieces of information (given name and family name) that we’d like to consider separately, these two pieces of information are “trapped” within a single string-format column called name.\nSo, our goal is to separate these two pieces of information within the name column, ideally into two columns called given_name and family_name.\nThe tidyverse library called tidyr provides us with three functions that can help us with precisely this task:\n\nseparate_wider_delim()\nseparate_wider_position()\nseparate_wider_regex()\n\nWe’ve seen in other writeups how to use Regular Expressions, so we could use the separate_wider_regex() function, but for this task we don’t even really need to get that fancy: we don’t need a whole language (regular expressions) to split the string in this case, since it boils down to a simple character-matching task, where we just want to tell R to split into two columns wherever it sees the character , followed by a space .\nWe don’t want to use separate_wider_position() here, since that function is for cases where you know the string contains one piece of information from the start of the string to some index i within the string, then a second piece of information from index i+1 up to the end of the string (if I knew, for example, that everyone in the dataset had a 5-letter given name and a 7-letter family name, then separate_wider_position() would work).\nSo, the function which remains is separate_wider_delim(), which we’ll use for our task.\nAs you can infer from the first example provided on the documentation page for the three functions, separate_wider_delim() has the following syntax:\nseparate_wider_delim(\n    &lt;data frame object&gt;,\n    &lt;name of column you want to split&gt;,\n    delim = &lt;character(s) you'd like to split on&gt;,\n    names = &lt;vector of new column names&gt;\n)\nSo, in our case, we know that we want to plug in the following values:\n\nData frame object: df\nName of column we want to split: name\nCharacter(s) we’d like to split on: ,\nVector of new column names: c(\"family_name\",\"given_name\")\n\nSo, let’s plug these parameters into the function, and check the result (remember that the pipe operator |&gt; in R takes whatever comes before it and plugs that in as the first argument to whatever comes after it: meaning that, here, we don’t explicitly include df as the first argument to separate_wider_delim(); tidyverse handles the first argument for us)\n\nlibrary(tidyr)\ndf |&gt; separate_wider_delim(\n    `name`,\n    delim = \", \",\n    names = c(\"family_name\", \"given_name\")\n)\n\n\n\n\n\nid\nfamily_name\ngiven_name\n\n\n\n\n0\nGamage\nPurna\n\n\n1\nHickman\nJames\n\n\n2\nJacobs\nJeff\n\n\n3\nPadalkar\nNakul\n\n\n\n\n\n\nAnd now we have what we want! Hopefully this function from tidyverse can help you with data cleaning, merging, etc., since I find it much more intuitive to use than e.g. the gsub() function from base-R (though that function is fine to use as well, if you are used to how gsub() works!)"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "",
    "text": "Colab Link\n\n\n\n\nClick here to open in Colab"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#lab-background",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#lab-background",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(1) Lab Background",
    "text": "(1) Lab Background\n\n(1.1) The Vietnam / Second Indochinese War\n\nA massive bombing campaign in Cambodia. Anything that flies on anything that moves.–Henry Kissinger, US National Security Advisor, Dec. 9, 1970.\n\nIn 1965, the United States launched Operation Rolling Thunder, a years-long aerial assault on Vietnam which devastated the country and its people (killing over 3 million), irreversably damaging its environment (with more extensive use of chemical weapons like Napalm than in any war before or since), its crops and forests (with 19 million gallons of Agent Orange sprayed over 500,000 acres of crops and 5 million acres of forests), and its infrastructure. As a campaign of genuinely unprecedented ferocity and destruction launched by a military superpower against a poor, mostly rural/agrarian country (only 20% of its citizens lived in cities at the time), its devastating effects are still felt to this day by those in Vietnam, Laos, and Cambodia.\n(All of which is all my not-so-subtle chance to plug, before moving onto the data analysis, the donation page for the HALO Trust: an org headquartered right here in NW DC which is leading the worldwide effort to defuse unexploded landmines in these countries, where for example 50,000 Laotians have been killed by these mines since the war “officially” ended)\n\n\n(1.2) The Role of Data Science in the Devastation\nAs an example of a war where modern science and technology was seen as a central military resource (with chemistry research leading to the deployment of Agent Orange and Napalm, computer science research used to track and target individuals via early instances of machine learning models, and social science research used to e.g. justify driving villagers from their homes into concentration camps called “Strategic Hamlets”), it is an important piece of history for data scientists to grapple with: how the technologies we develop could be used to help people, but also could be used to inflict unimaginable horrors upon millions of innocent civilians. Datasets pertaining to this war, many of which have only become publicly/easily-available in the past few years, provide the perfect opportunity to use data science itself to study this example of a military “data science project” of a prior generation.\n\n\n(1.3) EDA and Bayesian Statistics: Challenging Prior Beliefs to Derive Posterior Beliefs\nAlthough the entire point of EDA in theory is to go in “without any assumptions” (but see below/slides), I’ll give a quick summary of the impression(s) I came away with after the cleaning, tabulating, and visualizing below, in combination with what I knew about it beforehand, so (a) you know what to expect going in and/or (b) so you can use it as a “prior” that you update while working through the visualizations (challenging received assumptions, rather than pretending we have none, should be the goal of EDA imo!)\nBriefly: although Operation Rolling Thunder is often cited as the beginning of what people in the US usually call the “Vietnam War”, by looking at the basic collection of tabulations and visualizations herein it slowly emerges that probably the most horrifying and unprecedented destruction was experienced not even by those in Vietnam itself but by those in Laos over the timespan represented in the dataset (and, after a US-sponsored coup in 1970, Cambodia as well). I knew beforehand, for example, that Laos was the most-bombed (per square meter) territory in history, but in terms of the goals of EDA I definitely found my priors about the Vietnam War challenged by what the tables/plots seemed to be “saying” about the underlying historical event.\nAs a final piece of information to have as part of your prior knowledge, though, before we dive in: Keep in mind that although the numbers we’ll be examining are mostly in absolute units such as the number of bombings, they should be interpreted in the context of the relative populations of each country in the dataset, at the beginning of the campaign:\n\n\n\n\n\n\n\n\n\nCountry\nPopulation\nYear\nSource\n\n\n\n\nCambodia\n6.2 million\n1965\nWorld Bank\n\n\nLaos\n2.4 million\n1965\nWorld Bank\n\n\nDemocratic Republic of Vietnam(North Vietnam)\n17.7 million\n1965\nCIA Intelligence Report (via CREST), 1966, p. 26\n\n\nRepublic of Vietnam(South Vietnam)\n16 million\nEnd of 1964\nCIA Intelligence Report (via CREST), 1968, p. 11\n\n\n\n\n\n(1.4) Dataset Info\nThe dataset we’ll be analyzing is called the Vietnam War Theater History of Operations (THOR) dataset, released by the US Department of Defense in 2015. The raw data is too large (~1.5GB) to easily download and work with, so I’ll be using a reduced-form version below, but the full dataset is available at the previous link (from data.world).1"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#loading-the-data",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#loading-the-data",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(2) Loading the Data",
    "text": "(2) Loading the Data\nAs you should slowly be getting in the habit of doing, we start by importing:\n\nPandas (for dataset storage and manipulation),\nNumPy (for any mathematical transformations we might want to apply), and\nSeaborn (for visualization, whether EDA-focused or otherwise)\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nNext we download the dataset. Technically Pandas’ read_csv() function does support URL arguments, so that we could load the dataset directly into Pandas via URL, but since we often open and close notebooks, or Jupyter crashes, to avoid re-downloading each time you can run the following line, which should download the .csv file into the same directory as wherever this notebook is stored (on Colab, for example, the same portion of your Google Drive that Colab allocates for the storage of the notebook).\nSince we’ve already used Python’s requests library before, for data scraping, we’ll use that here to quickly request and save the file:\n\nimport requests\nthor_url = \"https://jpj.georgetown.domains/dsan5000-scratch/eda/thor_strikes.csv\"\nwith open(\"thor_strikes.csv\", 'wb') as outfile:\n  csv_content = requests.get(thor_url, stream=True).content\n  outfile.write(csv_content)\n\nIf the download was successful, you should have a thor_strikes.csv file in the same folder as this notebook (in Colab you can check the file tab in the sidebar on the left side of the page). We can then use pd.read_csv() to load the .csv-formatted dataset, and we can re-load the dataset again in the future without having to worry about re-downloading it.\n\nstrike_df = pd.read_csv(\"thor_strikes.csv\")\n\nAnother good habit you can get into is always checking the output of the .head() function after loading the dataset, just to make sure that everything loaded as you expected. It usually doesn’t (😅)\n\nstrike_df.head()\n\n\n\n\n\n\n\n\nMFUNC_DESC\nMISSIONID\nTGTCOUNTRY\nTHOR_DATA_VIET_ID\nMSNDATE\nSOURCEID\nNUMWEAPONSDELIVERED\nTGTTYPE\nTGTLATDD_DDD_WGS84\nTGTLONDDD_DDD_WGS84\n\n\n\n\n0\nSTRIKE\n1047\nLAOS\n4\n1970-02-02\n642780\n2\nTRUCKS\n16.902500\n106.014166\n\n\n1\nSTRIKE\n1407\nLAOS\n6\n1970-11-25\n642782\n6\nAAA\\37MM CR MORE\n19.602222\n103.597222\n\n\n2\nSTRIKE\n9064\nLAOS\n7\n1972-03-08\n642783\n0\nTRUCKS\n14.573611\n106.689722\n\n\n3\nSTRIKE\n8630\nLAOS\n16\n1971-05-12\n642792\n4\nTRK\\PRK\\STORE AREA\n17.563611\n105.756666\n\n\n4\nSTRIKE\n1391\nLAOS\n18\n1971-12-19\n642794\n0\nPERSONNEL\\ANY\n16.864166\n105.349166\n\n\n\n\n\n\n\nIn our case, though, the output looks pretty much as expected, in terms of Pandas being able to automatically detect the header row and the splits between columns. So, let’s move on to the EDA!"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#missing-data",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#missing-data",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(3) Missing Data",
    "text": "(3) Missing Data\nFirst things first, before we worry about missing data itself, we should think about which variables we want to analyze, since for now we can just worry about checking for missing values within the columns corresponding to these variables.\nThe official 59-page codebook for the dataset, an absolutely crucial thing to have on hand during any data analysis, can be found at this link. To skip over you having to read a 59-page document, though, I’ll now introduce a shortened, simplified codebook.\nIn our case, since we’re doing exploratory data analysis, in theory we shouldn’t have any hypotheses in mind yet. In reality we do have hypotheses in our heads, subconsciously at least, so when I say that I just mean that we’re not directly trying to confirm or deny these hypotheses at this stage. We are just hoping to question/interrogate them as we go along…\nKeep in mind Tukey’s metaphor, that:\n\nEDA = detective work, collecting evidence for a future trial, while\nCDA = the trial itself!\n\nI’m just going to focus on a few key variables which are central to understanding the processes underlying the data (the destruction of the countries across former Indochina). Let’s also think about what type of data each column represents—categorical vs. numeric, ordinal vs. cardinal, etc. Just like in the lab we can then use this info, as recorded in the following table, as our codebook for understanding what each variable represents:\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\nVariable Type\nDatatype\n\n\n\n\nTGTCOUNTRY\nThis should tell us, for a given strike, the country within whose borders the strike was carried out.\nCategorical\nstring\n\n\nMSNDATE\nThis should tell us the date (in Y/M/D format) that the strike was carried out.\nDiscrete numeric (date)\nobject(!) (see below)\n\n\nTGTTYPE\nThis should tell us the type of target, e.g., whether it was a vehicle, a group of people, an individual person, etc.\nCategorical\nstring\n\n\nTGTLATDD_DDD_WGS84\nThe latitude coordinate of the target\nContinuous\nfloat\n\n\nTGTLONDD_DDD_WGS84\nThe longitude coordinate of the target\nContinuous\nfloat\n\n\n\nAs our first EDA task, let’s look into the first variable in the table above: TGTCOUNTRY. As the section header suggests, we’re going to see if this column has any missing data and, if so, what the missing data means and what to do about it.\n[Crucial point here:] If we started using Pandas methods with their default arguments, we might get to thinking that nothing is wrong here, and move on from missing data checks… Take a look at what the value_counts() function returns when we use this function to look at the distribution of values in TGTCOUNTRY without including any additional parameters:\n\nstrike_df['TGTCOUNTRY'].value_counts()\n\nTGTCOUNTRY\nLAOS              690161\nNORTH VIETNAM     245977\nSOUTH VIETNAM      54253\nCAMBODIA           13271\nTHAILAND             146\nUNKNOWN               23\nWESTPAC WATERS         6\nName: count, dtype: int64\n\n\nAll seems well from this table: we might think “ok, great, they have encoded the 23 missing values from the original non-digitized data as \"UNKNOWN\", and all the other values are interpretable non-missing labels, so we’re good right?”\nSadly we’re NOT GOOD 😭. Let’s check one more thing, to slowly move towards the hidden issue here. Let’s start by summing up the counts that value_counts() has given us for each value:\n\nvalue_counts_sum = strike_df['TGTCOUNTRY'].value_counts().sum()\nvalue_counts_sum\n\n1003837\n\n\nNext let’s take a look at the total number of rows and columns in our DataFrame, by checking the shape attribute\n[Here it’s important to note that, unlike most of the other attributes that we use to analyze DataFrame objects, shape is not a function but a static variable, an attribute of the DataFrame object. That’s why we just use .shape rather than .shape() to access this information.]\n\nnum_rows = strike_df.shape[0]\nnum_rows\n\n1007674\n\n\nDo you see the issue? If not, let’s check the difference between these two values:\n\nnum_rows - value_counts_sum\n\n3837\n\n\nThis tells us, to our horror, that there are 3,837 rows in our DataFrame which are completely unaccounted-for in the output produced by value_counts() 😰\nAnd the reason is, as it turns out, the value_counts() function excludes missing values by default, so that the above output actually tells us absolutely nothing about whether or not there are missing values in the column! Scary stuff.\nThere is an easy fix, however: we can just include the dropna = False argument in our call to the value_counts() function, which will override this default behavior and show missing values as one of the possible values in the table of counts:\n\nstrike_df['TGTCOUNTRY'].value_counts(dropna = False)\n\nTGTCOUNTRY\nLAOS              690161\nNORTH VIETNAM     245977\nSOUTH VIETNAM      54253\nCAMBODIA           13271\nNaN                 3837\nTHAILAND             146\nUNKNOWN               23\nWESTPAC WATERS         6\nName: count, dtype: int64\n\n\nAnd now we see that, in fact, there are thousands of strikes for which no target country was recorded in the dataset: about 167 times more missing data than we originally thought when we only saw that 23 of the rows had a TGTCOUNTRY value of \"UNKNOWN\".\nSo, what can we do about this? In later weeks of the course and/or in later weeks of DSAN5100 we’ll learn about some more advanced methods for inputing missing values, for example by explicitly modeling the data-generating process that led some of the values to be missing. Until we know how to do that, however, for the sake of moving on with the EDA we will ignore these rows when we move onto other EDA tasks, keeping in mind that this is NOT acceptable practice in general for data science, outside of tutorial examples like this!!\nBefore we move to those tasks, however, let’s quickly look at how we can visualize the distribution of missing values in this column (along with the other columns), using the third-party missingno package mentioned in the lab instructions!\n\n!pip install missingno\n\nRequirement already satisfied: missingno in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (0.5.2)\nRequirement already satisfied: numpy in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from missingno) (1.26.0)\nRequirement already satisfied: matplotlib in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from missingno) (3.8.0)\nRequirement already satisfied: scipy in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from missingno) (1.11.2)\nRequirement already satisfied: seaborn in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from missingno) (0.12.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (1.1.1)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (4.42.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (23.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (10.0.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (3.1.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from matplotlib-&gt;missingno) (2.8.2)\nRequirement already satisfied: pandas&gt;=0.25 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from seaborn-&gt;missingno) (2.1.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pandas&gt;=0.25-&gt;seaborn-&gt;missingno) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from pandas&gt;=0.25-&gt;seaborn-&gt;missingno) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;missingno) (1.16.0)\n\n\nI don’t want to give away the answer to this portion of the lab by producing a missing-data matrix directly, so instead I will use a different but still very helpful function from missingno, the msno.bar() function, to visualize missingness across the different variables in our DataFrame:\n\nimport missingno as msno\n\n\nmsno.bar(strike_df)\n\n&lt;Axes: &gt;\n\n\n\n\n\nAlthough we see in hindsight that the visual properties of this plot would not have been that useful in discovering the missing values in TGTCOUNTRY (I say “visual properties” because it still helpfully provides the non-missing counts at the top of each bar, which could have allowed us to see this issue), the plot is very useful for discovering the missingness prevalent in the TGTTYPE variable. So, just like in the case of TGTCOUNTRY, we will move on while keeping in mind that this variable has an even more extreme issue with missingness that will bias our results (until we learn how to explicitly incorporate and de-bias the missingness!)"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#different-formats-within-the-same-column",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#different-formats-within-the-same-column",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(4) Different Formats Within The Same Column",
    "text": "(4) Different Formats Within The Same Column\n\n(4.1) The Ambiguous object Type\nAs our next EDA task, let’s look into that (!) in the table above: while the rest of the variables are in fairly unambiguous formats, object is a particularly scary datatype in Pandas, since “object” in fact just means that the datatype of a given value in this column could be anything. If you know Java, for example, you know that Object is the datatype that all other datatypes are subtypes of. Similarly, in Python, every class implicitly inherits the properties of the object class.\nLooking at the output of head() from above, however, you may reasonably think that the entries in this column will all be of string type. And this instinct would be further enforced by checking the datatypes of the first five entries in the column:\n\nstrike_df['MSNDATE'].head().apply(type)\n\n0    &lt;class 'str'&gt;\n1    &lt;class 'str'&gt;\n2    &lt;class 'str'&gt;\n3    &lt;class 'str'&gt;\n4    &lt;class 'str'&gt;\nName: MSNDATE, dtype: object\n\n\nTherefore, you might start moving ahead, working with this column as if it is a string column. You should push back on this instinct!! It will save you so many headaches later on, when you are neck-deep in the data analysis, if you verify what’s going on with any columns that Pandas loaded as object type right at the beginning!\nFor example, rather than drawing inferences about the entire dataset from only the first five columns (a procedure jokingly referred to as “engineer’s induction”), we can do something a bit less error-prone by making a new column MSNDATE_type which just tells us the type of each value in the MSNDATE column, then checking whether or not every entry in MSNDATE_type is string. If it is, then we’re fine adopting the it’s-a-string-variable assumption and moving on. But if it’s not, we’ll have to do something to handle the mismatch. Let’s see what happens. We’ll make the column, then we’ll use the value_counts() function from Pandas to see the distribution of datatypes in the column (where now we remember that we pretty much always want to include the dropna = False parameter):\n\nstrike_df['MSNDATE_type'] = strike_df['MSNDATE'].apply(type)\n\n\nstrike_df['MSNDATE_type'].value_counts(dropna=False)\n\nMSNDATE_type\n&lt;class 'str'&gt;    1007674\nName: count, dtype: int64\n\n\nSo far so good: Pandas loaded the column as type object, which often means but does not always mean that the column is filled with string variables. There is still one more issue with this column, however, looming on the horizon…\n\n\n(4.2) Splitting The Column Into Parts\nSince in general we want to analyze the yearly volume of strikes in each country, the format of the values in the MSNDATE column is not that helpful to us at the moment. So, again we might proceed by intuition, using engineer’s induction on the first five rows of the DataFrame to infer that we can just split on the - character to quickly divide this single column into three separate columns: one for year, one for month, and one for day.\nLet’s see what happens when we try this out.\nWriting the Parse Function\nIn Python, unlike the nice separate_wider_*() functions in R that I talked about in this writeup, I personally haven’t found all that many helpful functions specially-made for splitting a string up into pieces.\nSo, what I usually do in this type of situation (but see a few sections below) is utilize the following setup: as a nice “trick” for generating multiple columns from one individual column, Pandas allows you to write functions which return pd.Series objects (these Series objects are sort of like one-dimensional DataFrames), and then use this function to set the values of multiple columns in one “sweep” using syntax like:\ndef function_returning_series_object(value):\n  \"\"\"\n  A function that will take in a single value from a column and\n  return a pd.Series object, where the **keys** in the pd.Series\n  correspond to the **names of the columns** you're hoping to create\n  \"\"\"\n  // Do something with `value` to generate these results\n  resulting_x_value = 'first result'\n  resulting_y_value = 'second result'\n  resulting_z_value = 'third result'\n  series_values = {\n    'x' = resulting_x_value,\n    'y' = resulting_y_value,\n    'z' = resulting_z_value\n  }\n  return pd.Series(series_values)\n\n// Specify the names of the columns you're about to create\ncolumns_to_create = ['x','y','z']\n// And use the apply() function to \"send\" the values of some existing column\n// to the function_returning_series_object() function defined above\ndf[columns_to_create] = df['some_existing_column'].apply(function_returning_series_object)\nIn our case, so you can see this in action, let’s define a function that will take in the values of the MSNDATE column (which we expect to have the format YY-MM-DD, split these values on the - character, and return the three resulting pieces in pd.Series form with the keys year, month, and day.\n\ndef split_date(date_val):\n  date_elts = date_val.split(\"-\")\n  date_values = {\n      'year': date_elts[0],\n      'month': date_elts[1],\n      'day': date_elts[2]\n  }\n  return pd.Series(date_values)\n\n\n\n\n\n\n\nThe tqdm Library\n\n\n\nQuick Note: tqdm\nWhen we use the apply() function from Pandas to apply our split_date() function to each value of the MSNDATE column, it will take kind of a long time to iterate through and parse each date. For this reason, I’m including example code here showing how to import and use the third-party Python library tqdm to create a new progress_apply() function. If the waiting feels tedious on your machine, for example, you can call progress_apply() in the place of apply(), which will display a progress bar showing far along it is. In Colab this will work right away, since tqdm comes pre-installed. Depending on your environment it may crash though, since some versions of tqdm are incompatible with some versions of pandas, so to be safe I’m sticking with the regular .apply() function\n\n\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nBack to EDA\nThe running time of the progress_apply() call will be bad enough, but as we’ll see, it gets even worse: right near the end, about 63% of the way through, the code will crash with an error! (You can uncomment the following code, run it, and wait a bit to see the error if you’d like, or just jump past the following cell)\n\n## And apply it to MSNDATE\n#columns_to_create = ['year','month','day']\n#strike_df[columns_to_create] = strike_df['MSNDATE'].progress_apply(split_date)\n\nThe error looks like:\n&lt;ipython-input-35-ec79f5e7def4&gt; in split_date(date_val)\n      3   date_values = {\n      4       'year': date_elts[0],\n----&gt; 5       'month': date_elts[1],\n      6       'day': date_elts[2]\n      7   }\n\nIndexError: list index out of range\nSo what happened?\nLong story short, engineer’s induction failed us again. It turns out that some entries within the MSNDATE column contain dates in a different format from the format we saw in the first 5 rows of the dataset. But how did I magically figure this out?\nLet’s start by carrying out Engineer’s Induction 2.0 (TM), by looking at the tail of strike_df rather than the head. Sometimes, if the column has one format at the beginning, but another near the end, this will show us the alternative format:\n\nstrike_df.tail()\n\n\n\n\n\n\n\n\nMFUNC_DESC\nMISSIONID\nTGTCOUNTRY\nTHOR_DATA_VIET_ID\nMSNDATE\nSOURCEID\nNUMWEAPONSDELIVERED\nTGTTYPE\nTGTLATDD_DDD_WGS84\nTGTLONDDD_DDD_WGS84\nMSNDATE_type\n\n\n\n\n1007669\nSTRIKE\n1405\nLAOS\n4559448\n1973-01-01\n58102\n0\nPERSONNEL\\ANY\n15.191111\n106.206666\n&lt;class 'str'&gt;\n\n\n1007670\nSTRIKE\n8719\nLAOS\n4560531\n1972-05-08\n68563\n0\nNaN\n16.758055\n106.461944\n&lt;class 'str'&gt;\n\n\n1007671\nSTRIKE\n9526\nSOUTH VIETNAM\n4561255\n1972-07-10\n69287\n0\nNaN\n16.684166\n107.053888\n&lt;class 'str'&gt;\n\n\n1007672\nSTRIKE\n0692\nLAOS\n4650354\n1970-11-28\n527864\n0\nTRUCKS\n17.286388\n105.609722\n&lt;class 'str'&gt;\n\n\n1007673\nSTRIKE\n7232\nLAOS\n4651825\n1971-10-19\n541658\n4\nNaN\n15.292777\n107.137500\n&lt;class 'str'&gt;\n\n\n\n\n\n\n\nWe’re not so lucky this time. The last 5 values of the MSNDATE column look like they’re formatted the same way as the first 5 values are.\nSo, next let’s do something a bit smarter, and just check how many - characters each entry has—we can code this check in a very similar way to how we coded the check above when we looked at the type of each entry in the column.\nIn fact, here we don’t even need to come up with our own function and use apply() or anything like that, since Pandas has built-in functionality allowing us to access (by using the .str suffix after extracting just the string-format column) any of Python’s built-in functions for strings, like len() or replace(), at which point Pandas will then automatically apply our chosen to each value in the column.\nSo, let’s use this .str suffix to apply the built-in count() function (that Python provides for all string objects) to each value of MSNDATE, counting the number of times that - appears in each value, then look at the distribution of these counts across the entire column:\n\nstrike_df['MSNDATE_dashcount'] = strike_df['MSNDATE'].str.count(\"-\")\n\n\nstrike_df['MSNDATE_dashcount'].value_counts()\n\nMSNDATE_dashcount\n2    993524\n0     14150\nName: count, dtype: int64\n\n\nAnd we’ve found our issue: 14,150 of the rows do not use a YYYY-MM-DD format! Since we now have this MSNDATE_dashcount column available to use, let’s filter the full DataFrame to look at just (the first few) rows where it is equal to 0, i.e., rows where the MSNDATE column does not contain any dashes:\n\nstrike_df.loc[strike_df['MSNDATE_dashcount'] == 0,].head()\n\n\n\n\n\n\n\n\nMFUNC_DESC\nMISSIONID\nTGTCOUNTRY\nTHOR_DATA_VIET_ID\nMSNDATE\nSOURCEID\nNUMWEAPONSDELIVERED\nTGTTYPE\nTGTLATDD_DDD_WGS84\nTGTLONDDD_DDD_WGS84\nMSNDATE_type\nMSNDATE_dashcount\n\n\n\n\n639026\nSTRIKE\nJH301\nSOUTH VIETNAM\n2876015\n19700409\n258195\n12\nAREA\\DEPOT\n14.825953\n107.716270\n&lt;class 'str'&gt;\n0\n\n\n639027\nSTRIKE\nJH309\nSOUTH VIETNAM\n2876638\n19700216\n262548\n24\nAREA\\DEPOT\n14.510215\n108.768815\n&lt;class 'str'&gt;\n0\n\n\n639028\nSTRIKE\n01305\nSOUTH VIETNAM\n2876852\n19651029\n221263\n0\nUNKNOWN\\UNIDENTIFIED\n10.186505\n106.229604\n&lt;class 'str'&gt;\n0\n\n\n639029\nSTRIKE\nJG402\nSOUTH VIETNAM\n2877103\n19701114\n219532\n24\nAREA\\DEPOT\n16.331801\n107.012509\n&lt;class 'str'&gt;\n0\n\n\n639030\nSTRIKE\nJ8012\nLAOS\n2877181\n19700721\n219801\n48\nTRUCK PARK\\STOP\n16.286964\n106.922250\n&lt;class 'str'&gt;\n0\n\n\n\n\n\n\n\nAnd we see that, to our relief, the issue isn’t so bad: if engineer’s induction does hold for this situation, then (hopefully) we can split those 14,150 rows without any dashes by just taking the MSNDATE values in these rows and directly extracting the first 4 characters as the year, the next 2 characters as the month and the final 2 characters as the day. As an additional sanity check here, since we are using engineer’s induction, we can check the full range of possible lengths of the MSNDATE values:\n\nstrike_df['MSNDATE_len'] = strike_df['MSNDATE'].str.len()\nstrike_df['MSNDATE_len'].value_counts()\n\nMSNDATE_len\n10    993524\n8      14150\nName: count, dtype: int64\n\n\nAnd we see another encouraging sign: that there is not (for example) some third length that the values of this column can have, that we’d need to worry about on top of the two different formats we’ve found\n(There still could be more than two formats, if the additional formats happened to produce date strings with length 8 or 10, but we’ll worry about that if we experience more issues with splitting MSNDATE under the assumption of two formats)\nRe-Coding the Date Parsing Function: v2.1\nSo let’s make an updated version of our split_date() function, which explicitly checks whether the date it is given is encoded in the 8-character or 10-character format, and does the splitting accordingly:\n\ndef split_date_v2_1(date_val):\n  if len(date_val) == 10:\n    # Do what we did before, using .split()\n    date_elts = date_val.split(\"-\")\n    date_values = {\n      'year': date_elts[0],\n      'month': date_elts[1],\n      'day': date_elts[2]\n    }\n  else:\n    # Assume it is in YYYYMMDD format, so that we can\n    # just extract substrings to obtain Y, M, and D\n    date_values = {\n        'year': date_val[0:4],\n        'month': date_val[4:6],\n        'day': date_val[6:8]\n    }\n  return pd.Series(date_values)\n\nAnd now we could run this using the same .apply() code as before, to see if it successfully parses the column. In the first version of the writeup I did run it, but it took about ~5 minutes, which is too long for a reasonable quick-learning writeup, imo! (You could just go ahead and create a cell containing the following code, and run it, if you are ok waiting for 5mins:)\ncolumns_to_create = ['year','month','day']\nstrike_df[columns_to_create] = strike_df['MSNDATE'].progress_apply(split_date_v2)\nFor me, though, 5 minutes is too slow, so here is a screenshot of the output after this call to progress_apply() has finished:\n\n\n\n\n\n\n\nThe swifter Library\n\n\n\nAnother Quick Note: swifter\nLike last time, this may take a while to complete: cases like these are where I highly recommend using tqdm, at a minimum!\nThere is also the third-party library swifter, which would be nice for the kinds of embarrasingly parallel tasks we’re applying to the column here, except that (as far as I can tell) swifter is only able to provide speedups for numeric operations :/ I thought I’d mention it here, however, since if you are doing some numeric operation that’s taking this long, you might try seeing if swifter will speed up your code!\n\n\nAnd as an example of the vast differences that can arise between efficient and inefficient versions of processing code, even in EDA: since I really only need to extract the year (rather than the three elements of each date) for the remaining analysis, for speed I used the following lambda function instead, which just scoops out the first 4 characters of the MSNDATE value (which works because, in either format, the first 4 characters of the string contain the year), and it took approximately 1 second to finish:\n\n# Depending on your Jupyter environment, you may need to install/update the\n# `ipywidgets` library for `tqdm` to work correctly\n#!pip install ipywidgets\n\n\n# progress_apply() version (with progress bar)\n#strike_df['year'] = strike_df['MSNDATE'].progress_apply(lambda x: x[0:4])\n# apply() version\nstrike_df['year'] = strike_df['MSNDATE'].apply(lambda x: x[0:4])\n\nFinally, now that we’ve extracted just the year of the mission into its own column, let’s convert that column from string format into int format. There are a lot of ways to do this type of conversion, but for me the pd.to_numeric() function is most useful for several reasons, so I’ll use that:\n\nstrike_df['year'] = pd.to_numeric(strike_df['year'])\n\nAnd as we can see by checking the .dtypes attribute (as with shape, remember that dtypes is not a function!), year now has the int64 (64-bit integer) format, as we wanted:\n\nstrike_df.dtypes\n\nMFUNC_DESC              object\nMISSIONID               object\nTGTCOUNTRY              object\nTHOR_DATA_VIET_ID        int64\nMSNDATE                 object\nSOURCEID                 int64\nNUMWEAPONSDELIVERED      int64\nTGTTYPE                 object\nTGTLATDD_DDD_WGS84     float64\nTGTLONDDD_DDD_WGS84    float64\nMSNDATE_type            object\nMSNDATE_dashcount        int64\nMSNDATE_len              int64\nyear                     int64\ndtype: object"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#collapsing-messy-categorical-variables-into-clean-ones",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#collapsing-messy-categorical-variables-into-clean-ones",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(5) Collapsing Messy Categorical Variables Into Clean Ones",
    "text": "(5) Collapsing Messy Categorical Variables Into Clean Ones\nThe one variable we should still look at is TGTTYPE: although we already saw how messy it was in the sense of having lots of missing values, it turns out that it is also messy in the sense of having lots and lots of potential values, some of which are very similar but not identical, so that (for example) trying to include this variable as a “factor variable” in a cross-tabulation or a two-way plot (see below) would result in a very messy plot with too many bars/lines/colors/etc.\nThis becomes clear if we run the following code, showing that there are 235 possible values that this categorical variable can take on:\n\nstrike_df['TGTTYPE'].value_counts()\n\nTGTTYPE\nAREA\\DEPOT         118271\nMOTOR VEHICLE      108533\nTRUCK PARK\\STOP     77840\nROAD                60494\nANTI-AIRCRAFT       52870\n                    ...  \nAMMO BLDG               1\nCP                      1\nMOUNTAIN PASS           1\nSAM OXIDIZER            1\nMIG19                   1\nName: count, Length: 235, dtype: int64\n\n\nSince we’re focusing on EDA and visualization, though, let’s treat this value_counts() table as a “mini-dataset” and generate an EDA-style visualization of how many instances of each possible category actually appear in the broader dataset. We’ll call this mini-dataset target_dist, since it represents just the counts (and therefore, as a whole, the distribution) of the various possible target-type categories. Note, however, that because value_counts() returns a pd.Series object, not a pd.DataFrame object, we’ll need to explicitly convert its return value into a pd.DataFrame (plus turn the index column into a “regular” column and rename it from index into something more informative) in order to use it with Seaborn.\n\ntarget_dist_df = pd.DataFrame(strike_df['TGTTYPE'].value_counts()).reset_index()\ntarget_dist_df.columns = ['TGTTYPE_val','count']\ntarget_dist_df\n\n\n\n\n\n\n\n\nTGTTYPE_val\ncount\n\n\n\n\n0\nAREA\\DEPOT\n118271\n\n\n1\nMOTOR VEHICLE\n108533\n\n\n2\nTRUCK PARK\\STOP\n77840\n\n\n3\nROAD\n60494\n\n\n4\nANTI-AIRCRAFT\n52870\n\n\n...\n...\n...\n\n\n230\nAMMO BLDG\n1\n\n\n231\nCP\n1\n\n\n232\nMOUNTAIN PASS\n1\n\n\n233\nSAM OXIDIZER\n1\n\n\n234\nMIG19\n1\n\n\n\n\n235 rows × 2 columns\n\n\n\n\ntarget_dist_df.iloc[:20]\n\n\n\n\n\n\n\n\nTGTTYPE_val\ncount\n\n\n\n\n0\nAREA\\DEPOT\n118271\n\n\n1\nMOTOR VEHICLE\n108533\n\n\n2\nTRUCK PARK\\STOP\n77840\n\n\n3\nROAD\n60494\n\n\n4\nANTI-AIRCRAFT\n52870\n\n\n5\nTROOPS\n47532\n\n\n6\nBRIDGE\n28272\n\n\n7\nUNKNOWN\\UNIDENTIFIED\n27328\n\n\n8\nBUILDINGS\n24453\n\n\n9\nSEGMENT\\TRANS ROUTE\n24181\n\n\n10\nFORD\n13893\n\n\n11\nWATER VEHICLES\n10845\n\n\n12\nTRUCKS\n10224\n\n\n13\nINTERDICTION POINT\n9414\n\n\n14\nFERRY\n9121\n\n\n15\nBIVOUAC\n8505\n\n\n16\nROADS\\HIGHWAYS\n7762\n\n\n17\nSTORAGE AREA\n7762\n\n\n18\nCAVE\n6350\n\n\n19\nPERSONNEL\\ANY\n6314\n\n\n\n\n\n\n\nNow we can think about how to visualize this: since target_dist represents a distribution (in particular, a distribution of counts), essentially we have already done the set of computations that the sns.displot() function demonstrated in the very beginning of the W06 Lab Demo performs under the hood. Meaning: if we hadn’t already computed counts for each value of TGTTYPE, we could use sns.displot() with the original strike_df dataset to visualize these counts. Since we’ve already computed counts, however, we can more straightforwardly just plot each count as a bar in a sns.barplot() visualization (which will look identical to what sns.displot() would generate given strike_df).\n(I mentioned the fact that target_dist is a distribution of counts since, although displot() can be used for both probability distributions and distributions of counts, the y-axis for plots of counts will be even easier to interpret than the y-axis for plots of probability mass values, since in the latter case the y-axis values will be “squished down” into decimals ranging from 0 to 1)\n\n# Since Seaborn is built on top of `matplotlib`, we usually import these two libraries\n# in the following order, and use the aliases `plt` and `sns`, respectively\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.barplot(\n    data=target_dist_df,\n    x=\"TGTTYPE_val\",\n    y='count'\n).set(xticks=[], xticklabels=[])\nplt.show()\n\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nAfter the call to sns.barplot() in the previous cell, notice that I’m also calling .set() with the options xticks=[] and xticklabels=[]. This call with these options simply removes all ticks and tick labels on the x-axis of the plot, since otherwise it would have displayed a chaotic overlapping mass of 235 ticks and 235 labels. Without these ticks/labels, we are able to focus just on the distribution of the y-axis values, and especially the fact that they start to decrease rapidly after a certain point.\nThis rapid decrease tells us, for example, that we can probably either\n\n\naggregate the rarely-appearing values into broader categories, to see if they are (for example) just slightly-modified versions of the more-frequently-appearing values [the safer, more statistically-principled approach], or\n\n\nignore these values when we’re examining cross-tabulations and/or generating visualizations where data is grouped by these values.\n\n\nSince we’re focusing on EDA, I’ll be employing approach (b), but please keep in mind that you might be throwing away important information by ignoring these less-frequently-occurring values (many examples come to mind, but to avoid making the writeup any longer I will just say, feel free to ask about these cases!)\nTherefore, I’m going to (arbitrarily!) keep the top 10 most-frequently occurring values, and then collapse any values beyond this top 10 down into an 11th catch-all category called “other”:\n\n# Extract just the 10 most-frequently occurring values\nrank_cutoff = 10\nmost_frequent_values = target_dist_df['TGTTYPE_val'].iloc[:rank_cutoff].values\n# Print them out (for future reference)\nprint(most_frequent_values)\n# Then take the **set difference** between the **full set of values** and the\n# **values that we're keeping**, and re-map these remaining values onto the single value\n# \"other\"\nall_values = target_dist_df['TGTTYPE_val'].values\nnon_frequent_values = [v for v in all_values if v not in most_frequent_values]\n# Sanity check\nnum_non_frequent = len(non_frequent_values)\nprint(f\"{num_non_frequent} non-frequent values\")\n# And apply the mapping to the original strike_df dataset\nrename_map = {nfv: \"other\" for nfv in non_frequent_values}\nstrike_df['TGTTYPE'] = strike_df['TGTTYPE'].apply(lambda x: rename_map[x] if x in rename_map else x)\n# Print out the new value_counts, to check that the mapping worked\nstrike_df['TGTTYPE'].value_counts()\n\n['AREA\\\\DEPOT' 'MOTOR VEHICLE' 'TRUCK PARK\\\\STOP' 'ROAD' 'ANTI-AIRCRAFT'\n 'TROOPS' 'BRIDGE' 'UNKNOWN\\\\UNIDENTIFIED' 'BUILDINGS'\n 'SEGMENT\\\\TRANS ROUTE']\n225 non-frequent values\n\n\nTGTTYPE\nother                   161206\nAREA\\DEPOT              118271\nMOTOR VEHICLE           108533\nTRUCK PARK\\STOP          77840\nROAD                     60494\nANTI-AIRCRAFT            52870\nTROOPS                   47532\nBRIDGE                   28272\nUNKNOWN\\UNIDENTIFIED     27328\nBUILDINGS                24453\nSEGMENT\\TRANS ROUTE      24181\nName: count, dtype: int64"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#tabulation-and-cross-tabulation",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#tabulation-and-cross-tabulation",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(6) Tabulation and Cross-Tabulation",
    "text": "(6) Tabulation and Cross-Tabulation\nNow that we’re finally done with those fairly messy/tedious cleaning steps, let’s make some tables!\n\n(6.1) Summary Statistics Table: .describe()\nFirst, as in the other lab demo you have, let’s use .describe() to create a table showing us some basic summary statistics about our now-cleaned variables. Your first instinct might be to just call strike_df.describe(), as in the following code cell, but that’s another instinct you should probably push back on:\n\nstrike_df.describe()\n\n\n\n\n\n\n\n\nTHOR_DATA_VIET_ID\nSOURCEID\nNUMWEAPONSDELIVERED\nTGTLATDD_DDD_WGS84\nTGTLONDDD_DDD_WGS84\nMSNDATE_dashcount\nMSNDATE_len\nyear\n\n\n\n\ncount\n1.007674e+06\n1.007674e+06\n1.007674e+06\n969931.000000\n969931.000000\n1.007674e+06\n1.007674e+06\n1.007674e+06\n\n\nmean\n2.264882e+06\n1.712625e+06\n1.202383e+01\n17.221362\n105.875013\n1.971916e+00\n9.971916e+00\n1.969216e+03\n\n\nstd\n1.278396e+06\n1.321200e+06\n5.958607e+01\n1.801880\n1.249900\n2.353301e-01\n2.353301e-01\n1.738883e+00\n\n\nmin\n4.000000e+00\n3.000000e+00\n0.000000e+00\n0.247721\n43.416666\n0.000000e+00\n8.000000e+00\n1.965000e+03\n\n\n25%\n1.164722e+06\n5.796808e+05\n0.000000e+00\n16.430555\n105.712000\n2.000000e+00\n1.000000e+01\n1.968000e+03\n\n\n50%\n2.264671e+06\n1.454324e+06\n4.000000e+00\n17.125000\n106.155833\n2.000000e+00\n1.000000e+01\n1.969000e+03\n\n\n75%\n3.218455e+06\n2.593384e+06\n1.200000e+01\n18.000000\n106.638000\n2.000000e+00\n1.000000e+01\n1.970000e+03\n\n\nmax\n4.670411e+06\n4.909001e+06\n9.822000e+03\n135.717675\n167.500000\n2.000000e+00\n1.000000e+01\n1.975000e+03\n\n\n\n\n\n\n\nWe see that this output looks kind of messy, if we don’t filter out non-numeric-variable columns like the id variables or the _dashcount and _len variables we created as helper columns above. If we do take care to filter out those columns first, .describe() provides us with more focused and more useful information.\nFirst we drop the helper columns, since we don’t need these anymore now that MSNDATE has been cleaned to our satisfaction. We use the inplace = True argument to Pandas’ drop() function to indicate that we want to permanently drop these two columns:\n\ncols_to_drop = [\n    'MSNDATE_len',\n    'MSNDATE_dashcount',\n    'MSNDATE_type'\n]\nstrike_df.drop(columns = cols_to_drop, inplace = True)\n\nAnd now we specify just the subset of numeric columns we’d like summaries of, and call .describe() only on that subset:\n\n# I break the column names onto separate lines here to make it easy for\n# myself to go back and include/exclude columns from the call to describe():\n# I just comment/uncomment the line corresponding to the column I want to\n# hide/show, respectively\nnumeric_cols = [\n    'NUMWEAPONSDELIVERED',\n    'TGTLATDD_DDD_WGS84',\n    'TGTLONDDD_DDD_WGS84',\n    'year'\n]\n# pd.option_context() has kind of a strange syntax, but it just allows us to\n# specify how to format each number in the result of the call to describe()\n# without having to worry about permanently changing our Pandas display\n# settings\nwith pd.option_context('display.float_format', '{:.2f}'.format):\n  display(strike_df[numeric_cols].describe())\n\n\n\n\n\n\n\n\nNUMWEAPONSDELIVERED\nTGTLATDD_DDD_WGS84\nTGTLONDDD_DDD_WGS84\nyear\n\n\n\n\ncount\n1007674.00\n969931.00\n969931.00\n1007674.00\n\n\nmean\n12.02\n17.22\n105.88\n1969.22\n\n\nstd\n59.59\n1.80\n1.25\n1.74\n\n\nmin\n0.00\n0.25\n43.42\n1965.00\n\n\n25%\n0.00\n16.43\n105.71\n1968.00\n\n\n50%\n4.00\n17.12\n106.16\n1969.00\n\n\n75%\n12.00\n18.00\n106.64\n1970.00\n\n\nmax\n9822.00\n135.72\n167.50\n1975.00\n\n\n\n\n\n\n\nAnd already, just from this description, we can make some simple but important inferences: for example that 75% of the bombings were carried out before or during the year 1970, despite the fact that the bombing campaign as a whole did not end until 1975 (which we can see from the max value of year)\n\n\n(6.2) Cross-Tabulation of the Relationship Between Two Variables\nIf you googled “cross-tabulation in pandas” like the lab asks you to (which I hope you did in the days between the lab handout and this writeup!), you’ll find the Pandas function crosstab().\ncrosstab() works similarly to describe() except that, rather than computing summary statistics across the entire dataset, it computes a set of conditional summary statistics, so that we can use it e.g. to examine the distributions of a numeric variable (say, year) for different values of a categorical variable (say, TGTCOUNTRY). Here we use it with its default parameter settings to generate a table showing us the number of strikes within each cross-tabulated “bin”—that is, each (year,country) pair where year is some value in the year column and country is some value in the TGTCOUNTRY column:\n\npd.crosstab(strike_df['year'], strike_df['TGTCOUNTRY'])\n\n\n\n\n\n\n\nTGTCOUNTRY\nCAMBODIA\nLAOS\nNORTH VIETNAM\nSOUTH VIETNAM\nTHAILAND\nUNKNOWN\nWESTPAC WATERS\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n1965\n0\n1197\n2596\n9238\n0\n0\n0\n\n\n1966\n0\n16825\n24261\n4918\n11\n0\n0\n\n\n1967\n18\n29738\n69420\n1004\n6\n0\n0\n\n\n1968\n0\n105046\n108110\n3763\n81\n19\n6\n\n\n1969\n23\n167010\n729\n1353\n41\n4\n0\n\n\n1970\n6099\n209273\n1801\n2684\n7\n0\n0\n\n\n1971\n1130\n107675\n3342\n7932\n0\n0\n0\n\n\n1972\n1243\n48531\n34281\n23303\n0\n0\n0\n\n\n1973\n4758\n4866\n1437\n54\n0\n0\n0\n\n\n1975\n0\n0\n0\n4\n0\n0\n0\n\n\n\n\n\n\n\nAnd again, especially with whatever domain knowledge we possess, we can detect patterns in this table that help us understand the underlying phenomena:\nFor example, we see that bombing runs in Cambodia were sporadic before 1970, but quickly jumped to a high of 6,099 in 1970, mirroring a turbulent event in Cambodian history that year\n([very long story short,] Cambodia’s somewhat-popular leader Prince Sihanouk was overthrown in a US-sponsored coup in 1970 and replaced by the less-popular army general Lon Nol, leading the now-exiled-in-China Sihanouk to devote his energy towards supporting/accelerating communist [mostly, though not entirely, Khmer Rouge] resistance to the Lon Nol military dictatorship, which in turn led the US to begin bombing Khmer Rouge/other communist strongholds in support of the regime)\nMore relevant to our discussion of Laos in the beginning, though, we can also start to see how Laos became the most-bombed territory in history: unlike the other countries, wherein bombings had high “spikes” in some years but low-level “lulls” in other years, Laos was continually bombed over 100,000 times per year every year between 1968 and 1971 (inclusive)."
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#seaborn-time",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#seaborn-time",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "(7) Seaborn Time!",
    "text": "(7) Seaborn Time!\nAs mentioned above (and just in case you’re running these cells without running the cells above first), we import matplotlib and seaborn using aliases as follows:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n(7.1) Aggregation\nNow, since this data has a much larger number of rows than the cars-by-country dataset used in the W06 lab demonstration, using sns.catplot() will take a while to produce the same types of visualizations. So for our use case, we’re going to start off by working with aggregated data, to make the number of datapoints manageable and quickly visualizable using Seaborn.\nAs mentioned earlier (and as we saw in the cross-tabulation), we can start to get a good feel for “what’s going on” in the data by focusing on year-by-year statistics first, before “zooming in” on the particular months and/or days of the bombings. So, let’s perform that aggregation now: we’ll use Pandas’ agg() function to count the number of bombings per year in each country, while averaging the latitude and longitude and finding the maximum of the number of weapons delivered.\nWhile averaging and maximizing might be a bit contrived in this example, in general this is where a lot of your thought should go when you are first starting on EDA! You should ask yourself which variables you want to group by (country, in our case) and which variables you want to aggregate, and for the latter set of variables which specific aggregations you want to apply that will best allow you to answer the types of EDA questions you are brainstorming.\nFor example, while at first I thought about computing the mode of the longitude and latitude per year, to try and find the most-bombed location in each country during each year, in reality the latitude and longitude values are too fine-grained, in that a single village being bombed might 100 times might show up as 100 different sets of coordinates, unless two planes happened to bomb the exact same point on earth down to the maximum possible level of precision in the numeric recording mechanism used by the US military, not a very likely prospect\n(If that was confusing, it’s the same intuition around why, if we had 100 students each draw a unit circle and measure that circle’s circumference, all of these values would be approximately \\(2\\pi\\), and yet it would be unlikely to find that any two students’ values were equal down to 50 or 100 decimal places)\nSo, let’s run the agg() function to compute these aggregate values! There are a ton of different ways to use this function, syntax-wise, but I find the following syntax to be the most flexible for my most common use-cases (if you are curious about other ways to use it, or you can’t get this to work as a “template” for the aggregations you are trying to perform, let me know/schedule an office hour!)\n(Also, we are now going to drop the extraneous id functions that we kept along until now: I kept them in there because I want you to get comfortable with cases where you keep variables around just in case you end up needing them later on in your analysis, but since agg() requires an aggregation rule for each column in the dataset, I will just drop whatever values we’re not including in the aggregation at this point.)\n\n# Printing the column names here for reference\nstrike_df.columns\n\nIndex(['MFUNC_DESC', 'MISSIONID', 'TGTCOUNTRY', 'THOR_DATA_VIET_ID', 'MSNDATE',\n       'SOURCEID', 'NUMWEAPONSDELIVERED', 'TGTTYPE', 'TGTLATDD_DDD_WGS84',\n       'TGTLONDDD_DDD_WGS84', 'year'],\n      dtype='object')\n\n\nHere, so that I have some actual column in the dataset that I can aggregate to get the count per group, I make a column called num_bombings filled with the value 1 for now, so that when we aggregate this column by summing it, the column will thereafter contain the count of the number of observations (bombings) per group\n\nstrike_df['num_bombings'] = 1\n\n\n# And specifying columns to keep, that will be used as either *group* or\n# *aggregated* variables\ncols_to_keep = [\n    'TGTCOUNTRY',\n    'year',\n    'TGTTYPE',\n    'NUMWEAPONSDELIVERED',\n    'TGTLATDD_DDD_WGS84',\n    'TGTLONDDD_DDD_WGS84',\n    'num_bombings'\n]\n\n\n# And run!\nagg_df = strike_df[cols_to_keep].groupby(['TGTCOUNTRY','year','TGTTYPE']).agg(\n    mean_weapons = ('NUMWEAPONSDELIVERED', 'mean'),\n    num_bombings = ('num_bombings', 'sum'),\n    mean_lat = ('TGTLATDD_DDD_WGS84', 'mean'),\n    mean_lon = ('TGTLONDDD_DDD_WGS84', 'mean'),\n)\n\nAs we can see from the output of head(), it looks like it has aggregated the data as we wanted, it’s just a bit messy since the aggregated data is grouped in a three-level hierarchy (country-&gt;year-&gt;target type):\n\nagg_df.head()\n\n\n\n\n\n\n\n\n\n\nmean_weapons\nnum_bombings\nmean_lat\nmean_lon\n\n\nTGTCOUNTRY\nyear\nTGTTYPE\n\n\n\n\n\n\n\n\nCAMBODIA\n1967\nAREA\\DEPOT\n3.333333\n12\n14.693574\n106.988342\n\n\nROAD\n4.000000\n2\n14.675000\n106.783333\n\n\nTRUCK PARK\\STOP\n12.750000\n4\n14.573833\n106.990167\n\n\n1969\nANTI-AIRCRAFT\n5.000000\n11\n14.298727\n107.505909\n\n\nUNKNOWN\\UNIDENTIFIED\n2.333333\n12\n14.593815\n107.539481\n\n\n\n\n\n\n\nIf this type of hierarchical index is what you want for your use-case, then you’re done! In this case, though, we’d like to analyze various relationships among these variables which are unrelated to the specific country-&gt;year-&gt;target type hierarchy, so we’ll just use reset_index() (remembering to also include the inplace = True option!) to convert the hierarchical index columns into “regular” data columns:\n\nagg_df.reset_index(inplace=True)\nagg_df\n\n\n\n\n\n\n\n\nTGTCOUNTRY\nyear\nTGTTYPE\nmean_weapons\nnum_bombings\nmean_lat\nmean_lon\n\n\n\n\n0\nCAMBODIA\n1967\nAREA\\DEPOT\n3.333333\n12\n14.693574\n106.988342\n\n\n1\nCAMBODIA\n1967\nROAD\n4.000000\n2\n14.675000\n106.783333\n\n\n2\nCAMBODIA\n1967\nTRUCK PARK\\STOP\n12.750000\n4\n14.573833\n106.990167\n\n\n3\nCAMBODIA\n1969\nANTI-AIRCRAFT\n5.000000\n11\n14.298727\n107.505909\n\n\n4\nCAMBODIA\n1969\nUNKNOWN\\UNIDENTIFIED\n2.333333\n12\n14.593815\n107.539481\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n229\nUNKNOWN\n1968\nROAD\n0.800000\n5\n18.925000\n105.595900\n\n\n230\nUNKNOWN\n1968\nSEGMENT\\TRANS ROUTE\n6.833333\n6\n18.401953\n105.744138\n\n\n231\nUNKNOWN\n1968\nother\n4.250000\n8\n16.690458\n106.489083\n\n\n232\nUNKNOWN\n1969\nUNKNOWN\\UNIDENTIFIED\n0.000000\n4\nNaN\nNaN\n\n\n233\nWESTPAC WATERS\n1968\nother\n0.000000\n6\n17.566667\n107.044389\n\n\n\n\n234 rows × 7 columns\n\n\n\nAs a final consideration before we create our Seaborn catplots: if we had spent more time cleaning the TGTTYPE variable—for example, by collapsing the individual values down into a smaller set of categories like “people”, “vehicles”, “buildings”—that would provide us with another easy-to-analyze categorical variable. For the sake of getting through the visualizations without more code, though, I won’t be generating plots involving this variable, so I’m now going to aggregate our dataset one more time, to sum/average over the different target-type values for each (country, year) pair. But, if you decide you want to use this dataset for a project, you can stop here and clean TGTTYPE to carry out an EDA investigation in a different direction!\n\ncountry_year_df = strike_df.groupby(['TGTCOUNTRY','year']).agg(\n    mean_weapons = ('NUMWEAPONSDELIVERED', 'mean'),\n    num_bombings = ('num_bombings', 'sum'),\n    mean_lat = ('TGTLATDD_DDD_WGS84', 'mean'),\n    mean_lon = ('TGTLONDDD_DDD_WGS84', 'mean'),\n).reset_index()\n\n\n\n(7.2) Categorical Plots with catplot()\nLet’s generate our first catplot()! Here we’ll create a non-stacked bar plot, where the x-axis will represent years and the y-axis will represent number of bombings, while the bars themselves will be colored by country (the hue option in catplot:\n\nsns.catplot(\n    data = country_year_df,\n    kind = \"bar\",\n    x = 'year',\n    y = 'num_bombings',\n    hue = 'TGTCOUNTRY',\n    orient = 'v'\n)\nplt.show()\n\n\n\n\nWe can already infer some things from this plot, but since there were so few bombings in Thailand and in the “Unknown” and “WESTPAC WATERS” categories, let’s exclude these to make the plot more readable (it may be tempting to drop them, and it would typically be ok in many cases, but data-science-wise it’s a bit sketchy in the sense that it would skew our results if we wanted to compute e.g. the proportion of all bombings in each country at some point in our analysis later on)\n\ncountries_to_keep = [\n    'CAMBODIA',\n    'LAOS',\n    'NORTH VIETNAM',\n    'SOUTH VIETNAM'\n]\ncountry_year_sub_df = country_year_df.loc[\n    country_year_df['TGTCOUNTRY'].isin(countries_to_keep),\n].copy()\nsns.catplot(\n    data = country_year_sub_df,\n    kind = \"bar\",\n    x = 'year',\n    y = 'num_bombings',\n    hue = 'TGTCOUNTRY',\n    orient = 'v'\n)\nplt.show()\n\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nSo for me, at least, the plot in this form really already drives home the point from the beginning of the writeup: that “The Vietnam War” really could be called “The Vietnam War Plus The Continuous Onslaught Of Modern Weaponry Rained Down Upon Laos, A Similarly Poor And Agrarian Country With A Fraction Of The Population Of Vietnam”.\n\n\n(7.3) Plotting Non-Aggregated Data\nNow that we’ve seen how to make a basic catplot() using aggregated data, we can move back to the non-aggregated (individual-bombing level) data to see how catplot() can automatically aggregate data for us, in whatever way enables it to make the plot we ask for. This is a super powerful function, in that sense, it just runs much more slowly (so I wanted to show some quick-to-generate examples first!)\nLooking again at the columns in strike_df here, as a reminder:\n\nstrike_df.columns\n\nIndex(['MFUNC_DESC', 'MISSIONID', 'TGTCOUNTRY', 'THOR_DATA_VIET_ID', 'MSNDATE',\n       'SOURCEID', 'NUMWEAPONSDELIVERED', 'TGTTYPE', 'TGTLATDD_DDD_WGS84',\n       'TGTLONDDD_DDD_WGS84', 'year', 'num_bombings'],\n      dtype='object')\n\n\nWe see that so far we’ve neglected the NUMWEAPONSDELIVERED variable. Let’s use a violin plot this time to see what the data within this variable looks like:\n\nstrike_sub_df = strike_df.loc[strike_df['TGTCOUNTRY'].isin(countries_to_keep),].copy()\nsns.catplot(\n    data=strike_sub_df,\n    kind='violin',\n    x = 'NUMWEAPONSDELIVERED',\n    y='TGTCOUNTRY',\n    hue='TGTCOUNTRY'\n)\n\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nAlthough we now see that violin plots are probably not the best way to visualize this variable. In general violin plots are intended to plot the “curvature” of a distribution over some range of values, but in this case the distribution of our data is almost entirely concentrated at 0, with a few very extreme outlier values. We can still make some inferences, however: like how Laos was not only the most bombed country, but also the country which experienced the largest single bombing of the entire campaign—the extreme outlier corresponding to a bombing run where over 8000 were delivered to the target.\nFinally, while we unfortunately don’t have two full-on continuous numeric variables to plot in the form of (e.g.) a scatterplot, we can use the year of each mission as a type of pseudo-continous numeric variable (the full date would be even better, if we wanted to spend more time cleaning the date variable so we could visualize the day-by-day statistics), plotted against the NUMWEAPONSDELIVERED value for that mission (similar to the bar graph from above, but this time generated automatically by Seaborn from the raw, non-aggregated data):\n\n# Here, (1) Again we use strike_sub_df to filter out the countries outside of our 4 main countries\n# and thus make the plot slightly more readable, and (2) I'm using Pandas' sample() function to\n# make the number of points manageable for Seaborn to plot (since otherwise it takes over a minute,\n# which is mostly spent drawing nearly 1 million points that are almost entirely down near 0)\nstrike_sub_sample_df = strike_sub_df.sample(10000)\nsns.lmplot(\n    data=strike_sub_sample_df, \n    x=\"year\",\n    y=\"NUMWEAPONSDELIVERED\",\n    col=\"TGTCOUNTRY\",\n    hue=\"TGTCOUNTRY\")\nplt.show()\n\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/Users/jpj/.pyenv/versions/3.11.5/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\n\n\n\nAnd from these four plots we see what we could already probably see in the tabulations from earlier: that Cambodia is the only country which saw a clear year-to-year increase in bombings experienced, from the 1970 coup onwards. As another potential “branching path” in our EDA journey, we could feasibly dive far more deeply into this phenomenon in the Cambodia data specifically: visualizing these bombing-rate increases at a more fine-grained daily level, for example, and seeing whether/how they relate to political and military developments in Sihanouk’s/China’s backing of the Khmer Rouge.\nAnd, last but certainly not least, a correlogram of our aggregated variables, generated using the same code as used in the W06 lab demo (we first display the full country_year_sub_df, so we can glance back and forth between the correlations encoded as colors in the correlogram and the values themselves)\n\ncountry_year_sub_df\n\n\n\n\n\n\n\n\nTGTCOUNTRY\nyear\nmean_weapons\nnum_bombings\nmean_lat\nmean_lon\n\n\n\n\n0\nCAMBODIA\n1967\n5.500000\n18\n14.664901\n106.965969\n\n\n1\nCAMBODIA\n1969\n3.608696\n23\n14.452686\n107.523425\n\n\n2\nCAMBODIA\n1970\n19.907526\n6099\n12.656550\n106.588893\n\n\n3\nCAMBODIA\n1971\n84.664602\n1130\n13.221369\n106.461660\n\n\n4\nCAMBODIA\n1972\n80.004827\n1243\n12.587244\n106.407726\n\n\n5\nCAMBODIA\n1973\n106.332072\n4758\n11.898209\n105.176818\n\n\n6\nLAOS\n1965\n10.939850\n1197\n17.552708\n105.521857\n\n\n7\nLAOS\n1966\n11.938068\n16825\n16.618527\n106.264797\n\n\n8\nLAOS\n1967\n12.266561\n29738\n16.364346\n106.365654\n\n\n9\nLAOS\n1968\n9.391200\n105046\n17.052071\n105.898343\n\n\n10\nLAOS\n1969\n10.355326\n167010\n17.470780\n105.498338\n\n\n11\nLAOS\n1970\n11.221041\n209273\n17.145868\n105.686660\n\n\n12\nLAOS\n1971\n13.407160\n107675\n17.140651\n105.549005\n\n\n13\nLAOS\n1972\n18.685891\n48531\n17.093275\n105.228933\n\n\n14\nLAOS\n1973\n46.943280\n4866\n17.607340\n104.471988\n\n\n15\nNORTH VIETNAM\n1965\n12.527735\n2596\n20.248847\n105.176626\n\n\n16\nNORTH VIETNAM\n1966\n9.555418\n24261\n18.333420\n106.140361\n\n\n17\nNORTH VIETNAM\n1967\n9.060746\n69420\n18.516883\n106.320436\n\n\n18\nNORTH VIETNAM\n1968\n7.279752\n108110\n17.895921\n106.223039\n\n\n19\nNORTH VIETNAM\n1969\n2.847737\n729\n17.957101\n106.128279\n\n\n20\nNORTH VIETNAM\n1970\n7.349806\n1801\n17.063296\n105.956814\n\n\n21\nNORTH VIETNAM\n1971\n12.736385\n3342\n17.170001\n106.070248\n\n\n22\nNORTH VIETNAM\n1972\n14.750387\n34281\n18.364637\n106.144553\n\n\n23\nNORTH VIETNAM\n1973\n28.956159\n1437\n18.439947\n105.850921\n\n\n24\nSOUTH VIETNAM\n1965\n13.495670\n9238\n13.340375\n107.601053\n\n\n25\nSOUTH VIETNAM\n1966\n13.735665\n4918\n13.053547\n107.553177\n\n\n26\nSOUTH VIETNAM\n1967\n9.340637\n1004\n15.226087\n107.357685\n\n\n27\nSOUTH VIETNAM\n1968\n11.776774\n3763\n15.712929\n107.287747\n\n\n28\nSOUTH VIETNAM\n1969\n17.725795\n1353\n14.273498\n107.630294\n\n\n29\nSOUTH VIETNAM\n1970\n8.029434\n2684\n14.339186\n107.127581\n\n\n30\nSOUTH VIETNAM\n1971\n7.367625\n7932\n16.382974\n106.494151\n\n\n31\nSOUTH VIETNAM\n1972\n7.496674\n23303\n14.872121\n106.908634\n\n\n32\nSOUTH VIETNAM\n1973\n95.425926\n54\n13.715959\n107.502381\n\n\n33\nSOUTH VIETNAM\n1975\n1.000000\n4\n11.007222\n106.834444\n\n\n\n\n\n\n\n\nsns.set_theme(style=\"white\")\ncorr_vars = ['mean_weapons', 'num_bombings', 'mean_lat', 'mean_lon']\ncorr = country_year_sub_df[corr_vars].corr()  #Compute the correlation matrix\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool)) \nf, ax = plt.subplots(figsize=(11, 9)) #initialize figure\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True) #custom diverging colormap\n\n    # # Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()"
  },
  {
    "objectID": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#footnotes",
    "href": "writeups/eda-seaborn/THOR_EDA_with_Seaborn.html#footnotes",
    "title": "Exploratory Data Analysis of the THOR Dataset in Seaborn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy old link to the dataset, from data.mil, does not seem to work anymore, and I can’t locate this dataset on the Department of Defense’s official website anymore, so if someone knows what happened there let me know :D↩︎"
  },
  {
    "objectID": "writeups/domains-ssh/index.html",
    "href": "writeups/domains-ssh/index.html",
    "title": "Troubleshooting SSH/SCP/rsync on Georgetown Domains",
    "section": "",
    "text": "This tutorial assumes that you have already created a Georgetown Domains account. If you haven’t already done so, or you’re having trouble setting one up, please feel free to email me."
  },
  {
    "objectID": "writeups/domains-ssh/index.html#step-1-creating-a-private-key",
    "href": "writeups/domains-ssh/index.html#step-1-creating-a-private-key",
    "title": "Troubleshooting SSH/SCP/rsync on Georgetown Domains",
    "section": "Step 1: Creating a Private Key",
    "text": "Step 1: Creating a Private Key\nTo remotely access your Georgetown Domains server from your local computer (outside of the method we used before, of accessing the terminal through a web interface), you will first need to create an SSH keypair, which you can think of in two steps:\n\nGeorgetown Domains generates a “keyhole” on your Domains server, a Public Key, which allows your Domains server to be accessed remotely by someone who has the matching private key. Hence, it also creates a\nPrivate Key: a file (in this case, it will be called id_rsa, with no file extension) that you save onto your computer, and you remember where you saved it, that will allow you to access your Georgetown Domains server remotely. There is a third piece of information here, which is the passphrase that will protect your private key from unauthorized usage (this is required by Georgetown, but is optional in general), but we’ll worry about that in a moment.\n\nLet’s generate this keypair step-by-step:\n\nStep 1.1: Opening the “SSH Access” Tool in Georgetown Domains\nOpen your browser and navigate to the Georgetown Domains dashboard. This is the interface that opens if you navigate to https://georgetown.domains/dashboard\nIf you scroll down to the category of tools with the heading “Security”, you will see an icon and a link for “SSH Access”:\n\nClick this link to open the SSH key generation page, and then once this page opens, click the “Manage SSH Keys” button:\n\n\n\nStep 1.2: Generating the Key\nAfter clicking “Manage SSH Keys” and waiting for the next page to load, don’t worry about any of the other options, just click the “+ Generate a New Key” button near the top:\n\nWithin the key-generation form that appears, you don’t need to enter or modify anything except for the Key Password and Reenter Password fields. The way these two fields are phrased is a bit confusing, since you already have a password to access Georgetown services in general (as in, the password that you use to log into Canvas, for example, that requires Two-Factor Authentication): do not enter that password here. This is a new password, really called a passphrase, and all it does it protect your SSH private key from unauthorized usage: if someone manages to steal your id_rsa file, they still won’t be able to access your Georgetown Domains server without knowing this passphrase.\n\nSo, choose a passphrase that is easy to remember and passes Georgetown’s security requirements (it won’t let you submit the form until the passphrase meets those requirements – you can use the “Password Generator” button to auto-generate one if that’s easier), and submit the form by clicking the “Generate Key” button at the bottom.\nIf all went well, you should see a success screen that says “Key Generation Complete!” at the top:\n\nThis screen is also confusing, because it makes it seem like you need to care about the console output that it shows you underneath “Key Generation Complete!”, but in reality you don’t need to worry about it (I excluded it here just in case).\n\n\nStep 1.3: Authorizing This Key to Acess Your Server\nFrom the “Key Generation Complete!” screen, click the “Go Back” link at the bottom of the page to return to the SSH Key Generation panel. You should see the window you saw before, with the “+ Generate a New Key” button at the top, but now below this button you will also see your newly-created pair of keys: the public key just created will now be listed in the “Public Keys” list, and then underneath this the private key just created will be listed in the “Private Keys” list.\nThe first, basic step we need to do is verify with Georgetown Domains that the public key is a valid “keyhole” for remote computers to use to access your Domains server (you’ll notice that it is not authorized by default, so you will see the status “not authorized” within the “Authorization Status” column of the Public Keys list). To activate it, scroll to the Public Keys list and click the “Manage” link (with a wrench icon to the left of it):\n\nThis should bring up a screen where you can straightforwardly click the “Authorize” button to authorize this public key:\n\nAnd, once you click this, you should see a success message like the following:\n\nClick “Go Back” once again to return to the SSH Key Management page.\n\n\nStep 1.4: Downloading the Private Key\nThat is the last time we’ll deal with the public key, the “keyhole” to your server, now that it has been authorized for use. Next, scroll further down to the Private Keys listing, where you should see your id_rsa private key that we generated in Step 1.2:\n\nIn this case, we do want to download this key (as a file) to our local computer, since the ssh (and scp and rsync) command will need to use this key to acess your server. So, click the “View/Download” button (with a disk icon to the left of it). This will bring you to a scary-looking page, that contains your full private key in a big textbox. You don’t need to worry about this, or about the “Convert to PPK” section at the bottom of the page. Just scroll to the button which allows you to download the key file:\n\nIMPORTANT: Make sure that, once this file has downloaded, you move it to a location on your local computer that is secure but also easy to locate, since you will need to know the absolute path to this file, in order to tell the ssh command where to find it. For example, if I copy the key file to my home directory on a Mac, its path might be:\n/Users/jpj/id_rsa\nIf you’re on Windows, and you move it directly onto your C: drive for example, then the path will instead be\nC:\\id_rsa\nSo, just make sure you know this absolute path to the id_rsa file, remembering that it has no file extension: it’s not id_rsa.txt, or id_rsa.pub (that could be what the public key is called), but just id_rsa without extension."
  },
  {
    "objectID": "writeups/domains-ssh/index.html#step-2-using-the-private-key-to-access-your-georgetown-domains-server",
    "href": "writeups/domains-ssh/index.html#step-2-using-the-private-key-to-access-your-georgetown-domains-server",
    "title": "Troubleshooting SSH/SCP/rsync on Georgetown Domains",
    "section": "Step 2: Using the Private Key to Access Your Georgetown Domains Server",
    "text": "Step 2: Using the Private Key to Access Your Georgetown Domains Server\nNow that you have the id_rsa file downloaded, in a location that you can remember the path to (or write down the path to), open up your terminal if you’re on Mac or Git Bash if you’re on Windows. We’ll start by using the ssh command, with our private key (id_rsa) file, to connect to the server.\nThis order is actually somewhat important: we want to start with ssh, not scp or rsync, since the syntax for the scp and rsync commands build upon the syntax for the ssh command. So, I recommend starting with ssh, and working your way towards scp and/or rsync.\nWithin your terminal, we will now “craft” a correct ssh command (you may actually want to have a text editor open, next to your terminal, where you can write out commands to make sure they’re correct before copying-and-pasting them into the terminal). The first part is the name of the command, ssh. So, right now, you should just have the following written in your terminal (don’t press Enter or anything yet, we’re going to press Enter once we finish writing out the command!)\nssh\nNow, we need to give to the ssh command the following information: (a) that we want to use a private key to connect, and (b) the location of our private key, in the form of a path on your local computer’s drive where that key exists.\n\nStep 2.1: Providing ssh the Absolute Path to the Private Key File (id_rsa)\nTo give the information (a), you add the -i flag right after ssh, separated by a space. This tells ssh that you are about to provide a path to a private key file (again, don’t press Enter yet! I will indicate when the command is finished and ready to execute in the terminal):\nssh -i\nNext, we need to provide information (b), the actual path to the private key file, which you should include in double quotes after the -i character (separated by a space). So, for example, if I had moved my private key to /Users/jpj/id_rsa, then the command at this point would look like:\nssh -i \"/Users/jpj/id_rsa\"\nIf I was on Windows, and I had saved it directly in the root of my C: directory, then the command would instead look like:\nssh -i \"C:\\id_rsa\"\n\n\nStep 2.2: Providing Your Georgetown Domains Username and Domain\nNow, there are two final pieces of information that we need to provide to the ssh command: (a) our username on our Georgetown Domains server, and (b) the domain that the Georgetown Domains system has allocated for us. To find both pieces of information, you can navigate back to the Georgetown Domains Dashboard main page: https://georgetown.domains/dashboard, and look for this info box near the top of the page, on the right side of the page:\n\nThe only information that we need is in the first two rows of this info box: in my case, this shows me that:\n\nMy username is jpjgeorg, and\nMy domain is jpj.georgetown.domains\n\nThese are exactly the two pieces of information we were looking for! Moving back to the terminal, we can now complete the ssh command by entering this information in the form username@domain, separated from the end of the private key file path by a space. Using my information, the command is now\nssh -i \"/path/to/private/key/file\" jpjgeorg@jpj.georgetown.domains\nKeep in mind that this username@domain portion may look strange to you, if your username and your domain are similar/identical. For example, it may have assigned you the username dsanstudent, and the domain dsanstudent.georgetown.domains: in this case, you need to make sure to include both pieces of information: dsanstudent@dsanstudent.georgetown.domains. Long story short, the Georgetown Domains system creates a custom server just for you, and makes you the only user on this custom server, so that information gets repeated in this address: the dsanstudent.georgetown.domains server is created, with one user called dsanstudent.\n\n\nStep 2.3: Running the SSH Command\nNow that we understand this, we can run our command:\nssh -i \"/path/to/private/key/file\" username@domain\nIf you don’t see a scary all-caps error that looks like the following:\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nPermissions for 'id_rsa' are too open.\nIt is required that your private key files are NOT accessible by others.\nThis private key will be ignored.\nLoad key \"id_rsa\": bad permissions\nubuntu@192.168.0.1: Permission denied (publickey).\nthen you can jump to the next step. However, some students are getting this error. If you see this, you can fix it by doing the following:\n\n\n\n\n\n\nFixing the INCORRECT PRIVATE KEY PERMISSIONS Error\n\n\n\nIf you see an error that looks like the above, with WARNING: UNPROTECTED PRIVATE KEY FILE! all in caps, this means that when you downloaded you private key from the browser (Chrome, Firefox, etc.), it set the permissions on this file to be the “standard” permissions for any downloaded file, which is not what we want for the private key file, since this is an extremely important file that we want to keep secured. So, we’ll have the change the permissions on this file to be as secure as possible while still allowing us to use it in the terminal.\nSo, open up a new terminal window. Using the cd command (along with pwd to see what your current working directory is), navigate to the location where your id_rsa file is stored. Once you have arrived at that path (you can tell by running ls and seeing if id_rsa is one of the files listed in the directory), you can use the following command to change the permissions on the id_rsa file to the required settings for ssh access:\nchmod 600 id_rsa\nThis tells your computer to change the permissions of the id_rsa file to the permissions code 600, which corresponds to the case where you (the owner) have read and write permissions, but anyone else has no permissions (no read, no write, and no execute) on this file. One you’ve done this, to make sure it worked, you can check the permissions on the id_rsa file specifically by running\nls -lah id_rsa\nThis should print out a detailed listing of information on the id_rsa file, and should look like the following if the permissions were set correctly:\n\n-rw-------@ 1 jpj  staff   1.7K Jun  9 23:40 id_rsa\n(Your computer should show your own username rather than jpj, and some usergroup besides staff, but the permissions string should look similar to the string all the way on the left of the above output: the -rw-------@ portion)\n\n\n\n\nStep 2.4: Entering the SSH Key Passphrase\nIf you did not encounter this WARNING: UNPROTECTED PRIVATE KEY FILE error, or if you encountered it but fixed it by changing the permissions on the id_rsa file and re-running the previous command (the ssh command), then the terminal should now be asking you for a passphrase for your SSH private key. In my case, this looks like:\n\njpj@GSAS-AL-06LVHQQ domains % ssh -i \"id_rsa\" jpjgeorg@jpj.georgetown.domains\nEnter passphrase for key 'id_rsa': \nAnd then (on Mac) it shows a tiny key icon after the : character (on Windows it should just show an empty black or white box). This is asking you to enter the SSH key passphrase that you created in step 1.2 above, not your general Georgetown account password that you use to access other Georgetown services like Canvas! So, start typing (or paste) that SSH key passphrase into the terminal.\nSome students have gotten worried at this point, because they notice that the cursor does not move as they type their passwords: this is the expected behavior, even though it feels weird to type and not see any feedback on the screen! It is a feature on Linux, which ensures that even if people are watching your screen, they cannot see how long your password is (it turns out that cracking passwords becomes much much easier, in many cases, if you know in advance how long the person’s password is).\nSo, you won’t see any feedback on the screen, but once you’ve typed or pasted your SSH secret key passphrase, press enter.\n\n\nStep 2.5: Checking That The Connection Was Successful\nIf the location of the private key file was successfully provided to the ssh command via the string after the -i flag, and if the private key passphrase was entered correctly, then it should successfully connect to your Georgetown Domains server. If you have indeed successfully connected, your terminal prompt (the portion of the terminal where you type new commands) will suddenly change. Whereas before it may have looked like\n\njpj@GSAS-AL-06LVHQQ ~ %\nNow it should look like\n\n[jpjgeorg@gtown3 ~]$ \nIf you’ve never logged in before, it may show some additional info above this prompt, but if you have logged in before, it will also show the following above the prompt:\n\nLast login: Sun Sep 10 21:47:30 2023 from 216.15.21.131\n[jpjgeorg@gtown3 ~]$\nYou have now successfully connected to your Georgetown Domains server! There is one remaining step, which will help us to move from just connecting to the server via ssh to transferring files to/from the server via scp or rsync.\n\n\nStep 2.6: Finding Your public_html Path\nOnce you have successfully connected to the server, you should also browse the directory structure to see how it works. In particular, as a first step, try just running\n\npwd\nThis should just show the full, absolute path to your home directory. In my case, the output was\n\n/home/jpjgeorg\nNext, run\n\nls\nIt should show a bunch of files and folders (probably not the same as the following output), but you should at least see a public_html subfolder:\n\naccess-logs    dsan-conference-posters  mail           public_ftp   tmp     www\ncomposer.json  etc                      _metadata.yml  public_html  var\ncomposer.lock  logs                     perl5          ssl          vendor\nThis public_html subfolder, within your home folder, is where your Georgetown Domains web server will look for and serve files when people type your domain (like dsanstudent.georgetown.domains) into their browser’s address bar.\nSo, let’s move into this public_html directory, by entering and executing\ncd public_html\nIf we now check our working directory, by entering and executing\npwd\nYou should now see something that looks like the following:\n/home/jpjgeorg/public_html\nYou should take a screenshot of this path, or write it down, or remember it some other way. Because, this is the remote path (the path on the remote server) that you’ll want to copy your local files to, when we ask you e.g. to copy your _site directory to your Georgetown Domains server."
  },
  {
    "objectID": "writeups/domains-ssh/index.html#step-2.7-exiting-the-ssh-session-and-returning-to-your-local-computer",
    "href": "writeups/domains-ssh/index.html#step-2.7-exiting-the-ssh-session-and-returning-to-your-local-computer",
    "title": "Troubleshooting SSH/SCP/rsync on Georgetown Domains",
    "section": "Step 2.7: Exiting the SSH Session and Returning to Your Local Computer",
    "text": "Step 2.7: Exiting the SSH Session and Returning to Your Local Computer\nNow that you know how to connect to a remote server using ssh, you’ll have to be a bit more vigilant about noticing which computer you’re working on when you are typing and running commands in the terminal. As mentioned above, if you see a terminal prompt that looks like the following\n[jpjgeorg@gtown3 ~]$\nThen this terminal is probably connected to your Georgetown Domains server, meaning that any commands you enter will be run on the Georgetown Domains server. If instead you see a terminal prompt that looks like\njpj@GSAS-AL-06LVHQQ ~ %\nThat means you are safely back on your local machine, so that the commands you enter will be run locally, on the processor of your laptop. Since we just finished an SSH session, however, we are probably still logged in to the Georgetown Domains server. So, to exit this session and return back to executing commands on your local machine, just enter and execute the exit command:\nexit\nIf successful, it should show output like the following:\nlogout\nConnection to jpj.georgetown.domains closed."
  },
  {
    "objectID": "writeups/domains-ssh/index.html#step-3-moving-from-ssh-to-scp",
    "href": "writeups/domains-ssh/index.html#step-3-moving-from-ssh-to-scp",
    "title": "Troubleshooting SSH/SCP/rsync on Georgetown Domains",
    "section": "Step 3: Moving from ssh to scp",
    "text": "Step 3: Moving from ssh to scp\nNow that you know how to provide the necessary information and connect to your Georgetown Domains server using the ssh command, we’ll move to the scp command. Basically, while the ssh command lets you connect to and “look around” inside your server, the scp command allows you to transfer files to and from your server. The information you need to provide, and the syntax, are very similar, so we’re done with the hardest parts!\n\nStep 3.1: Providing the Absolute Path to Your Private Key File (id_rsa)\nJust like the ssh command, you can provide a -i flag (short for “identity file”), followed by the path to your ssh private key file, as information for the scp command, as follows (again, don’t press Enter until we’re finished writing out the full command, in a later step):\nscp -i \"/path/to/your/file\"\nIn my case, since my file is located within the /Users/jpj/ directory on my hard drive, I start the scp command with\nscp -i \"/Users/jpj/id_rsa\"\n\n\nStep 3.2: Providing the Path (Absolute or Relative) to the Local File You’d Like to Copy\nNext, I need to give scp the path to the file that I’d like to copy to my Georgetown Domains server. Note that here we’re starting with file copying rather than folder copying, which I strongly recommend so that you can check whether the syntax of your scp command is correct before trying to copy a ton of files. Specifically, I recommend just making a test file called hello.txt, and seeing if you can successfully use scp to copy this file from your local drive to your Georgetown Domains server. To create a fake file like this very quickly, you can open a new terminal window and run the following commands:\ntouch hello.txt\necho \"Hello World\" &gt;&gt; hello.txt\nWherever you run these two commands, there will now be a hello.txt file which contains the text Hello World. Now, let’s try to copy this file to our Georgetown Domains server—specifically, the public_html directory within our Georgetown Domains server. To get there, we need to finish up the remaining parts of our scp command. Add in the path to the file on your local drive that you want to copy to the remote server as the next argument to scp, like:\nscp -i \"/path/to/your/key/file\" \"hello.txt\"\nIn this case, since I created hello.txt in the same directory that I am now executing the scp command from, I don’t need to provide an absolute path (like /Users/jpj/hello.txt). In general, though, if you’re trying to copy a file from somewhere in your computer that is not your current working directory, you need to include an absolute or relative path to that file here.\n\n\nStep 3.3: Providing the username@domain String\nNext, we include the username@domain information in the exact same way we included it in the ssh command, making our command look like this:\nscp -i \"/path/to/your/key/file\" \"hello.txt\" jpjgeorg@jpj.georgetown.domains\n\n\nStep 3.4: Providing the Remote Path Where We’d Like Our File(s) To Be Copied To\nThere is one last piece of information we need to provide: the path on the remote server where we would like to copy hello.txt to! In this case, we wrote it down in a previous step: it was /home/jpjgeorg/public_html. So, we enter this as the last piece of information in our scp command, but in a bit of a strange-looking way: we add a colon (:) to the end of the username@domain string, and then provide the desired remote path after this colon character. So, in this case, to transfer to the /home/jpjgeorg/public_html directory, we finish writing out the command as follows:\nscp -i \"/path/to/your/key/file\" \"hello.txt\" jpjgeorg@jpj.georgetown.domains:/home/jpjgeorg/public_html\n\n\nStep 3.5: Executing the Command\nFinally, we can press Enter and execute the command! If it worked successfully, you may see a summary of the information: that it has transferred a single file, with size of only a few bytes, called hello.txt. Now let’s check to make sure the copy actually worked\n\n\nStep 3.6: Making Sure The Copy Was Successful\nThere are two ways you could check that this copy worked. The first, and safer way, would be to ssh into our remote server again, use cd to navigate to the public_html directory, and then execute ls and check that hello.txt is one of the listed files.\nHowever, since public_html is a special directory, that is publicly-available, we could also check that hello.txt transferred correctly by opening our browser and typing our domain followed by /hello.txt. So, in my case for example, you or I or anyone else could open https://jpj.georgetown.domains/hello.txt, and check to see that it indeed shows the content Hello world!. If your domain also shows this, when you go to (for example) dsanstudent.georgetown.domains, then you know that the transfer was a success!\n\n\nStep 3.7: Copying Entire Folders Instead Of Just One File\nThere is only one more change you need to make to the scp command from above (the command which copied a file), to make it copy an entire folder rather than just one file. If you add an additional -r flag to the very beginning of your ssh command, before the -i flag, you are instructing ssh that it should copy the local path that you provide recursively. Meaning: if you now provide a path to a folder as the argument to ssh immediately after the path to your private key file (rather than a path to a file like we entered before), ssh will now recursiviely copy the folder and all of its contents to the specified remote directory.\nSo for example, if I had a folder called mysite within my home directory /home/jpj/ on my local drive, then I could modify the previous scp command as follows, to copy this folder to my Georgetown Domains server:\nscp -r -i \"/path/to/my/key/file\" \"/Users/jpj/mysite\" jpjgeorg@jpj.georgetown.domains:/home/jpjgeorg/public_html\nThis command, once executed, should now show a series of messages, indicating the progress/completion of each file within the mysite folder, as it copies them one-by-one into the /home/jpjgeorg/public_html/mysite folder on my remote machine.\nOnce this command completes, you can again check that the copy was successful in one of two ways: either (a) by using ssh to log into the server, then using cd and ls to move to the public_html folder and check its contents, or (b) opening your browser and checking (in my case) https://jpj.georgetown.domains/mysite.\nAs one final thing to notice: unlike in the case of ssh, we don’t need to use exit to leave an scp “session”. That is because, scp immediately exits from the remote server once the files have been copied. So, immediately after the scp command completes, you are still on your local computer, and commands that you enter and execute are still run locally (when in doubt, look for the terminal prompt! If it looks like [jpjgeorg@gtown3 ~]$, that means you still need to exit from the Georgetown Domains server back to your local machine.)"
  },
  {
    "objectID": "recordings/index.html",
    "href": "recordings/index.html",
    "title": "Lecture Recordings",
    "section": "",
    "text": "Order By\n       Default\n         \n          Week\n        \n         \n          Section\n        \n         \n          Title\n        \n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nWeek\n\n\nTitle\n\n\nSection\n\n\nLast Updated\n\n\n\n\n\n\n6\n\n\nW06 Lecture (Sec 03)\n\n\n03\n\n\nWednesday Sep 27, 2023\n\n\n\n\n6\n\n\nW06 Lecture (Sec 02)\n\n\n02\n\n\nTuesday Sep 26, 2023\n\n\n\n\n5\n\n\nW05 Lecture (Sec 03)\n\n\n03\n\n\nWednesday Sep 20, 2023\n\n\n\n\n5\n\n\nW05 Lecture (Sec 02)\n\n\n02\n\n\nTuesday Sep 19, 2023\n\n\n\n\n4\n\n\nWeek 04: Lecture (Sec 03)\n\n\n03\n\n\nWednesday Sep 13, 2023\n\n\n\n\n4\n\n\nWeek 04: Lab (Sec 03)\n\n\n03\n\n\nWednesday Sep 13, 2023\n\n\n\n\n4\n\n\nWeek 04: Lecture+Lab (Sec 02)\n\n\n02\n\n\nTuesday Sep 12, 2023\n\n\n\n\n3\n\n\nW03 Lecture+Lab (Sec 02 and 03)\n\n\n\n\n\nTuesday Sep 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w05/slides.html#data-formats-network-data",
    "href": "w05/slides.html#data-formats-network-data",
    "title": "Week 5: Data Cleaning",
    "section": "Data Formats: Network Data",
    "text": "Data Formats: Network Data\n\nNetwork data form two tabular datasets: one for nodes and one for edges\nEx: Network on left could be represented via pair of tabular datasets on right\n\n\n\n\n\n\n\nFigure 1: Network of some sort of ancient kinship structure\n\n\n\n\n\n\n\n\nnode_id\nlabel\n\n\n\n\n1\nBulbasaur\n\n\n2\nIvysaur\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n9\nBlastoise\n\n\n\nFigure 2: A tabular dataset representing the nodes in the network above\n\n\n\n\n\n\n\nedge_id\nsource\ntarget\nweight\n\n\n\n\n0\n1\n2\n16\n\n\n1\n2\n3\n32\n\n\n2\n4\n5\n16\n\n\n3\n5\n6\n36\n\n\n4\n7\n8\n16\n\n\n5\n8\n9\n36\n\n\n\nFigure 3: A tabular dataset representing the edges in the network above"
  },
  {
    "objectID": "w05/slides.html#web-scraping-with-requests-and-beautifulsoup",
    "href": "w05/slides.html#web-scraping-with-requests-and-beautifulsoup",
    "title": "Week 5: Data Cleaning",
    "section": "Web Scraping with requests and BeautifulSoup",
    "text": "Web Scraping with requests and BeautifulSoup\n\n\n\n\n\n\n\nWhat I Want To Do\nPython Code I Can Use\n\n\n\n\nSend an HTTP GET request\nresponse = requests.get(url)\n\n\nSend an HTTP POST request\nresponse = requests.post(url, post_data)\n\n\nGet just the plain HTML code (excluding headers, JS) returned by the request\nhtml_str = response.text\n\n\nParse a string containing HTML code\nsoup = BeautifulSoup(html_str, 'html.parser')\n\n\nGet contents of all &lt;xyz&gt; tags in the parsed HTML\nxyz_elts = soup.find_all('xyz')\n\n\nGet contents of the first &lt;xyz&gt; tag in the parsed HTML\nxyz_elt = soup.find('xyz')\n\n\nGet just the text (without formatting or tag info) contained in a given element\nxyz_elt.text"
  },
  {
    "objectID": "w05/slides.html#apis-recap",
    "href": "w05/slides.html#apis-recap",
    "title": "Week 5: Data Cleaning",
    "section": "APIs Recap",
    "text": "APIs Recap\n\nKeep in mind the distinction between your entire application and the API endpoints you want to make available to other developers!\n\n\n\n\n\n\n\n\n\nApplication\nShould Be Endpoints\nShouldn’t Be Endpoints\n\n\n\n\nVoting Machine\ncast_vote() (End User), get_vote_totals() (Admin)\nget_vote(name), get_previous_vote()\n\n\nGaming Platform\nget_points() (Anyone), add_points(), remove_points() (Game Companies)\nset_points()\n\n\nThermometer\nview_temperature()\nrelease_mercury()\n\n\nCanvas App for Georgetown\nview_grades() (different for Students and Teachers)\nSQL Statement for Storing and Retrieving Grades in Georgetown DB"
  },
  {
    "objectID": "w05/slides.html#rest-vs.-soap-vs.-graphql",
    "href": "w05/slides.html#rest-vs.-soap-vs.-graphql",
    "title": "Week 5: Data Cleaning",
    "section": "REST vs. SOAP vs. GraphQL",
    "text": "REST vs. SOAP vs. GraphQL\n\nSOAP: Standard Object Access Protocol: Operates using a stricter schema (data must have this form, must include these keys, etc.), XML only\nREST (REpresentational State Transfer): Uses standard HTTP, wide range of formats\nGraphQL (Graph Query Language): Rather than exposing several endpoints, each performing a single function, GraphQL exposes a single endpoint for queries (so that the server then goes and figures out how to satisfy these queries)\n\n\n\n\n\n\n\nCode\ndigraph G1 {\n    rankdir=TB;\n    subgraph cluster_00 {\n        dev[label=\"Developer\"];\n        api1[label=\"get_users\"];\n        api2[label=\"get_friends\"];\n        label=\"Endpoints\";\n        labelloc=\"bottom\";\n    }\n    dev -&gt; api1[label=\"API call\"];\n    api1 -&gt; dev;\n    dev -&gt; api2 [label=\"API call (id = 5)\"];\n    api2 -&gt; dev;\n    figureOut[label=\"is User 5 friends w Pitbull?\"]\n    dev -&gt; figureOut;\n}\n\n\n\n\n\n\n\nG1\n\n \n\ncluster_00\n\n Endpoints   \n\ndev\n\n Developer   \n\napi1\n\n get_users   \n\ndev-&gt;api1\n\n  API call   \n\napi2\n\n get_friends   \n\ndev-&gt;api2\n\n  API call (id = 5)   \n\nfigureOut\n\n is User 5 friends w Pitbull?   \n\ndev-&gt;figureOut\n\n    \n\napi1-&gt;dev\n\n    \n\napi2-&gt;dev\n\n   \n\n\n\n\n\nFigure 4: Using individual endpoints (get_users and get_friends) to derive answer to “Is User 5 friends with Pitbull?”\n\n\n\n\n\n\nCode\ndigraph G2 {\n    newrank=true;\n    rankdir=TB;\n    api1[label=\"get_users\"];\n    \n    //gqlServer -&gt; api1[label=\"API call\"];\n    subgraph cluster_00 {\n        node [label=\"Developer\"] dev;\n        node [label=\"GraphQL Server\"] gqlServer;\n        margin=\"25.5,0.5\"\n        dev -&gt; gqlServer[label=\"GraphQL API call\\n(\\\"is User 5 friends\\nwith Pitbull?\\\")\"];\n        gqlServer -&gt; dev;\n        label=\"(Single) Endpoint\";\n        labelloc=\"bottom\";\n    }\n    \n    {\n        rank=same;\n        api2[label=\"get_friends\"];\n        gqlServer -&gt; api1;\n        api1 -&gt; gqlServer[label=\"(internal)\"];\n        gqlServer -&gt; api2 [label=\"(internal)\"];\n        api2 -&gt; gqlServer;\n    }\n}\n\n\n\n\n\n\n\nG2\n\n \n\ncluster_00\n\n (Single) Endpoint   \n\napi1\n\n get_users   \n\ngqlServer\n\n GraphQL Server   \n\napi1-&gt;gqlServer\n\n  (internal)   \n\ndev\n\n Developer   \n\ndev-&gt;gqlServer\n\n  GraphQL API call (“is User 5 friends with Pitbull?”)   \n\ngqlServer-&gt;api1\n\n    \n\ngqlServer-&gt;dev\n\n    \n\napi2\n\n get_friends   \n\ngqlServer-&gt;api2\n\n  (internal)   \n\napi2-&gt;gqlServer\n\n   \n\n\n\n\n\nFigure 5: Answering the same question (“Is User 5 friends with Pitbull?”) directly using GraphQL"
  },
  {
    "objectID": "w05/slides.html#what-should-i-include-in-my-api",
    "href": "w05/slides.html#what-should-i-include-in-my-api",
    "title": "Week 5: Data Cleaning",
    "section": "What Should I Include in My API?",
    "text": "What Should I Include in My API?\nKey Principle: CRUD\n\nCreate: Place a new record in some table(s)\nRead: Get data for all (or subset of) records\nUpdate*: Locate record, change its value(s)\n\n“Upsert”: Update if already exists, otherwise create\n\nDelete: Remove record from some table(s)"
  },
  {
    "objectID": "w05/slides.html#the-unexpected-pitfalls",
    "href": "w05/slides.html#the-unexpected-pitfalls",
    "title": "Week 5: Data Cleaning",
    "section": "The Unexpected Pitfalls",
    "text": "The Unexpected Pitfalls\n\nYou find the perfect dataset for your project, only to open it and find…"
  },
  {
    "objectID": "w05/slides.html#data-cleaning",
    "href": "w05/slides.html#data-cleaning",
    "title": "Week 5: Data Cleaning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nThe most undervalued skill in data science!\nRegardless of industry, absurd variety of data formats1\n\n\n\n\n\nSource: XKCD #927\n\n\n\nTo fully appreciate the importance of standards in the modern industrial/technology-driven world, see Chapter 18: “The Empire of the Red Octagon” (referring to the US’s octagonal stop sign), in Immerwahr (2019)."
  },
  {
    "objectID": "w05/slides.html#the-data-cleaning-toolbox",
    "href": "w05/slides.html#the-data-cleaning-toolbox",
    "title": "Week 5: Data Cleaning",
    "section": "The Data Cleaning Toolbox",
    "text": "The Data Cleaning Toolbox\n\nText Editors\nRegular Expressions\nConversion Tools\nHTML Parsers"
  },
  {
    "objectID": "w05/slides.html#text-editors",
    "href": "w05/slides.html#text-editors",
    "title": "Week 5: Data Cleaning",
    "section": "Text Editors",
    "text": "Text Editors\n\n“Broken” data can often be fixed by manually examining it in a text editor!\n\n\n\n\nmy_data.csv\n\nid,var_A,var_B,var_C\\n\n1,val_1A,val_1B,val_1C\\r\\n\n2,val_2A,val_2B,val_2C\\n\n3,val_3A,val_3B,val_3C\\n\n\n\n\n\nCode\nlibrary(readr)\ndata &lt;- read_csv(\"assets/my_data.csv\")\ndata\n\n\n\n\n\n\nindex\nvar_1\nvar_2\nvar_3\n\n\n\n\nA\nval_A1\nval_A2\nval_A3\n\n\nB\nval_B1\nval_B2\nval_B3\n\n\nC\nval_C1\nval_C2\nval_C3"
  },
  {
    "objectID": "w05/slides.html#regular-expressions",
    "href": "w05/slides.html#regular-expressions",
    "title": "Week 5: Data Cleaning",
    "section": "Regular Expressions",
    "text": "Regular Expressions\n\nThe language for turning unstructured data into structured data\nIn Computer Science, a whole course, if not two…\ntldr: a regular expression, or a RegEx string, represents a machine that either accepts or rejects input strings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegEx\n[A-Za-z0-9]+\n@\n[A-Za-z0-9.-]+\n\\.\n(com|org|edu)\nResult:\n\n\nString A\njj1088\n@\ngeorgetown\n.\nedu\nAccept✅\n\n\nString B\nspammer\n@\nfakesite!!\n.\ncoolio\nReject"
  },
  {
    "objectID": "w05/slides.html#regular-expressions-intuition",
    "href": "w05/slides.html#regular-expressions-intuition",
    "title": "Week 5: Data Cleaning",
    "section": "Regular Expressions: Intuition",
    "text": "Regular Expressions: Intuition\n\nThe guiding principle: think of the types of strings you want to match:\n\nWhat kinds of characters do they contain? (and, what kinds of characters should they not contain?)\nWhat is the pattern in which these characters appear: is there a character limit? Which characters/patterns are optional, which are required?\n\nYou can then use the RegEx syntax to encode the answers to these questions!"
  },
  {
    "objectID": "w05/slides.html#regex-syntax-single-characters",
    "href": "w05/slides.html#regex-syntax-single-characters",
    "title": "Week 5: Data Cleaning",
    "section": "RegEx Syntax: Single Characters",
    "text": "RegEx Syntax: Single Characters\n\nz: Match lowercase z, a single time\nzz: Match two lowercase zs in a row\nz{n}: Match \\(n\\) lowercase zs in a row\n[abc]: Match a, b, or c, a single time\n[A-Z]: Match one uppercase letter\n[0-9]: Match one numeric digit\n[A-Za-z0-9]: Match a single alphanumeric character\n[A-Za-z0-9]{n}: Match \\(n\\) alphanumeric characters"
  },
  {
    "objectID": "w05/slides.html#regex-syntax-repeating-patterns",
    "href": "w05/slides.html#regex-syntax-repeating-patterns",
    "title": "Week 5: Data Cleaning",
    "section": "RegEx Syntax: Repeating Patterns",
    "text": "RegEx Syntax: Repeating Patterns\n\nz*: Match lowercase z zero or more times\nz+: Match lowercase z one or more times\nz?: Match zero or one lowercase zs\n\n\n\n\n\n\n\n\n\n\n\n\n\nz*\nz+\nz?\nz{3}\n\n\n\n\n\"\"\n✅\n\n✅\n\n\n\n\"z\"\n✅\n✅\n✅\n\n\n\n\"zzz\"\n✅\n✅\n\n✅"
  },
  {
    "objectID": "w05/slides.html#example-us-phone-numbers",
    "href": "w05/slides.html#example-us-phone-numbers",
    "title": "Week 5: Data Cleaning",
    "section": "Example: US Phone Numbers",
    "text": "Example: US Phone Numbers\n\nArea code sometimes surrounded by parentheses:\n\n202-687-1587 and (202) 687-1587 both valid!\n\nWhich repeating pattern syntax (from the previous slide) helps us here?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegEx\n[(]?\n[0-9]{3}\n[)]?\n[ -]\n[0-9]{3}-[0-9]{4}\nResult\n\n\n\"202-687-1587\"\n\\(\\varepsilon\\)\n202\n\\(\\varepsilon\\)\n-\n687-1587\nAccept\n\n\n\"(202) 687-1587\"\n(\n202\n)\n \n687-1587\nAccept\n\n\n\"2020687-1587\"\n\\(\\varepsilon\\)\n202\n\\(\\varepsilon\\)\n0\n687-1587\nReject"
  },
  {
    "objectID": "w05/slides.html#building-and-testing-regex-strings",
    "href": "w05/slides.html#building-and-testing-regex-strings",
    "title": "Week 5: Data Cleaning",
    "section": "Building and Testing RegEx Strings",
    "text": "Building and Testing RegEx Strings\n\nRegExr.com →"
  },
  {
    "objectID": "w05/slides.html#tidy-data",
    "href": "w05/slides.html#tidy-data",
    "title": "Week 5: Data Cleaning",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\n\nOverview | In-Depth\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a value\n\n\n\n\n\n\n\n\nVar1\nVar 2\n\n\n\n\nObs 1\nVal 1\nVal 2\n\n\nObs 2\nVal 3\nVal 4\n\n\n\nFigure 6: A template for tidy data, with observations in blue, variables in orange, and values in green\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\ntable1\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583\n\n\n\n\n\n\nFigure 7: An example panel dataset in tidy format\n\n\n\n\n\n\nCode\ntable2 %&gt;% head(6)\n\n\n\n\n\n\ncountry\nyear\ntype\ncount\n\n\n\n\nAfghanistan\n1999\ncases\n745\n\n\nAfghanistan\n1999\npopulation\n19987071\n\n\nAfghanistan\n2000\ncases\n2666\n\n\nAfghanistan\n2000\npopulation\n20595360\n\n\nBrazil\n1999\ncases\n37737\n\n\nBrazil\n1999\npopulation\n172006362\n\n\n\n\n\n\nFigure 8: An example panel dataset in non-tidy format"
  },
  {
    "objectID": "w05/slides.html#how-do-we-get-our-data-into-tidy-form",
    "href": "w05/slides.html#how-do-we-get-our-data-into-tidy-form",
    "title": "Week 5: Data Cleaning",
    "section": "How Do We Get Our Data Into Tidy Form?",
    "text": "How Do We Get Our Data Into Tidy Form?\n\nR: The Tidyverse\nPython: Pandas + Regular Expressions (lab demo!)"
  },
  {
    "objectID": "w05/slides.html#the-tidyverse",
    "href": "w05/slides.html#the-tidyverse",
    "title": "Week 5: Data Cleaning",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\n\nhttps://www.tidyverse.org/\n\n\n\ndplyr →\nggplot2 →\nforcats →\ntibble →\nreadr →\nstringr →\ntidyr →\npurrr →"
  },
  {
    "objectID": "w05/slides.html#tibble",
    "href": "w05/slides.html#tibble",
    "title": "Week 5: Data Cleaning",
    "section": "tibble",
    "text": "tibble\nHomepage | Overview | Cheatsheet\n\n\n\n\nReplaces R’s built-in data.frame objects, but retains syntax for backwards compatibility:\n\n\n\nCode\nlibrary(tibble)\ndata &lt;- c(3.4,1.1,9.6)\nlabels &lt;- c(0,1,0)\nsupervised_df &lt;- tibble(x=data, y=labels)\nsupervised_df\n\n\n\n\n\n\nx\ny\n\n\n\n\n3.4\n0\n\n\n1.1\n1\n\n\n9.6\n0\n\n\n\n\n\n\n\n\n\nProvides a surprisingly useful function: tribble() (tibble defined row-by-row)\n\n\n\nCode\nlibrary(tibble)\ndsan_df &lt;- tibble::tribble(\n    ~code, ~topic, ~credits,\n    \"dsan5000\", \"Data Science\", 3,\n    \"dsan5100\", \"Probabilistic Modeling\", 3\n)\ndsan_df\n\n\n\n\n\n\ncode\ntopic\ncredits\n\n\n\n\ndsan5000\nData Science\n3\n\n\ndsan5100\nProbabilistic Modeling\n3"
  },
  {
    "objectID": "w05/slides.html#dplyr",
    "href": "w05/slides.html#dplyr",
    "title": "Week 5: Data Cleaning",
    "section": "dplyr",
    "text": "dplyr\nHomepage | Overview | Cheatsheet\n\n\nGrammar of data manipulation (think verbs):\n\nfilter()\nselect()\narrange()\nmutate()\nsummarize()\n\n\nfilter():\n\n\nCode\ntable1 |&gt; filter(year == 2000)\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n2000\n213766\n1280428583"
  },
  {
    "objectID": "w05/slides.html#section",
    "href": "w05/slides.html#section",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "select():\n\n\nCode\ntable1 |&gt; select(country)\n\n\n\n\n\n\ncountry\n\n\n\n\nAfghanistan\n\n\nAfghanistan\n\n\nBrazil\n\n\nBrazil\n\n\nChina\n\n\nChina\n\n\n\n\n\n\n\narrange():\n\n\nCode\ntable1 |&gt; arrange(population)\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n\n\nAfghanistan\n2000\n2666\n20595360\n\n\nBrazil\n1999\n37737\n172006362\n\n\nBrazil\n2000\n80488\n174504898\n\n\nChina\n1999\n212258\n1272915272\n\n\nChina\n2000\n213766\n1280428583"
  },
  {
    "objectID": "w05/slides.html#section-1",
    "href": "w05/slides.html#section-1",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "mutate():\n\n\nCode\ntable1 |&gt; mutate(newvar = 300)\n\n\n\n\n\n\ncountry\nyear\ncases\npopulation\nnewvar\n\n\n\n\nAfghanistan\n1999\n745\n19987071\n300\n\n\nAfghanistan\n2000\n2666\n20595360\n300\n\n\nBrazil\n1999\n37737\n172006362\n300\n\n\nBrazil\n2000\n80488\n174504898\n300\n\n\nChina\n1999\n212258\n1272915272\n300\n\n\nChina\n2000\n213766\n1280428583\n300"
  },
  {
    "objectID": "w05/slides.html#section-2",
    "href": "w05/slides.html#section-2",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "summarize():\n\n\nCode\ntable1 |&gt; \n  summarize(\n    avg_cases = mean(cases),\n    avg_pop = mean(population)\n  )\n\n\n\n\n\n\navg_cases\navg_pop\n\n\n\n\n91276.67\n490072924\n\n\n\n\n\n\n\nsummarize() with grouping:\n\n\nCode\ntable1 |&gt;\n  group_by(country) |&gt;\n  summarize(\n    avg_cases = mean(cases),\n    avg_pop = mean(population)\n  )\n\n\n\n\n\n\ncountry\navg_cases\navg_pop\n\n\n\n\nAfghanistan\n1705.5\n20291216\n\n\nBrazil\n59112.5\n173255630\n\n\nChina\n213012.0\n1276671928"
  },
  {
    "objectID": "w05/slides.html#the-rest-of-the-tidyverse",
    "href": "w05/slides.html#the-rest-of-the-tidyverse",
    "title": "Week 5: Data Cleaning",
    "section": "The Rest of the Tidyverse",
    "text": "The Rest of the Tidyverse\n\nforcats\nreadr\nstringr\ntidyr\npurrr\nlubridate*"
  },
  {
    "objectID": "w05/slides.html#forcats",
    "href": "w05/slides.html#forcats",
    "title": "Week 5: Data Cleaning",
    "section": "forcats",
    "text": "forcats\nHomepage | Overview | Cheatsheet\n\nUtilities for working with factor variables (R’s data structure for categorical variables)\nfactors = data + levels:\n\n\n\n\n\nCode\nmonth_levels &lt;- c(\n    \"Jan\", \"Feb\", \"Mar\", \"Apr\",\n    \"May\", \"Jun\", \"Jul\", \"Aug\",\n    \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\nd &lt;- c(\"Jan\",\"Jan\",\"Feb\",\"Dec\")\nprint(d)\n\n\n[1] \"Jan\" \"Jan\" \"Feb\" \"Dec\"\n\n\n\n\nCode\ndataf &lt;- parse_factor(\n    d,\n    levels=month_levels\n)\nprint(dataf)\n\n\n[1] Jan Jan Feb Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec"
  },
  {
    "objectID": "w05/slides.html#factors-for-ordering-plot-elements",
    "href": "w05/slides.html#factors-for-ordering-plot-elements",
    "title": "Week 5: Data Cleaning",
    "section": "Factors for Ordering Plot Elements",
    "text": "Factors for Ordering Plot Elements\n\n\n\n\nCode\nrelig_summary &lt;- gss_cat %&gt;%\n  group_by(relig) %&gt;%\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\nrelig_labs &lt;- labs(\n  x = \"TV Hours / Day\",\n  y = \"Religion\"\n)\n\n\nWithout forcats:\n\n\nCode\nrelig_summary |&gt;\n  ggplot(aes(tvhours, relig)) +\n  geom_point(size=g_pointsize) +\n  geom_segment(aes(yend = relig, x=0, xend = tvhours)) +\n  dsan_theme(\"half\") +\n  relig_labs\n\n\n\n\n\n\nWith forcats:\n\n\nCode\nrelig_summary |&gt;\n  mutate(relig = fct_reorder(relig, tvhours)) |&gt;\n  ggplot(aes(x=tvhours, y=relig)) +\n    geom_point(size=g_pointsize) +\n    geom_segment(aes(yend = relig, x=0, xend = tvhours)) +\n    dsan_theme(\"half\") +\n    relig_labs"
  },
  {
    "objectID": "w05/slides.html#sorting-barplots-with-fct_infreq",
    "href": "w05/slides.html#sorting-barplots-with-fct_infreq",
    "title": "Week 5: Data Cleaning",
    "section": "Sorting Barplots with fct_infreq()",
    "text": "Sorting Barplots with fct_infreq()\n\n\nCode\nbarplot_labs &lt;- labs(\n  title = \"Respondents by Marital Status\",\n  x = \"Marital Status\",\n  y = \"Count\"\n)\n\n\n\n\nCode\ngss_cat |&gt;\n  mutate(marital = marital |&gt; fct_infreq() |&gt; fct_rev()) |&gt;\n  ggplot(aes(marital)) + geom_bar() + barplot_labs +\n  dsan_theme(\"full\")"
  },
  {
    "objectID": "w05/slides.html#recodingcombining-categories",
    "href": "w05/slides.html#recodingcombining-categories",
    "title": "Week 5: Data Cleaning",
    "section": "Recoding/Combining Categories",
    "text": "Recoding/Combining Categories\n\n\nAutomatically combining using fct_lump():\n\n\nCode\ngss_cat |&gt;\n    mutate(relig = fct_lump(relig)) |&gt;\n    count(relig)\n\n\n\n\n\n\nrelig\nn\n\n\n\n\nProtestant\n10846\n\n\nOther\n10637\n\n\n\n\n\n\n\nManually Combining using fct_recode():\n\n\nCode\ngss_cat |&gt;\n    mutate(partyid = fct_recode(partyid,\n    \"Republican\"  = \"Strong republican\",\n    \"Republican\"  = \"Not str republican\",\n    \"Independent\" = \"Ind,near rep\",\n    \"Independent\" = \"Ind,near dem\",\n    \"Democrat\"    = \"Not str democrat\",\n    \"Democrat\"    = \"Strong democrat\",\n    \"Other\"       = \"No answer\",\n    \"Other\"       = \"Don't know\",\n    \"Other\"       = \"Other party\"\n  )) |&gt;\n  count(partyid)\n\n\n\n\n\n\npartyid\nn\n\n\n\n\nOther\n548\n\n\nRepublican\n5346\n\n\nIndependent\n8409\n\n\nDemocrat\n7180"
  },
  {
    "objectID": "w05/slides.html#readr",
    "href": "w05/slides.html#readr",
    "title": "Week 5: Data Cleaning",
    "section": "readr",
    "text": "readr\nHomepage | Overview | Cheatsheet\n\nTwo key functions: read_csv(), write_csv()1\n\n\n\n\n\n\nWarning! read_csv() vs. read.csv()\n\n\nNote that these are not the same as R’s built-in read.csv() and write.csv()! The built-in R functions will produce a plain data.frame object, not a tibble\n\n\n\n\n\n\n\n\nCan handle URLs as well!\nShare data+code in seconds by using in combination with GitHub Gist\n(No more read_csv() path issues either…)\nGist dataset →\n\n\n\n\nCode\ngdp_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/fecd437b96d0954893de727383f2eaf2/raw/fec58507f7095cb8341b229d6eb74ce53232d663/gdp_2010.csv\")\ngdp_df |&gt; head(6)\n\n\n\n\n\n\nname\ncode\nyear\nvalue\n\n\n\n\nAfghanistan\nAFG\n2010\n15936800636\n\n\nAlbania\nALB\n2010\n11926953259\n\n\nAlgeria\nDZA\n2010\n161207268655\n\n\nAmerican Samoa\nASM\n2010\n576000000\n\n\nAndorra\nAND\n2010\n3355695364\n\n\nAngola\nAGO\n2010\n82470913121\n\n\n\n\n\n\n\n\nPlus a bonus read_delim() if read_csv() fails"
  },
  {
    "objectID": "w05/slides.html#purrr-functional-programming",
    "href": "w05/slides.html#purrr-functional-programming",
    "title": "Week 5: Data Cleaning",
    "section": "purrr: Functional Programming",
    "text": "purrr: Functional Programming\nHomepage | Overview | Cheatsheet | Tutorials, with Applications\n\n\nProvides an anonymous function operator ~, arguments get named .x, .y:\n\n\nCode\nmy_points &lt;- c(\"Midterm\"=18, \"Final\"=300)\ntotal_points &lt;- c(\"Midterm\"=20, \"Final\"=400)\n(avg_score &lt;- map2(my_points, total_points,\n  ~ list(frac=.x / .y, pct=(.x/.y)*100)))\n\n\n$Midterm\n$Midterm$frac\n[1] 0.9\n\n$Midterm$pct\n[1] 90\n\n\n$Final\n$Final$frac\n[1] 0.75\n\n$Final$pct\n[1] 75\n\n\n\nAlong with helpful functions for transforming the output\n\n\nCode\nlist_flatten(avg_score)\n\n\n$Midterm_frac\n[1] 0.9\n\n$Midterm_pct\n[1] 90\n\n$Final_frac\n[1] 0.75\n\n$Final_pct\n[1] 75\n\n\n\n\nCode\nevery(avg_score, ~ .x$frac &gt; 0.5)\n\n\n[1] TRUE"
  },
  {
    "objectID": "w05/slides.html#lubridate",
    "href": "w05/slides.html#lubridate",
    "title": "Week 5: Data Cleaning",
    "section": "lubridate*",
    "text": "lubridate*\nHomepage | Overview | Cheatsheet\n\n\n\n\n\n\nCaution: Importing lubridate\n\n\nNote that lubridate does not get loaded with the other packages in the tidyverse when you call library(tidyverse). It needs to be imported explicitly:\nlibrary(tidyverse)\nlibrary(lubridate)"
  },
  {
    "objectID": "w05/slides.html#one-of-the-scariest-papers-of-all-time",
    "href": "w05/slides.html#one-of-the-scariest-papers-of-all-time",
    "title": "Week 5: Data Cleaning",
    "section": "One of the Scariest Papers of All Time",
    "text": "One of the Scariest Papers of All Time\n\nText Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It (Denny and Spirling 2018) (PDF Link)\n\n\n\n\n\nFigure 9: A plot where \\(x\\)-axis represents UK party manifestos (arranged on left-right ideology dimension, \\(y\\)-axis slots represent an ordering of preprocessing steps, and a filled bar means the manifesto was placed incorrectly on the ideological dimension when that preprocessing combination was used.\n\n\n\n\n\n\nFigure 10: An ancient hieroglyph"
  },
  {
    "objectID": "w05/slides.html#the-secret-behind-all-text-analysis",
    "href": "w05/slides.html#the-secret-behind-all-text-analysis",
    "title": "Week 5: Data Cleaning",
    "section": "The Secret Behind All Text Analysis",
    "text": "The Secret Behind All Text Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Excerpts from two data science textbooks, plus another book\n\n\n\n\n\n\n\n\n\n\ndoc_id\ntext\ntexts\nKékkek\nvoice\n\n\n\n\n\n0\n0\n6\n0\n1\n\n\n\n1\n0\n0\n3\n1\n\n\n\n2\n6\n0\n0\n0\n\n\n\n\nFigure 12: The Document-Term Matrix (DTM)\n\n\n\n\n\n\n\n\n\n \n\n\n\ndoc_id\ntext\nkekkek\nvoice\n\n\n\n\n\n\n0\n6\n0\n1\n\n\n\n\n1\n0\n3\n1\n\n\n\n\n2\n6\n0\n0\n\n\n\n\n\nFigure 13: The cleaned DTM, after lowercasing, lemmatization, and unicode standardization"
  },
  {
    "objectID": "w05/slides.html#lab-assignment-overview",
    "href": "w05/slides.html#lab-assignment-overview",
    "title": "Week 5: Data Cleaning",
    "section": "Lab Assignment Overview",
    "text": "Lab Assignment Overview\nLab-2.2: Cleaning Record Data in R and Python"
  },
  {
    "objectID": "w05/slides.html#references",
    "href": "w05/slides.html#references",
    "title": "Week 5: Data Cleaning",
    "section": "References",
    "text": "References\n\n\nDenny, Matthew J., and Arthur Spirling. 2018. “Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It.” Political Analysis 26 (2): 168–89. https://doi.org/10.1017/pan.2017.44.\n\n\nImmerwahr, Daniel. 2019. How to Hide an Empire: A History of the Greater United States. Farrar, Straus and Giroux.\n\n\n\n\nDSAN 5000 Week 5: Data Cleaning"
  },
  {
    "objectID": "w05/index.html",
    "href": "w05/index.html",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w05/index.html#data-formats-network-data",
    "href": "w05/index.html#data-formats-network-data",
    "title": "Week 5: Data Cleaning",
    "section": "Data Formats: Network Data",
    "text": "Data Formats: Network Data\n\nNetwork data form two tabular datasets: one for nodes and one for edges\nEx: Network on left could be represented via pair of tabular datasets on right\n\n\n\n\n\n\n\nFigure 1: Network of some sort of ancient kinship structure\n\n\n\n\n\n\n\n\nnode_id\nlabel\n\n\n\n\n1\nBulbasaur\n\n\n2\nIvysaur\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n9\nBlastoise\n\n\n\nFigure 2: A tabular dataset representing the nodes in the network above\n\n\n\n\n\n\n\nedge_id\nsource\ntarget\nweight\n\n\n\n\n0\n1\n2\n16\n\n\n1\n2\n3\n32\n\n\n2\n4\n5\n16\n\n\n3\n5\n6\n36\n\n\n4\n7\n8\n16\n\n\n5\n8\n9\n36\n\n\n\nFigure 3: A tabular dataset representing the edges in the network above"
  },
  {
    "objectID": "w05/index.html#web-scraping-with-requests-and-beautifulsoup",
    "href": "w05/index.html#web-scraping-with-requests-and-beautifulsoup",
    "title": "Week 5: Data Cleaning",
    "section": "Web Scraping with requests and BeautifulSoup",
    "text": "Web Scraping with requests and BeautifulSoup\n\n\n\n\n\n\n\nWhat I Want To Do\nPython Code I Can Use\n\n\n\n\nSend an HTTP GET request\nresponse = requests.get(url)\n\n\nSend an HTTP POST request\nresponse = requests.post(url, post_data)\n\n\nGet just the plain HTML code (excluding headers, JS) returned by the request\nhtml_str = response.text\n\n\nParse a string containing HTML code\nsoup = BeautifulSoup(html_str, 'html.parser')\n\n\nGet contents of all &lt;xyz&gt; tags in the parsed HTML\nxyz_elts = soup.find_all('xyz')\n\n\nGet contents of the first &lt;xyz&gt; tag in the parsed HTML\nxyz_elt = soup.find('xyz')\n\n\nGet just the text (without formatting or tag info) contained in a given element\nxyz_elt.text"
  },
  {
    "objectID": "w05/index.html#apis-recap",
    "href": "w05/index.html#apis-recap",
    "title": "Week 5: Data Cleaning",
    "section": "APIs Recap",
    "text": "APIs Recap\n\nKeep in mind the distinction between your entire application and the API endpoints you want to make available to other developers!\n\n\n\n\n\n\n\n\n\nApplication\nShould Be Endpoints\nShouldn’t Be Endpoints\n\n\n\n\nVoting Machine\ncast_vote() (End User), get_vote_totals() (Admin)\nget_vote(name), get_previous_vote()\n\n\nGaming Platform\nget_points() (Anyone), add_points(), remove_points() (Game Companies)\nset_points()\n\n\nThermometer\nview_temperature()\nrelease_mercury()\n\n\nCanvas App for Georgetown\nview_grades() (different for Students and Teachers)\nSQL Statement for Storing and Retrieving Grades in Georgetown DB"
  },
  {
    "objectID": "w05/index.html#rest-vs.-soap-vs.-graphql",
    "href": "w05/index.html#rest-vs.-soap-vs.-graphql",
    "title": "Week 5: Data Cleaning",
    "section": "REST vs. SOAP vs. GraphQL",
    "text": "REST vs. SOAP vs. GraphQL\n\nSOAP: Standard Object Access Protocol: Operates using a stricter schema (data must have this form, must include these keys, etc.), XML only\nREST (REpresentational State Transfer): Uses standard HTTP, wide range of formats\nGraphQL (Graph Query Language): Rather than exposing several endpoints, each performing a single function, GraphQL exposes a single endpoint for queries (so that the server then goes and figures out how to satisfy these queries)\n\n\n\n\n\n\ndigraph G1 {\n    rankdir=TB;\n    subgraph cluster_00 {\n        dev[label=\"Developer\"];\n        api1[label=\"get_users\"];\n        api2[label=\"get_friends\"];\n        label=\"Endpoints\";\n        labelloc=\"bottom\";\n    }\n    dev -&gt; api1[label=\"API call\"];\n    api1 -&gt; dev;\n    dev -&gt; api2 [label=\"API call (id = 5)\"];\n    api2 -&gt; dev;\n    figureOut[label=\"is User 5 friends w Pitbull?\"]\n    dev -&gt; figureOut;\n}\n\n\n\n\n\n\nG1\n\n \n\ncluster_00\n\n Endpoints   \n\ndev\n\n Developer   \n\napi1\n\n get_users   \n\ndev-&gt;api1\n\n  API call   \n\napi2\n\n get_friends   \n\ndev-&gt;api2\n\n  API call (id = 5)   \n\nfigureOut\n\n is User 5 friends w Pitbull?   \n\ndev-&gt;figureOut\n\n    \n\napi1-&gt;dev\n\n    \n\napi2-&gt;dev\n\n   \n\n\n\n\n\nFigure 4: Using individual endpoints (get_users and get_friends) to derive answer to “Is User 5 friends with Pitbull?”\n\n\n\n\n\ndigraph G2 {\n    newrank=true;\n    rankdir=TB;\n    api1[label=\"get_users\"];\n    \n    //gqlServer -&gt; api1[label=\"API call\"];\n    subgraph cluster_00 {\n        node [label=\"Developer\"] dev;\n        node [label=\"GraphQL Server\"] gqlServer;\n        margin=\"25.5,0.5\"\n        dev -&gt; gqlServer[label=\"GraphQL API call\\n(\\\"is User 5 friends\\nwith Pitbull?\\\")\"];\n        gqlServer -&gt; dev;\n        label=\"(Single) Endpoint\";\n        labelloc=\"bottom\";\n    }\n    \n    {\n        rank=same;\n        api2[label=\"get_friends\"];\n        gqlServer -&gt; api1;\n        api1 -&gt; gqlServer[label=\"(internal)\"];\n        gqlServer -&gt; api2 [label=\"(internal)\"];\n        api2 -&gt; gqlServer;\n    }\n}\n\n\n\n\n\n\nG2\n\n \n\ncluster_00\n\n (Single) Endpoint   \n\napi1\n\n get_users   \n\ngqlServer\n\n GraphQL Server   \n\napi1-&gt;gqlServer\n\n  (internal)   \n\ndev\n\n Developer   \n\ndev-&gt;gqlServer\n\n  GraphQL API call (“is User 5 friends with Pitbull?”)   \n\ngqlServer-&gt;api1\n\n    \n\ngqlServer-&gt;dev\n\n    \n\napi2\n\n get_friends   \n\ngqlServer-&gt;api2\n\n  (internal)   \n\napi2-&gt;gqlServer\n\n   \n\n\n\n\n\nFigure 5: Answering the same question (“Is User 5 friends with Pitbull?”) directly using GraphQL"
  },
  {
    "objectID": "w05/index.html#what-should-i-include-in-my-api",
    "href": "w05/index.html#what-should-i-include-in-my-api",
    "title": "Week 5: Data Cleaning",
    "section": "What Should I Include in My API?",
    "text": "What Should I Include in My API?\nKey Principle: CRUD\n\nCreate: Place a new record in some table(s)\nRead: Get data for all (or subset of) records\nUpdate*: Locate record, change its value(s)\n\n“Upsert”: Update if already exists, otherwise create\n\nDelete: Remove record from some table(s)"
  },
  {
    "objectID": "w05/index.html#the-unexpected-pitfalls",
    "href": "w05/index.html#the-unexpected-pitfalls",
    "title": "Week 5: Data Cleaning",
    "section": "The Unexpected Pitfalls",
    "text": "The Unexpected Pitfalls\n\nYou find the perfect dataset for your project, only to open it and find…"
  },
  {
    "objectID": "w05/index.html#data-cleaning",
    "href": "w05/index.html#data-cleaning",
    "title": "Week 5: Data Cleaning",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\nThe most undervalued skill in data science!\nRegardless of industry, absurd variety of data formats1\n\n\n\n\n\nSource: XKCD #927"
  },
  {
    "objectID": "w05/index.html#the-data-cleaning-toolbox",
    "href": "w05/index.html#the-data-cleaning-toolbox",
    "title": "Week 5: Data Cleaning",
    "section": "The Data Cleaning Toolbox",
    "text": "The Data Cleaning Toolbox\n\nText Editors\nRegular Expressions\nConversion Tools\nHTML Parsers"
  },
  {
    "objectID": "w05/index.html#text-editors",
    "href": "w05/index.html#text-editors",
    "title": "Week 5: Data Cleaning",
    "section": "Text Editors",
    "text": "Text Editors\n\n“Broken” data can often be fixed by manually examining it in a text editor!\n\n\n\n\nmy_data.csv\n\nid,var_A,var_B,var_C\\n\n1,val_1A,val_1B,val_1C\\r\\n\n2,val_2A,val_2B,val_2C\\n\n3,val_3A,val_3B,val_3C\\n\n\n\n\nlibrary(readr)\ndata &lt;- read_csv(\"assets/my_data.csv\")\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): index, var_1, var_2, var_3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndata\n\n# A tibble: 3 × 4\n  index var_1  var_2  var_3 \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; \n1 A     val_A1 val_A2 val_A3\n2 B     val_B1 val_B2 val_B3\n3 C     val_C1 val_C2 val_C3"
  },
  {
    "objectID": "w05/index.html#regular-expressions",
    "href": "w05/index.html#regular-expressions",
    "title": "Week 5: Data Cleaning",
    "section": "Regular Expressions",
    "text": "Regular Expressions\n\nThe language for turning unstructured data into structured data\nIn Computer Science, a whole course, if not two…\ntldr: a regular expression, or a RegEx string, represents a machine that either accepts or rejects input strings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegEx\n[A-Za-z0-9]+\n@\n[A-Za-z0-9.-]+\n\\.\n(com|org|edu)\nResult:\n\n\nString A\njj1088\n@\ngeorgetown\n.\nedu\nAccept✅\n\n\nString B\nspammer\n@\nfakesite!!\n.\ncoolio\nReject"
  },
  {
    "objectID": "w05/index.html#regular-expressions-intuition",
    "href": "w05/index.html#regular-expressions-intuition",
    "title": "Week 5: Data Cleaning",
    "section": "Regular Expressions: Intuition",
    "text": "Regular Expressions: Intuition\n\nThe guiding principle: think of the types of strings you want to match:\n\nWhat kinds of characters do they contain? (and, what kinds of characters should they not contain?)\nWhat is the pattern in which these characters appear: is there a character limit? Which characters/patterns are optional, which are required?\n\nYou can then use the RegEx syntax to encode the answers to these questions!"
  },
  {
    "objectID": "w05/index.html#regex-syntax-single-characters",
    "href": "w05/index.html#regex-syntax-single-characters",
    "title": "Week 5: Data Cleaning",
    "section": "RegEx Syntax: Single Characters",
    "text": "RegEx Syntax: Single Characters\n\nz: Match lowercase z, a single time\nzz: Match two lowercase zs in a row\nz{n}: Match \\(n\\) lowercase zs in a row\n[abc]: Match a, b, or c, a single time\n[A-Z]: Match one uppercase letter\n[0-9]: Match one numeric digit\n[A-Za-z0-9]: Match a single alphanumeric character\n[A-Za-z0-9]{n}: Match \\(n\\) alphanumeric characters"
  },
  {
    "objectID": "w05/index.html#regex-syntax-repeating-patterns",
    "href": "w05/index.html#regex-syntax-repeating-patterns",
    "title": "Week 5: Data Cleaning",
    "section": "RegEx Syntax: Repeating Patterns",
    "text": "RegEx Syntax: Repeating Patterns\n\nz*: Match lowercase z zero or more times\nz+: Match lowercase z one or more times\nz?: Match zero or one lowercase zs\n\n\n\n\n\n\n\n\n\n\n\n\n\nz*\nz+\nz?\nz{3}\n\n\n\n\n\"\"\n✅\n\n✅\n\n\n\n\"z\"\n✅\n✅\n✅\n\n\n\n\"zzz\"\n✅\n✅\n\n✅"
  },
  {
    "objectID": "w05/index.html#example-us-phone-numbers",
    "href": "w05/index.html#example-us-phone-numbers",
    "title": "Week 5: Data Cleaning",
    "section": "Example: US Phone Numbers",
    "text": "Example: US Phone Numbers\n\nArea code sometimes surrounded by parentheses:\n\n202-687-1587 and (202) 687-1587 both valid!\n\nWhich repeating pattern syntax (from the previous slide) helps us here?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegEx\n[(]?\n[0-9]{3}\n[)]?\n[ -]\n[0-9]{3}-[0-9]{4}\nResult\n\n\n\"202-687-1587\"\n\\(\\varepsilon\\)\n202\n\\(\\varepsilon\\)\n-\n687-1587\nAccept\n\n\n\"(202) 687-1587\"\n(\n202\n)\n \n687-1587\nAccept\n\n\n\"2020687-1587\"\n\\(\\varepsilon\\)\n202\n\\(\\varepsilon\\)\n0\n687-1587\nReject"
  },
  {
    "objectID": "w05/index.html#building-and-testing-regex-strings",
    "href": "w05/index.html#building-and-testing-regex-strings",
    "title": "Week 5: Data Cleaning",
    "section": "Building and Testing RegEx Strings",
    "text": "Building and Testing RegEx Strings\n\nRegExr.com →"
  },
  {
    "objectID": "w05/index.html#tidy-data",
    "href": "w05/index.html#tidy-data",
    "title": "Week 5: Data Cleaning",
    "section": "Tidy Data",
    "text": "Tidy Data\n\n\n\nOverview | In-Depth\n\nEvery column is a variable\nEvery row is an observation\nEvery cell is a value\n\n\n\n\n\n\n\n\nVar1\nVar 2\n\n\n\n\nObs 1\nVal 1\nVal 2\n\n\nObs 2\nVal 3\nVal 4\n\n\n\nFigure 6: A template for tidy data, with observations in blue, variables in orange, and values in green\n\n\n\n\n\n\n\nlibrary(tidyverse)\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nFigure 7: An example panel dataset in tidy format\n\n\n\n\n\ntable2 %&gt;% head(6)\n\n# A tibble: 6 × 4\n  country      year type           count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;\n1 Afghanistan  1999 cases            745\n2 Afghanistan  1999 population  19987071\n3 Afghanistan  2000 cases           2666\n4 Afghanistan  2000 population  20595360\n5 Brazil       1999 cases          37737\n6 Brazil       1999 population 172006362\n\n\nFigure 8: An example panel dataset in non-tidy format"
  },
  {
    "objectID": "w05/index.html#how-do-we-get-our-data-into-tidy-form",
    "href": "w05/index.html#how-do-we-get-our-data-into-tidy-form",
    "title": "Week 5: Data Cleaning",
    "section": "How Do We Get Our Data Into Tidy Form?",
    "text": "How Do We Get Our Data Into Tidy Form?\n\nR: The Tidyverse\nPython: Pandas + Regular Expressions (lab demo!)"
  },
  {
    "objectID": "w05/index.html#the-tidyverse",
    "href": "w05/index.html#the-tidyverse",
    "title": "Week 5: Data Cleaning",
    "section": "The Tidyverse",
    "text": "The Tidyverse\n\n\nhttps://www.tidyverse.org/\n\n\n\ndplyr →\nggplot2 →\nforcats →\ntibble →\nreadr →\nstringr →\ntidyr →\npurrr →"
  },
  {
    "objectID": "w05/index.html#tibble",
    "href": "w05/index.html#tibble",
    "title": "Week 5: Data Cleaning",
    "section": "tibble",
    "text": "tibble\nHomepage | Overview | Cheatsheet\n\n\n\n\nReplaces R’s built-in data.frame objects, but retains syntax for backwards compatibility:\n\n\nlibrary(tibble)\ndata &lt;- c(3.4,1.1,9.6)\nlabels &lt;- c(0,1,0)\nsupervised_df &lt;- tibble(x=data, y=labels)\nsupervised_df\n\n# A tibble: 3 × 2\n      x     y\n  &lt;dbl&gt; &lt;dbl&gt;\n1   3.4     0\n2   1.1     1\n3   9.6     0\n\n\n\n\n\nProvides a surprisingly useful function: tribble() (tibble defined row-by-row)\n\n\nlibrary(tibble)\ndsan_df &lt;- tibble::tribble(\n    ~code, ~topic, ~credits,\n    \"dsan5000\", \"Data Science\", 3,\n    \"dsan5100\", \"Probabilistic Modeling\", 3\n)\ndsan_df\n\n# A tibble: 2 × 3\n  code     topic                  credits\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dbl&gt;\n1 dsan5000 Data Science                 3\n2 dsan5100 Probabilistic Modeling       3"
  },
  {
    "objectID": "w05/index.html#dplyr",
    "href": "w05/index.html#dplyr",
    "title": "Week 5: Data Cleaning",
    "section": "dplyr",
    "text": "dplyr\nHomepage | Overview | Cheatsheet\n\n\nGrammar of data manipulation (think verbs):\n\nfilter()\nselect()\narrange()\nmutate()\nsummarize()\n\n\nfilter():\n\n\nCode\ntable1 |&gt; filter(year == 2000)\n\n\n# A tibble: 3 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  2000   2666   20595360\n2 Brazil       2000  80488  174504898\n3 China        2000 213766 1280428583"
  },
  {
    "objectID": "w05/index.html#section",
    "href": "w05/index.html#section",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "select():\n\n\nCode\ntable1 |&gt; select(country)\n\n\n# A tibble: 6 × 1\n  country    \n  &lt;chr&gt;      \n1 Afghanistan\n2 Afghanistan\n3 Brazil     \n4 Brazil     \n5 China      \n6 China      \n\n\n\narrange():\n\n\nCode\ntable1 |&gt; arrange(population)\n\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583"
  },
  {
    "objectID": "w05/index.html#section-1",
    "href": "w05/index.html#section-1",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "mutate():\n\n\nCode\ntable1 |&gt; mutate(newvar = 300)\n\n\n# A tibble: 6 × 5\n  country      year  cases population newvar\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071    300\n2 Afghanistan  2000   2666   20595360    300\n3 Brazil       1999  37737  172006362    300\n4 Brazil       2000  80488  174504898    300\n5 China        1999 212258 1272915272    300\n6 China        2000 213766 1280428583    300"
  },
  {
    "objectID": "w05/index.html#section-2",
    "href": "w05/index.html#section-2",
    "title": "Week 5: Data Cleaning",
    "section": "",
    "text": "summarize():\n\n\nCode\ntable1 |&gt; \n  summarize(\n    avg_cases = mean(cases),\n    avg_pop = mean(population)\n  )\n\n\n# A tibble: 1 × 2\n  avg_cases    avg_pop\n      &lt;dbl&gt;      &lt;dbl&gt;\n1    91277. 490072924.\n\n\n\nsummarize() with grouping:\n\n\nCode\ntable1 |&gt;\n  group_by(country) |&gt;\n  summarize(\n    avg_cases = mean(cases),\n    avg_pop = mean(population)\n  )\n\n\n# A tibble: 3 × 3\n  country     avg_cases     avg_pop\n  &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n1 Afghanistan     1706.   20291216.\n2 Brazil         59112.  173255630 \n3 China         213012  1276671928."
  },
  {
    "objectID": "w05/index.html#the-rest-of-the-tidyverse",
    "href": "w05/index.html#the-rest-of-the-tidyverse",
    "title": "Week 5: Data Cleaning",
    "section": "The Rest of the Tidyverse",
    "text": "The Rest of the Tidyverse\n\nforcats\nreadr\nstringr\ntidyr\npurrr\nlubridate*"
  },
  {
    "objectID": "w05/index.html#forcats",
    "href": "w05/index.html#forcats",
    "title": "Week 5: Data Cleaning",
    "section": "forcats",
    "text": "forcats\nHomepage | Overview | Cheatsheet\n\nUtilities for working with factor variables (R’s data structure for categorical variables)\nfactors = data + levels:\n\n\n\n\n\nCode\nmonth_levels &lt;- c(\n    \"Jan\", \"Feb\", \"Mar\", \"Apr\",\n    \"May\", \"Jun\", \"Jul\", \"Aug\",\n    \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\nd &lt;- c(\"Jan\",\"Jan\",\"Feb\",\"Dec\")\nprint(d)\n\n\n[1] \"Jan\" \"Jan\" \"Feb\" \"Dec\"\n\n\n\n\nCode\ndataf &lt;- parse_factor(\n    d,\n    levels=month_levels\n)\nprint(dataf)\n\n\n[1] Jan Jan Feb Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec"
  },
  {
    "objectID": "w05/index.html#factors-for-ordering-plot-elements",
    "href": "w05/index.html#factors-for-ordering-plot-elements",
    "title": "Week 5: Data Cleaning",
    "section": "Factors for Ordering Plot Elements",
    "text": "Factors for Ordering Plot Elements\n\n\n\nrelig_summary &lt;- gss_cat %&gt;%\n  group_by(relig) %&gt;%\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\nrelig_labs &lt;- labs(\n  x = \"TV Hours / Day\",\n  y = \"Religion\"\n)\n\nWithout forcats:\n\n\nCode\nrelig_summary |&gt;\n  ggplot(aes(tvhours, relig)) +\n  geom_point(size=g_pointsize) +\n  geom_segment(aes(yend = relig, x=0, xend = tvhours)) +\n  dsan_theme(\"half\") +\n  relig_labs\n\n\n\n\n\n\nWith forcats:\n\n\nCode\nrelig_summary |&gt;\n  mutate(relig = fct_reorder(relig, tvhours)) |&gt;\n  ggplot(aes(x=tvhours, y=relig)) +\n    geom_point(size=g_pointsize) +\n    geom_segment(aes(yend = relig, x=0, xend = tvhours)) +\n    dsan_theme(\"half\") +\n    relig_labs"
  },
  {
    "objectID": "w05/index.html#sorting-barplots-with-fct_infreq",
    "href": "w05/index.html#sorting-barplots-with-fct_infreq",
    "title": "Week 5: Data Cleaning",
    "section": "Sorting Barplots with fct_infreq()",
    "text": "Sorting Barplots with fct_infreq()\n\nbarplot_labs &lt;- labs(\n  title = \"Respondents by Marital Status\",\n  x = \"Marital Status\",\n  y = \"Count\"\n)\n\n\n\nCode\ngss_cat |&gt;\n  mutate(marital = marital |&gt; fct_infreq() |&gt; fct_rev()) |&gt;\n  ggplot(aes(marital)) + geom_bar() + barplot_labs +\n  dsan_theme(\"full\")"
  },
  {
    "objectID": "w05/index.html#recodingcombining-categories",
    "href": "w05/index.html#recodingcombining-categories",
    "title": "Week 5: Data Cleaning",
    "section": "Recoding/Combining Categories",
    "text": "Recoding/Combining Categories\n\n\nAutomatically combining using fct_lump():\n\n\nCode\ngss_cat |&gt;\n    mutate(relig = fct_lump(relig)) |&gt;\n    count(relig)\n\n\n# A tibble: 2 × 2\n  relig          n\n  &lt;fct&gt;      &lt;int&gt;\n1 Protestant 10846\n2 Other      10637\n\n\n\nManually Combining using fct_recode():\n\n\nCode\ngss_cat |&gt;\n    mutate(partyid = fct_recode(partyid,\n    \"Republican\"  = \"Strong republican\",\n    \"Republican\"  = \"Not str republican\",\n    \"Independent\" = \"Ind,near rep\",\n    \"Independent\" = \"Ind,near dem\",\n    \"Democrat\"    = \"Not str democrat\",\n    \"Democrat\"    = \"Strong democrat\",\n    \"Other\"       = \"No answer\",\n    \"Other\"       = \"Don't know\",\n    \"Other\"       = \"Other party\"\n  )) |&gt;\n  count(partyid)\n\n\n# A tibble: 4 × 2\n  partyid         n\n  &lt;fct&gt;       &lt;int&gt;\n1 Other         548\n2 Republican   5346\n3 Independent  8409\n4 Democrat     7180"
  },
  {
    "objectID": "w05/index.html#readr",
    "href": "w05/index.html#readr",
    "title": "Week 5: Data Cleaning",
    "section": "readr",
    "text": "readr\nHomepage | Overview | Cheatsheet\n\nTwo key functions: read_csv(), write_csv()2\n\n\n\n\n\n\nWarning! read_csv() vs. read.csv()\n\n\n\nNote that these are not the same as R’s built-in read.csv() and write.csv()! The built-in R functions will produce a plain data.frame object, not a tibble\n\n\n\n\n\n\n\nCan handle URLs as well!\nShare data+code in seconds by using in combination with GitHub Gist\n(No more read_csv() path issues either…)\nGist dataset →\n\n\n\ngdp_df &lt;- read_csv(\"https://gist.githubusercontent.com/jpowerj/fecd437b96d0954893de727383f2eaf2/raw/fec58507f7095cb8341b229d6eb74ce53232d663/gdp_2010.csv\")\n\nRows: 204 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): name, code\ndbl (2): year, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ngdp_df |&gt; head(6)\n\n# A tibble: 6 × 4\n  name           code   year         value\n  &lt;chr&gt;          &lt;chr&gt; &lt;dbl&gt;         &lt;dbl&gt;\n1 Afghanistan    AFG    2010  15936800636.\n2 Albania        ALB    2010  11926953259.\n3 Algeria        DZA    2010 161207268655.\n4 American Samoa ASM    2010    576000000 \n5 Andorra        AND    2010   3355695364.\n6 Angola         AGO    2010  82470913121."
  },
  {
    "objectID": "w05/index.html#purrr-functional-programming",
    "href": "w05/index.html#purrr-functional-programming",
    "title": "Week 5: Data Cleaning",
    "section": "purrr: Functional Programming",
    "text": "purrr: Functional Programming\nHomepage | Overview | Cheatsheet | Tutorials, with Applications\n\n\nProvides an anonymous function operator ~, arguments get named .x, .y:\n\n\nCode\nmy_points &lt;- c(\"Midterm\"=18, \"Final\"=300)\ntotal_points &lt;- c(\"Midterm\"=20, \"Final\"=400)\n(avg_score &lt;- map2(my_points, total_points,\n  ~ list(frac=.x / .y, pct=(.x/.y)*100)))\n\n\n$Midterm\n$Midterm$frac\n[1] 0.9\n\n$Midterm$pct\n[1] 90\n\n\n$Final\n$Final$frac\n[1] 0.75\n\n$Final$pct\n[1] 75\n\n\n\nAlong with helpful functions for transforming the output\n\n\nCode\nlist_flatten(avg_score)\n\n\n$Midterm_frac\n[1] 0.9\n\n$Midterm_pct\n[1] 90\n\n$Final_frac\n[1] 0.75\n\n$Final_pct\n[1] 75\n\n\n\n\nCode\nevery(avg_score, ~ .x$frac &gt; 0.5)\n\n\n[1] TRUE"
  },
  {
    "objectID": "w05/index.html#lubridate",
    "href": "w05/index.html#lubridate",
    "title": "Week 5: Data Cleaning",
    "section": "lubridate*",
    "text": "lubridate*\nHomepage | Overview | Cheatsheet\n\n\n\n\n\n\nCaution: Importing lubridate\n\n\n\nNote that lubridate does not get loaded with the other packages in the tidyverse when you call library(tidyverse). It needs to be imported explicitly:\nlibrary(tidyverse)\nlibrary(lubridate)"
  },
  {
    "objectID": "w05/index.html#one-of-the-scariest-papers-of-all-time",
    "href": "w05/index.html#one-of-the-scariest-papers-of-all-time",
    "title": "Week 5: Data Cleaning",
    "section": "One of the Scariest Papers of All Time",
    "text": "One of the Scariest Papers of All Time\n\nText Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It (Denny and Spirling 2018) (PDF Link)\n\n\n\n\n\nFigure 9: A plot where \\(x\\)-axis represents UK party manifestos (arranged on left-right ideology dimension, \\(y\\)-axis slots represent an ordering of preprocessing steps, and a filled bar means the manifesto was placed incorrectly on the ideological dimension when that preprocessing combination was used.\n\n\n\n\n\n\nFigure 10: An ancient hieroglyph"
  },
  {
    "objectID": "w05/index.html#the-secret-behind-all-text-analysis",
    "href": "w05/index.html#the-secret-behind-all-text-analysis",
    "title": "Week 5: Data Cleaning",
    "section": "The Secret Behind All Text Analysis",
    "text": "The Secret Behind All Text Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Excerpts from two data science textbooks, plus another book\n\n\n\n\n\n\n\n\n\n\ndoc_id\ntext\ntexts\nKékkek\nvoice\n\n\n\n\n\n0\n0\n6\n0\n1\n\n\n\n1\n0\n0\n3\n1\n\n\n\n2\n6\n0\n0\n0\n\n\n\n\nFigure 12: The Document-Term Matrix (DTM)\n\n\n\n\n\n\n\n\n\n \n\n\n\ndoc_id\ntext\nkekkek\nvoice\n\n\n\n\n\n\n0\n6\n0\n1\n\n\n\n\n1\n0\n3\n1\n\n\n\n\n2\n6\n0\n0\n\n\n\n\n\nFigure 13: The cleaned DTM, after lowercasing, lemmatization, and unicode standardization"
  },
  {
    "objectID": "w05/index.html#lab-assignment-overview",
    "href": "w05/index.html#lab-assignment-overview",
    "title": "Week 5: Data Cleaning",
    "section": "Lab Assignment Overview",
    "text": "Lab Assignment Overview\nLab-2.2: Cleaning Record Data in R and Python"
  },
  {
    "objectID": "w05/index.html#references",
    "href": "w05/index.html#references",
    "title": "Week 5: Data Cleaning",
    "section": "References",
    "text": "References\n\n\nDenny, Matthew J., and Arthur Spirling. 2018. “Text Preprocessing For Unsupervised Learning: Why It Matters, When It Misleads, And What To Do About It.” Political Analysis 26 (2): 168–89. https://doi.org/10.1017/pan.2017.44.\n\n\nImmerwahr, Daniel. 2019. How to Hide an Empire: A History of the Greater United States. Farrar, Straus and Giroux."
  },
  {
    "objectID": "w05/index.html#footnotes",
    "href": "w05/index.html#footnotes",
    "title": "Week 5: Data Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo fully appreciate the importance of standards in the modern industrial/technology-driven world, see Chapter 18: “The Empire of the Red Octagon” (referring to the US’s octagonal stop sign), in Immerwahr (2019).↩︎\nPlus a bonus read_delim() if read_csv() fails↩︎"
  },
  {
    "objectID": "w03/slides.html#intranet-vs.-internet",
    "href": "w03/slides.html#intranet-vs.-internet",
    "title": "Week 3: Data Science Workflow",
    "section": "Intranet vs. Internet",
    "text": "Intranet vs. Internet\n\nCrucial distinction: can set up a “mini-internet”, an intranet, within your own home\nOrganizations (businesses, government agencies) with security needs often do exactly this: link a set of computers and servers together, no outside access\n\n\n\nInternet = basically a giant intranet, open to the whole world"
  },
  {
    "objectID": "w03/slides.html#key-building-blocks-locating-servers",
    "href": "w03/slides.html#key-building-blocks-locating-servers",
    "title": "Week 3: Data Science Workflow",
    "section": "Key Building Blocks: Locating Servers",
    "text": "Key Building Blocks: Locating Servers\n\nIP Addresses (Internet Protocol addresses): Numeric addresses for uniquely identifying computers on a network\n\nGeorgetown University, for example, is allocated IP addresses between 141.161.0.0 and 141.161.255.255\n\nURLs (Uniform Resource Locators): The more human-readable website addresses you’re used to: google.com, georgetown.edu, etc.\n\nBuilt on top of IP addresses, via a directory which maps URLs → IP addresses\ngeorgetown.edu, for example, is really 23.185.0.21\n\n\nTo see this, you can open your Terminal and run the ping command: ping georgetown.edu."
  },
  {
    "objectID": "w03/slides.html#what-happens-when-i-visit-a-urlip",
    "href": "w03/slides.html#what-happens-when-i-visit-a-urlip",
    "title": "Week 3: Data Science Workflow",
    "section": "What Happens When I Visit a URL/IP?",
    "text": "What Happens When I Visit a URL/IP?\n\nHTTP(S) (HyperText Transfer Protocol (Secure)): common syntax for web clients to make requests and servers to respond\n\nSeveral types of requests can be made: GET, POST, HEAD; for now, we focus on the GET request, the request your browser makes by default\n\nHTML (HyperText Markup Language): For specifying layout and content of page\n\nStructure is analogous to boxes of content: &lt;html&gt; box contains &lt;head&gt; (metadata, e.g., page title) and &lt;body&gt; (page content) boxes, &lt;body&gt; box contains e.g. header, footer, navigation bar, and main content of page.\nModern webpages also include CSS (Cascading Style Sheets) for styling this content, and Javascript1 for interactivity (changing/updating content)\nHTML allows linking to another page with a special anchor tag (&lt;a&gt;): &lt;a href=\"https://npr.org/\"&gt;news&lt;/a&gt; creates a link, so when you click “news”, browser will request (fetch the HTML for) the URL https://npr.org\n\n\nIncredibly, despite the name, Javascript has absolutely nothing to do with the Java programming language…"
  },
  {
    "objectID": "w03/slides.html#https-requests-in-action",
    "href": "w03/slides.html#https-requests-in-action",
    "title": "Week 3: Data Science Workflow",
    "section": "HTTP(S) Requests in Action",
    "text": "HTTP(S) Requests in Action\n\nImage from Menczer, Fortunato, and Davis (2020, 90)"
  },
  {
    "objectID": "w03/slides.html#how-does-a-web-server-work",
    "href": "w03/slides.html#how-does-a-web-server-work",
    "title": "Week 3: Data Science Workflow",
    "section": "How Does a Web Server Work?",
    "text": "How Does a Web Server Work?\n\nWe use the term “server” metonymously1\n\nSometimes we mean the hardware, the box of processors and hard drives\nBut, sometimes we mean the software that runs on the hardware\n\nA web server, in the software sense, is a program that is always running, 24/7\nWaits for requests (via HTTPS), then serves HTML code in response (also via HTTPS)\n\n\n\n\n\n\n\nhello_server.py\n\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n@app.route(\"/hack\")\ndef hacker_detected():\n    return \"&lt;p&gt;Hacker detected, pls stop&lt;/p&gt;\"\n\n$ flask --app hello_server run\n * Serving Flask app 'hello_server'\n * Running on http://127.0.0.1:5000 (CTRL+C to quit)\n127.0.0.1 [06/Sep/2023 00:11:05] \"GET / HTTP\" 200\n127.0.0.1 [06/Sep/2023 00:11:06] \"GET /hack HTTP\" 200\nFigure 3: Basic web server (written in Flask)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: [Browser-parsed] responses to GET requests\n\n\n\n\nSorry for jargon: it just means using the same word for different levels of a system (dangerous when talking computers!)"
  },
  {
    "objectID": "w03/slides.html#how-does-a-web-client-work",
    "href": "w03/slides.html#how-does-a-web-client-work",
    "title": "Week 3: Data Science Workflow",
    "section": "How Does a Web Client Work?",
    "text": "How Does a Web Client Work?\n\n\nOnce the server has responded to your request, you still only have raw HTML code\nSo, the browser is the program that renders this raw HTML code as a visual, (possibly) interactive webpage\nAs a data scientist, the most important thing to know is that different browsers can render the same HTML differently!\n\n\n\n\nA headache when pages are accessed through laptops\nA nightmare when pages are accessed through laptops and mobile"
  },
  {
    "objectID": "w03/slides.html#connecting-to-servers",
    "href": "w03/slides.html#connecting-to-servers",
    "title": "Week 3: Data Science Workflow",
    "section": "Connecting to Servers",
    "text": "Connecting to Servers\n\nWe’ve talked about the shell on your local computer, as well as the Georgetown Domains shell\nWe used Georgetown Domains’ web interface to access that shell, but you can remotely connect to any other shell from your local computer using the ssh command!"
  },
  {
    "objectID": "w03/slides.html#transferring-files-tofrom-servers",
    "href": "w03/slides.html#transferring-files-tofrom-servers",
    "title": "Week 3: Data Science Workflow",
    "section": "Transferring Files to/from Servers",
    "text": "Transferring Files to/from Servers\n\nRecall the copy command, cp, for files on your local computer\nThere is a remote equivalent, scp (Secure Copy Protocol), which you can use to copy files to/from remote servers to your local computer"
  },
  {
    "objectID": "w03/slides.html#important-alternative-rsync",
    "href": "w03/slides.html#important-alternative-rsync",
    "title": "Week 3: Data Science Workflow",
    "section": "Important Alternative: rsync",
    "text": "Important Alternative: rsync\n\nSimilar to scp, with same syntax, except it synchronizes (only copies files which are different or missing)\n\n\n\nsync_files.sh\n\nrsync -avz source_directory/ user@remote_server:/path/to/destination/\n\n\n-a (“archive”) tells rsync you want it to copy recursively\n-v (“verbose”) tells rsync to print information as it copies\n-z (“zip/compress”) tells rsync to compress files before copying and then decompress them on the server (thus massively speeding up the transfer)\nhttps://explainshell.com/explain?cmd=rsync+-avz"
  },
  {
    "objectID": "w03/slides.html#why-do-we-need-reproducible-research",
    "href": "w03/slides.html#why-do-we-need-reproducible-research",
    "title": "Week 3: Data Science Workflow",
    "section": "Why Do We Need Reproducible Research?",
    "text": "Why Do We Need Reproducible Research?\n\nMain human motivations (Max Weber): Wealth, Prestige, Power → “TED talk circuit”\n\n\nNew York Times Magazine, October 18, 2017."
  },
  {
    "objectID": "w03/slides.html#science-vs.-human-fallibility",
    "href": "w03/slides.html#science-vs.-human-fallibility",
    "title": "Week 3: Data Science Workflow",
    "section": "Science vs. Human Fallibility",
    "text": "Science vs. Human Fallibility\n\nScientific method + replicability/pre-registration = “Tying ourselves to the mast”\n\n\nJohn William Waterhouse, Ulysses and the Sirens, Public domain, via Wikimedia Commons\nIf we aim to disprove (!) our hypotheses, and we pre-register our methodology, we are bound to discovering truth, even when it is disadvantageous to our lives…"
  },
  {
    "objectID": "w03/slides.html#human-fallibility-is-winning",
    "href": "w03/slides.html#human-fallibility-is-winning",
    "title": "Week 3: Data Science Workflow",
    "section": "Human Fallibility is Winning…",
    "text": "Human Fallibility is Winning…\n\nMore than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature’s survey of 1,576 researchers (Baker 2016)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n\n[1] 0.9921178"
  },
  {
    "objectID": "w03/slides.html#r-vs.-rstudio-vs.-quarto",
    "href": "w03/slides.html#r-vs.-rstudio-vs.-quarto",
    "title": "Week 3: Data Science Workflow",
    "section": "R vs. RStudio vs. Quarto",
    "text": "R vs. RStudio vs. Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUI wrapper around R (Integrated Development Environment = IDE)\nRun blocks of R code (.qmd chunks)\n\n\n\nThe R Language \n\nProgramming language\nRuns scripts via Rscript &lt;script&gt;.r \n\n\n\n\n\n\n+\n\n\n\n\n\n\n\nGUI wrapper around Python (IDE)\nRun blocks of Python code (.ipynb cells)\n\n\n\n\nThe Python Language \n\nScripting language\nOn its own, just runs scripts via python &lt;script&gt;.py"
  },
  {
    "objectID": "w03/slides.html#reproducibility-and-literate-programming",
    "href": "w03/slides.html#reproducibility-and-literate-programming",
    "title": "Week 3: Data Science Workflow",
    "section": "Reproducibility and Literate Programming",
    "text": "Reproducibility and Literate Programming\n\nReproducible document: includes both the content (text, tables, figures) and the code or instructions required to generate that content.\n\nDesigned to ensure that others can reproduce the same document, including its data analysis, results, and visualizations, consistently and accurately.\ntldr: If you’re copying-and-pasting results from your code output to your results document, a red flag should go off in your head!\n\nLiterate programming is a coding and documentation approach where code and explanations of the code are combined in a single document.\n\nEmphasizes clear and understandable code by interleaving human-readable text (explanations, comments, and documentation) with executable code."
  },
  {
    "objectID": "w03/slides.html#single-source-many-outputs",
    "href": "w03/slides.html#single-source-many-outputs",
    "title": "Week 3: Data Science Workflow",
    "section": "Single Source, Many Outputs",
    "text": "Single Source, Many Outputs\n\nWe can create content (text, code, results, graphics) within a source document, and then use different weaving engines to create different document types:\n\n\n\n\nDocuments\n\nWeb pages (HTML)\nWord documents\nPDF files\n\nPresentations\n\nHTML\nPowerPoint\n\n\n\n\nWebsites/blogs\nBooks\nDashboards\nInteractive documents\nFormatted journal articles"
  },
  {
    "objectID": "w03/slides.html#interactivity",
    "href": "w03/slides.html#interactivity",
    "title": "Week 3: Data Science Workflow",
    "section": "Interactivity!",
    "text": "Interactivity!\n\nAre we “hiding something” by choosing a specific bin width? Make it transparent!"
  },
  {
    "objectID": "w03/slides.html#git-vs.-github",
    "href": "w03/slides.html#git-vs.-github",
    "title": "Week 3: Data Science Workflow",
    "section": "Git vs. GitHub",
    "text": "Git vs. GitHub\n(Important distinction!)\n\n\nGit \n\nCommand-line program\ngit init in shell to create\ngit add to track files\ngit commit to commit changes to tracked files\n\n\nGitHub \n\nCode hosting website\nCreate a repository (repo) for each project\nCan clone repos onto your local machine\n\n\n\n\ngit push/git pull: The link between the two!"
  },
  {
    "objectID": "w03/slides.html#git-diagram",
    "href": "w03/slides.html#git-diagram",
    "title": "Week 3: Data Science Workflow",
    "section": "Git Diagram",
    "text": "Git Diagram"
  },
  {
    "objectID": "w03/slides.html#initializing-a-repo",
    "href": "w03/slides.html#initializing-a-repo",
    "title": "Week 3: Data Science Workflow",
    "section": "Initializing a Repo",
    "text": "Initializing a Repo\n\nLet’s make a directory for our project called cool-project, and initialize a Git repo for it\n\n\nuser@hostname:~$ mkdir cool-project\nuser@hostname:~$ cd cool-project\nuser@hostname:~/cool-project$ git init\nInitialized empty Git repository in /home/user/cool-project/.git/\n\nThis creates a hidden folder, .git, in the directory:\n\n\nuser@hostname:~/cool-project$ ls -lah\ntotal 12K\ndrwxr-xr-x  3 user user 4.0K May 28 00:53 .\ndrwxr-xr-x 12 user user 4.0K May 28 00:53 ..\ndrwxr-xr-x  7 user user 4.0K May 28 00:53 .git\n\nThe Git Side: Local I"
  },
  {
    "objectID": "w03/slides.html#adding-and-committing-a-file",
    "href": "w03/slides.html#adding-and-committing-a-file",
    "title": "Week 3: Data Science Workflow",
    "section": "Adding and Committing a File",
    "text": "Adding and Committing a File\nWe’re writing Python code, so let’s create and track cool_code.py:\nuser@hostname:~/cool-project$ touch cool_code.py\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Initial version of cool_code.py\"\n[main (root-commit) b40dc25] Initial version of cool_code.py\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 cool_code.py"
  },
  {
    "objectID": "w03/slides.html#the-commit-log",
    "href": "w03/slides.html#the-commit-log",
    "title": "Week 3: Data Science Workflow",
    "section": "The Commit Log",
    "text": "The Commit Log\n\nView the commit log using git log:\n\nuser@hostname:~/cool-project$ git log\ncommit b40dc252a3b7355cc4c28397fefe7911ff3c94b9 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:57:16 2023 +0000\n\n    Initial version of cool_code.py\n\n\n\n\n\ngitGraph\n   commit id: \"b40dc25\""
  },
  {
    "objectID": "w03/slides.html#making-changes",
    "href": "w03/slides.html#making-changes",
    "title": "Week 3: Data Science Workflow",
    "section": "Making Changes",
    "text": "Making Changes\nuser@hostname:~/cool-project$ git status\nOn branch main\nnothing to commit, working tree clean\nuser@hostname:~/cool-project$ echo \"1 + 1\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ more cool_code.py\n1 + 1\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Added code to cool_code.py\"\n[main e3bc497] Added code to cool_code.py\n 1 file changed, 1 insertion(+)"
  },
  {
    "objectID": "w03/slides.html#section",
    "href": "w03/slides.html#section",
    "title": "Week 3: Data Science Workflow",
    "section": "",
    "text": "The git log will show the new version:\nuser@hostname:~/cool-project$ git log\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial version of cool_code.py"
  },
  {
    "objectID": "w03/slides.html#more-changes",
    "href": "w03/slides.html#more-changes",
    "title": "Week 3: Data Science Workflow",
    "section": "More Changes",
    "text": "More Changes\nuser@hostname:~/cool-project$ echo \"2 + 2\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ more cool_code.py\n1 + 1\n2 + 2\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Second version of cool_code.py\"\n[main 4007db9] Second version of cool_code.py\n 1 file changed, 1 insertion(+)"
  },
  {
    "objectID": "w03/slides.html#and-the-git-log",
    "href": "w03/slides.html#and-the-git-log",
    "title": "Week 3: Data Science Workflow",
    "section": "And the git log",
    "text": "And the git log\nuser@hostname:~/cool-project$ git log\ncommit 4007db9a031ca134fe09eab840b2bc845366a9c1 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:39:28 2023 +0000\n\n    Second version of cool_code.py\n\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py"
  },
  {
    "objectID": "w03/slides.html#undoing-a-commit-i",
    "href": "w03/slides.html#undoing-a-commit-i",
    "title": "Week 3: Data Science Workflow",
    "section": "Undoing a Commit I",
    "text": "Undoing a Commit I\nFirst check the git log to find the hash for the commit you want to revert back to:\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py"
  },
  {
    "objectID": "w03/slides.html#undoing-a-commit-ii",
    "href": "w03/slides.html#undoing-a-commit-ii",
    "title": "Week 3: Data Science Workflow",
    "section": "Undoing a Commit II",
    "text": "Undoing a Commit II\n\n This is irreversable! \n\nuser@hostname:~/cool-project$ git reset --hard e3bc497ac\nHEAD is now at e3bc497 Added code to cool_code.py\nuser@hostname:~/cool-project$ git log\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py"
  },
  {
    "objectID": "w03/slides.html#onwards-and-upwards",
    "href": "w03/slides.html#onwards-and-upwards",
    "title": "Week 3: Data Science Workflow",
    "section": "Onwards and Upwards",
    "text": "Onwards and Upwards\nuser@hostname:~/cool-project$ echo \"3 + 3\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Added different code to cool_code.py\"\n[main 700d955] Added different code to cool_code.py\n 1 file changed, 1 insertion(+)"
  },
  {
    "objectID": "w03/slides.html#section-1",
    "href": "w03/slides.html#section-1",
    "title": "Week 3: Data Science Workflow",
    "section": "",
    "text": "The final git log:\nuser@hostname:~/cool-project$ git log\ncommit 700d955faacb27d7b8bc464b9451851b5e319f20 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:44:49 2023 +0000\n\n    Added different code to cool_code.py\n\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py"
  },
  {
    "objectID": "w03/slides.html#but-why-these-diagrams",
    "href": "w03/slides.html#but-why-these-diagrams",
    "title": "Week 3: Data Science Workflow",
    "section": "But Why These Diagrams?",
    "text": "But Why These Diagrams?\nEven the simplest projects can start to look like:\n\n\n\n\n\ngitGraph\n       commit id: \"537dd67\"\n       commit id: \"6639143\"\n       branch nice_feature\n       checkout nice_feature\n       commit id: \"937ded8\"\n       checkout main\n       commit id: \"9e6679c\"\n       checkout nice_feature\n       branch very_nice_feature\n       checkout very_nice_feature\n       commit id: \"7f4de03\"\n       checkout main\n       commit id: \"6df80c1\"\n       checkout nice_feature\n       commit id: \"bd0ebb8\"\n       checkout main\n       merge nice_feature id: \"9ff61cc\" tag: \"V 1.0.0\" type: HIGHLIGHT\n       checkout very_nice_feature\n       commit id: \"370613b\"\n       checkout main\n       commit id: \"9a07a97\""
  },
  {
    "objectID": "w03/slides.html#the-github-side-remote",
    "href": "w03/slides.html#the-github-side-remote",
    "title": "Week 3: Data Science Workflow",
    "section": "The GitHub Side: Remote",
    "text": "The GitHub Side: Remote"
  },
  {
    "objectID": "w03/slides.html#an-empty-repo",
    "href": "w03/slides.html#an-empty-repo",
    "title": "Week 3: Data Science Workflow",
    "section": "An Empty Repo",
    "text": "An Empty Repo"
  },
  {
    "objectID": "w03/slides.html#refresh-after-git-push",
    "href": "w03/slides.html#refresh-after-git-push",
    "title": "Week 3: Data Science Workflow",
    "section": "Refresh after git push",
    "text": "Refresh after git push"
  },
  {
    "objectID": "w03/slides.html#commit-history",
    "href": "w03/slides.html#commit-history",
    "title": "Week 3: Data Science Workflow",
    "section": "Commit History",
    "text": "Commit History"
  },
  {
    "objectID": "w03/slides.html#checking-the-diff",
    "href": "w03/slides.html#checking-the-diff",
    "title": "Week 3: Data Science Workflow",
    "section": "Checking the diff",
    "text": "Checking the diff"
  },
  {
    "objectID": "w03/slides.html#web-development",
    "href": "w03/slides.html#web-development",
    "title": "Week 3: Data Science Workflow",
    "section": "Web Development",
    "text": "Web Development\n\n\n\n\n\n\n\n\n\nFrontend   \nBackend   \n\n\n\n\nLow Level\nHTML/CSS/JavaScript\nGitHub Pages\n\n\nMiddle Level\nJS Libraries\nPHP, SQL\n\n\nHigh Level\nReact, Next.js\nNode.js, Vercel\n\n\n\n\nFrontend icons: UI+UI elements, what the user sees (on the screen), user experience (UX), data visualization Backend icons: Databases, Security"
  },
  {
    "objectID": "w03/slides.html#getting-content-onto-the-internet",
    "href": "w03/slides.html#getting-content-onto-the-internet",
    "title": "Week 3: Data Science Workflow",
    "section": "Getting Content onto the Internet",
    "text": "Getting Content onto the Internet\n\n\n\n\nStep 1: index.html\n\n\nStep 2: Create GitHub repository\n\n\nStep 3: git init, git add -A ., git push\n\n\nStep 4: Enable GitHub Pages in repo settings\n\n\nStep 5: &lt;username&gt;.github.io!"
  },
  {
    "objectID": "w03/slides.html#deploying-from-a-branchfolder",
    "href": "w03/slides.html#deploying-from-a-branchfolder",
    "title": "Week 3: Data Science Workflow",
    "section": "Deploying from a Branch/Folder",
    "text": "Deploying from a Branch/Folder"
  },
  {
    "objectID": "w03/slides.html#lab-demonstration-1-transferring-files",
    "href": "w03/slides.html#lab-demonstration-1-transferring-files",
    "title": "Week 3: Data Science Workflow",
    "section": "Lab Demonstration 1: Transferring Files",
    "text": "Lab Demonstration 1: Transferring Files"
  },
  {
    "objectID": "w03/slides.html#lab-demonstration-2-quarto",
    "href": "w03/slides.html#lab-demonstration-2-quarto",
    "title": "Week 3: Data Science Workflow",
    "section": "Lab Demonstration 2: Quarto",
    "text": "Lab Demonstration 2: Quarto"
  },
  {
    "objectID": "w03/slides.html#lab-demonstration-3-git-and-github",
    "href": "w03/slides.html#lab-demonstration-3-git-and-github",
    "title": "Week 3: Data Science Workflow",
    "section": "Lab Demonstration 3: Git and GitHub",
    "text": "Lab Demonstration 3: Git and GitHub"
  },
  {
    "objectID": "w03/slides.html#assignment-overview",
    "href": "w03/slides.html#assignment-overview",
    "title": "Week 3: Data Science Workflow",
    "section": "Assignment Overview",
    "text": "Assignment Overview\n\nCreate a repo on your private GitHub account called 5000-lab-1.2\nClone the repo to your local machine with git clone\nCreate a blank Quarto website project, then use a .bib file to add citations\nAdd content to index.qmd\nAdd content to about.ipynb\nBuild a simple presentation in slides/slides.ipynb using the revealjs format\nRender the website using quarto render\nSync your changes to GitHub\nUse rsync or scp to copy the _site directory to your GU domains server (within ~/public_html)\nCreate a Zotero (or Mendeley) account, download the software, and add at least one reference to your site by syncing the .bib file"
  },
  {
    "objectID": "w03/slides.html#references",
    "href": "w03/slides.html#references",
    "title": "Week 3: Data Science Workflow",
    "section": "References",
    "text": "References\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nMenczer, Filippo, Santo Fortunato, and Clayton A. Davis. 2020. A First Course in Network Science. Cambridge University Press."
  },
  {
    "objectID": "w03/index.html",
    "href": "w03/index.html",
    "title": "Week 3: Data Science Workflow",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w03/index.html#intranet-vs.-internet",
    "href": "w03/index.html#intranet-vs.-internet",
    "title": "Week 3: Data Science Workflow",
    "section": "Intranet vs. Internet",
    "text": "Intranet vs. Internet\n\nCrucial distinction: can set up a “mini-internet”, an intranet, within your own home\nOrganizations (businesses, government agencies) with security needs often do exactly this: link a set of computers and servers together, no outside access\n\n\n\n\n\n\n\nInternet = basically a giant intranet, open to the whole world"
  },
  {
    "objectID": "w03/index.html#key-building-blocks-locating-servers",
    "href": "w03/index.html#key-building-blocks-locating-servers",
    "title": "Week 3: Data Science Workflow",
    "section": "Key Building Blocks: Locating Servers",
    "text": "Key Building Blocks: Locating Servers\n\nIP Addresses (Internet Protocol addresses): Numeric addresses for uniquely identifying computers on a network\n\nGeorgetown University, for example, is allocated IP addresses between 141.161.0.0 and 141.161.255.255\n\nURLs (Uniform Resource Locators): The more human-readable website addresses you’re used to: google.com, georgetown.edu, etc.\n\nBuilt on top of IP addresses, via a directory which maps URLs → IP addresses\ngeorgetown.edu, for example, is really 23.185.0.21"
  },
  {
    "objectID": "w03/index.html#what-happens-when-i-visit-a-urlip",
    "href": "w03/index.html#what-happens-when-i-visit-a-urlip",
    "title": "Week 3: Data Science Workflow",
    "section": "What Happens When I Visit a URL/IP?",
    "text": "What Happens When I Visit a URL/IP?\n\nHTTP(S) (HyperText Transfer Protocol (Secure)): common syntax for web clients to make requests and servers to respond\n\nSeveral types of requests can be made: GET, POST, HEAD; for now, we focus on the GET request, the request your browser makes by default\n\nHTML (HyperText Markup Language): For specifying layout and content of page\n\nStructure is analogous to boxes of content: &lt;html&gt; box contains &lt;head&gt; (metadata, e.g., page title) and &lt;body&gt; (page content) boxes, &lt;body&gt; box contains e.g. header, footer, navigation bar, and main content of page.\nModern webpages also include CSS (Cascading Style Sheets) for styling this content, and Javascript2 for interactivity (changing/updating content)\nHTML allows linking to another page with a special anchor tag (&lt;a&gt;): &lt;a href=\"https://npr.org/\"&gt;news&lt;/a&gt; creates a link, so when you click “news”, browser will request (fetch the HTML for) the URL https://npr.org"
  },
  {
    "objectID": "w03/index.html#https-requests-in-action",
    "href": "w03/index.html#https-requests-in-action",
    "title": "Week 3: Data Science Workflow",
    "section": "HTTP(S) Requests in Action",
    "text": "HTTP(S) Requests in Action\n\n\n\nImage from Menczer, Fortunato, and Davis (2020, 90)"
  },
  {
    "objectID": "w03/index.html#how-does-a-web-server-work",
    "href": "w03/index.html#how-does-a-web-server-work",
    "title": "Week 3: Data Science Workflow",
    "section": "How Does a Web Server Work?",
    "text": "How Does a Web Server Work?\n\nWe use the term “server” metonymously3\n\nSometimes we mean the hardware, the box of processors and hard drives\nBut, sometimes we mean the software that runs on the hardware\n\nA web server, in the software sense, is a program that is always running, 24/7\nWaits for requests (via HTTPS), then serves HTML code in response (also via HTTPS)\n\n\n\n\n\n\n\nhello_server.py\n\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n@app.route(\"/hack\")\ndef hacker_detected():\n    return \"&lt;p&gt;Hacker detected, pls stop&lt;/p&gt;\"\n\n$ flask --app hello_server run\n * Serving Flask app 'hello_server'\n * Running on http://127.0.0.1:5000 (CTRL+C to quit)\n127.0.0.1 [06/Sep/2023 00:11:05] \"GET / HTTP\" 200\n127.0.0.1 [06/Sep/2023 00:11:06] \"GET /hack HTTP\" 200\nFigure 3: Basic web server (written in Flask)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: [Browser-parsed] responses to GET requests"
  },
  {
    "objectID": "w03/index.html#how-does-a-web-client-work",
    "href": "w03/index.html#how-does-a-web-client-work",
    "title": "Week 3: Data Science Workflow",
    "section": "How Does a Web Client Work?",
    "text": "How Does a Web Client Work?\n\n\nOnce the server has responded to your request, you still only have raw HTML code\nSo, the browser is the program that renders this raw HTML code as a visual, (possibly) interactive webpage\nAs a data scientist, the most important thing to know is that different browsers can render the same HTML differently!\n\n\n\n\nA headache when pages are accessed through laptops\nA nightmare when pages are accessed through laptops and mobile"
  },
  {
    "objectID": "w03/index.html#connecting-to-servers",
    "href": "w03/index.html#connecting-to-servers",
    "title": "Week 3: Data Science Workflow",
    "section": "Connecting to Servers",
    "text": "Connecting to Servers\n\nWe’ve talked about the shell on your local computer, as well as the Georgetown Domains shell\nWe used Georgetown Domains’ web interface to access that shell, but you can remotely connect to any other shell from your local computer using the ssh command!"
  },
  {
    "objectID": "w03/index.html#transferring-files-tofrom-servers",
    "href": "w03/index.html#transferring-files-tofrom-servers",
    "title": "Week 3: Data Science Workflow",
    "section": "Transferring Files to/from Servers",
    "text": "Transferring Files to/from Servers\n\nRecall the copy command, cp, for files on your local computer\nThere is a remote equivalent, scp (Secure Copy Protocol), which you can use to copy files to/from remote servers to your local computer"
  },
  {
    "objectID": "w03/index.html#important-alternative-rsync",
    "href": "w03/index.html#important-alternative-rsync",
    "title": "Week 3: Data Science Workflow",
    "section": "Important Alternative: rsync",
    "text": "Important Alternative: rsync\n\nSimilar to scp, with same syntax, except it synchronizes (only copies files which are different or missing)\n\n\n\nsync_files.sh\n\nrsync -avz source_directory/ user@remote_server:/path/to/destination/\n\n\n-a (“archive”) tells rsync you want it to copy recursively\n-v (“verbose”) tells rsync to print information as it copies\n-z (“zip/compress”) tells rsync to compress files before copying and then decompress them on the server (thus massively speeding up the transfer)\nhttps://explainshell.com/explain?cmd=rsync+-avz"
  },
  {
    "objectID": "w03/index.html#why-do-we-need-reproducible-research",
    "href": "w03/index.html#why-do-we-need-reproducible-research",
    "title": "Week 3: Data Science Workflow",
    "section": "Why Do We Need Reproducible Research?",
    "text": "Why Do We Need Reproducible Research?\n\nMain human motivations (Max Weber): Wealth, Prestige, Power → “TED talk circuit”\n\n\n\n\nNew York Times Magazine, October 18, 2017."
  },
  {
    "objectID": "w03/index.html#science-vs.-human-fallibility",
    "href": "w03/index.html#science-vs.-human-fallibility",
    "title": "Week 3: Data Science Workflow",
    "section": "Science vs. Human Fallibility",
    "text": "Science vs. Human Fallibility\n\nScientific method + replicability/pre-registration = “Tying ourselves to the mast”\n\n\n\n\nJohn William Waterhouse, Ulysses and the Sirens, Public domain, via Wikimedia Commons\n\n\n\nIf we aim to disprove (!) our hypotheses, and we pre-register our methodology, we are bound to discovering truth, even when it is disadvantageous to our lives…"
  },
  {
    "objectID": "w03/index.html#human-fallibility-is-winning",
    "href": "w03/index.html#human-fallibility-is-winning",
    "title": "Week 3: Data Science Workflow",
    "section": "Human Fallibility is Winning…",
    "text": "Human Fallibility is Winning…\n\nMore than 70% of researchers have tried and failed to reproduce another scientist’s experiments, and more than half have failed to reproduce their own experiments. Those are some of the telling figures that emerged from Nature’s survey of 1,576 researchers (Baker 2016)\n\n\n\n\nsource(\"../_globals.r\")\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nga_lawyers &lt;- c(21362, 22254, 23134, 23698, 24367, 24930, 25632, 26459, 27227, 27457)\nski_df &lt;- tibble::tribble(\n  ~year, ~varname, ~value,\n  2000, \"ski_revenue\", 1551,\n  2001, \"ski_revenue\", 1635,\n  2002, \"ski_revenue\", 1801,\n  2003, \"ski_revenue\", 1827,\n  2004, \"ski_revenue\", 1956,\n  2005, \"ski_revenue\", 1989,\n  2006, \"ski_revenue\", 2178,\n  2007, \"ski_revenue\", 2257,\n  2008, \"ski_revenue\", 2476,\n  2009, \"ski_revenue\", 2438,\n)\nski_mean &lt;- mean(ski_df$value)\nski_sd &lt;- sd(ski_df$value)\nski_df &lt;- ski_df %&gt;% mutate(val_scaled = 12*value, val_norm = (value - ski_mean)/ski_sd)\nlaw_df &lt;- tibble::tibble(year=2000:2009, varname=\"ga_lawyers\", value=ga_lawyers)\nlaw_mean &lt;- mean(law_df$value)\nlaw_sd &lt;- sd(law_df$value)\nlaw_df &lt;- law_df %&gt;% mutate(val_norm = (value - law_mean)/law_sd)\nspur_df &lt;- dplyr::bind_rows(ski_df, law_df)\nggplot(spur_df, aes(x=year, y=val_norm, color=factor(varname, labels = c(\"Ski Revenue\",\"Lawyers in Georgia\")))) +\n  stat_smooth(method=\"loess\", se=FALSE) +\n  geom_point(size=g_pointsize/4) +\n  labs(\n    fill=\"\",\n    title=\"Ski Revenue vs. Georgia Lawyers\",\n    x=\"Year\",\n    color=\"Correlation: 99.2%\",\n    linetype=NULL\n  ) +\n  dsan_theme(\"custom\", 18) +\n  scale_x_continuous(\n    breaks=seq(from=2000, to=2014, by=2)\n  ) +\n  #scale_y_continuous(\n  #  name=\"Total Revenue, Ski Facilities (Million USD)\",\n  #  sec.axis = sec_axis(~ . * law_sd + law_mean, name = \"Number of Lawyers in Georgia\")\n  #) +\n  scale_y_continuous(breaks = -1:1,\n    labels = ~ . * round(ski_sd,1) + round(ski_mean,1),\n    name=\"Total Revenue, Ski Facilities (Million USD)\",\n    sec.axis = sec_axis(~ . * law_sd + law_mean, name = \"Number of Lawyers in Georgia\")) +\n  expand_limits(x=2010) +\n  #geom_hline(aes(yintercept=x, color=\"Mean Values\"), as.data.frame(list(x=0)), linewidth=0.75, alpha=1.0, show.legend = TRUE) +\n  scale_color_manual(\n    breaks=c('Ski Revenue', 'Lawyers in Georgia'),\n    values=c('Ski Revenue'=cbPalette[1], 'Lawyers in Georgia'=cbPalette[2]))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\ncor(ski_df$value, law_df$value)\n\n[1] 0.9921178"
  },
  {
    "objectID": "w03/index.html#r-vs.-rstudio-vs.-quarto",
    "href": "w03/index.html#r-vs.-rstudio-vs.-quarto",
    "title": "Week 3: Data Science Workflow",
    "section": "R vs. RStudio vs. Quarto",
    "text": "R vs. RStudio vs. Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUI wrapper around R (Integrated Development Environment = IDE)\nRun blocks of R code (.qmd chunks)\n\n\n\nThe R Language \n\nProgramming language\nRuns scripts via Rscript &lt;script&gt;.r \n\n\n\n\n\n\n+\n\n\n\n\n\n\n\nGUI wrapper around Python (IDE)\nRun blocks of Python code (.ipynb cells)\n\n\n\n\nThe Python Language \n\nScripting language\nOn its own, just runs scripts via python &lt;script&gt;.py"
  },
  {
    "objectID": "w03/index.html#reproducibility-and-literate-programming",
    "href": "w03/index.html#reproducibility-and-literate-programming",
    "title": "Week 3: Data Science Workflow",
    "section": "Reproducibility and Literate Programming",
    "text": "Reproducibility and Literate Programming\n\nReproducible document: includes both the content (text, tables, figures) and the code or instructions required to generate that content.\n\nDesigned to ensure that others can reproduce the same document, including its data analysis, results, and visualizations, consistently and accurately.\ntldr: If you’re copying-and-pasting results from your code output to your results document, a red flag should go off in your head!\n\nLiterate programming is a coding and documentation approach where code and explanations of the code are combined in a single document.\n\nEmphasizes clear and understandable code by interleaving human-readable text (explanations, comments, and documentation) with executable code."
  },
  {
    "objectID": "w03/index.html#single-source-many-outputs",
    "href": "w03/index.html#single-source-many-outputs",
    "title": "Week 3: Data Science Workflow",
    "section": "Single Source, Many Outputs",
    "text": "Single Source, Many Outputs\n\nWe can create content (text, code, results, graphics) within a source document, and then use different weaving engines to create different document types:\n\n\n\n\nDocuments\n\nWeb pages (HTML)\nWord documents\nPDF files\n\nPresentations\n\nHTML\nPowerPoint\n\n\n\n\nWebsites/blogs\nBooks\nDashboards\nInteractive documents\nFormatted journal articles"
  },
  {
    "objectID": "w03/index.html#interactivity",
    "href": "w03/index.html#interactivity",
    "title": "Week 3: Data Science Workflow",
    "section": "Interactivity!",
    "text": "Interactivity!\n\nAre we “hiding something” by choosing a specific bin width? Make it transparent!"
  },
  {
    "objectID": "w03/index.html#git-vs.-github",
    "href": "w03/index.html#git-vs.-github",
    "title": "Week 3: Data Science Workflow",
    "section": "Git vs. GitHub",
    "text": "Git vs. GitHub\n(Important distinction!)\n\n\nGit \n\nCommand-line program\ngit init in shell to create\ngit add to track files\ngit commit to commit changes to tracked files\n\n\nGitHub \n\nCode hosting website\nCreate a repository (repo) for each project\nCan clone repos onto your local machine\n\n\n\n\ngit push/git pull: The link between the two!"
  },
  {
    "objectID": "w03/index.html#git-diagram",
    "href": "w03/index.html#git-diagram",
    "title": "Week 3: Data Science Workflow",
    "section": "Git Diagram",
    "text": "Git Diagram"
  },
  {
    "objectID": "w03/index.html#initializing-a-repo",
    "href": "w03/index.html#initializing-a-repo",
    "title": "Week 3: Data Science Workflow",
    "section": "Initializing a Repo",
    "text": "Initializing a Repo\n\nLet’s make a directory for our project called cool-project, and initialize a Git repo for it\n\n\nuser@hostname:~$ mkdir cool-project\nuser@hostname:~$ cd cool-project\nuser@hostname:~/cool-project$ git init\nInitialized empty Git repository in /home/user/cool-project/.git/\n\nThis creates a hidden folder, .git, in the directory:\n\n\nuser@hostname:~/cool-project$ ls -lah\ntotal 12K\ndrwxr-xr-x  3 user user 4.0K May 28 00:53 .\ndrwxr-xr-x 12 user user 4.0K May 28 00:53 ..\ndrwxr-xr-x  7 user user 4.0K May 28 00:53 .git\n\nThe Git Side: Local I"
  },
  {
    "objectID": "w03/index.html#adding-and-committing-a-file",
    "href": "w03/index.html#adding-and-committing-a-file",
    "title": "Week 3: Data Science Workflow",
    "section": "Adding and Committing a File",
    "text": "Adding and Committing a File\nWe’re writing Python code, so let’s create and track cool_code.py:\nuser@hostname:~/cool-project$ touch cool_code.py\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Initial version of cool_code.py\"\n[main (root-commit) b40dc25] Initial version of cool_code.py\n 1 file changed, 0 insertions(+), 0 deletions(-)\n create mode 100644 cool_code.py"
  },
  {
    "objectID": "w03/index.html#the-commit-log",
    "href": "w03/index.html#the-commit-log",
    "title": "Week 3: Data Science Workflow",
    "section": "The Commit Log",
    "text": "The Commit Log\n\nView the commit log using git log:\n\nuser@hostname:~/cool-project$ git log\ncommit b40dc252a3b7355cc4c28397fefe7911ff3c94b9 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:57:16 2023 +0000\n\n    Initial version of cool_code.py\n\n\n\n\n\ngitGraph\n   commit id: \"b40dc25\""
  },
  {
    "objectID": "w03/index.html#making-changes",
    "href": "w03/index.html#making-changes",
    "title": "Week 3: Data Science Workflow",
    "section": "Making Changes",
    "text": "Making Changes\nuser@hostname:~/cool-project$ git status\nOn branch main\nnothing to commit, working tree clean\nuser@hostname:~/cool-project$ echo \"1 + 1\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ more cool_code.py\n1 + 1\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Added code to cool_code.py\"\n[main e3bc497] Added code to cool_code.py\n 1 file changed, 1 insertion(+)"
  },
  {
    "objectID": "w03/index.html#section",
    "href": "w03/index.html#section",
    "title": "Week 3: Data Science Workflow",
    "section": "",
    "text": "The git log will show the new version:\nuser@hostname:~/cool-project$ git log\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial version of cool_code.py"
  },
  {
    "objectID": "w03/index.html#more-changes",
    "href": "w03/index.html#more-changes",
    "title": "Week 3: Data Science Workflow",
    "section": "More Changes",
    "text": "More Changes\nuser@hostname:~/cool-project$ echo \"2 + 2\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ more cool_code.py\n1 + 1\n2 + 2\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Second version of cool_code.py\"\n[main 4007db9] Second version of cool_code.py\n 1 file changed, 1 insertion(+)"
  },
  {
    "objectID": "w03/index.html#and-the-git-log",
    "href": "w03/index.html#and-the-git-log",
    "title": "Week 3: Data Science Workflow",
    "section": "And the git log",
    "text": "And the git log\nuser@hostname:~/cool-project$ git log\ncommit 4007db9a031ca134fe09eab840b2bc845366a9c1 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:39:28 2023 +0000\n\n    Second version of cool_code.py\n\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py"
  },
  {
    "objectID": "w03/index.html#undoing-a-commit-i",
    "href": "w03/index.html#undoing-a-commit-i",
    "title": "Week 3: Data Science Workflow",
    "section": "Undoing a Commit I",
    "text": "Undoing a Commit I\nFirst check the git log to find the hash for the commit you want to revert back to:\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py"
  },
  {
    "objectID": "w03/index.html#undoing-a-commit-ii",
    "href": "w03/index.html#undoing-a-commit-ii",
    "title": "Week 3: Data Science Workflow",
    "section": "Undoing a Commit II",
    "text": "Undoing a Commit II\n\n This is irreversable! \n\nuser@hostname:~/cool-project$ git reset --hard e3bc497ac\nHEAD is now at e3bc497 Added code to cool_code.py\nuser@hostname:~/cool-project$ git log\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py"
  },
  {
    "objectID": "w03/index.html#onwards-and-upwards",
    "href": "w03/index.html#onwards-and-upwards",
    "title": "Week 3: Data Science Workflow",
    "section": "Onwards and Upwards",
    "text": "Onwards and Upwards\nuser@hostname:~/cool-project$ echo \"3 + 3\" &gt;&gt; cool_code.py\nuser@hostname:~/cool-project$ git add cool_code.py\nuser@hostname:~/cool-project$ git status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   cool_code.py\n\nuser@hostname:~/cool-project$ git commit -m \"Added different code to cool_code.py\"\n[main 700d955] Added different code to cool_code.py\n 1 file changed, 1 insertion(+)"
  },
  {
    "objectID": "w03/index.html#section-1",
    "href": "w03/index.html#section-1",
    "title": "Week 3: Data Science Workflow",
    "section": "",
    "text": "The final git log:\nuser@hostname:~/cool-project$ git log\ncommit 700d955faacb27d7b8bc464b9451851b5e319f20 (HEAD -&gt; main)\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:44:49 2023 +0000\n\n    Added different code to cool_code.py\n\ncommit e3bc497acbb5a487566ff2014dcd7b83d0c75224\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:38:05 2023 +0000\n\n    Added code to cool_code.py\n\ncommit b40dc25b14c0426b06c8d182184e147853f3c12e\nAuthor: Jeff Jacobs &lt;jjacobs3@cs.stanford.edu&gt;\nDate:   Sun May 28 00:37:02 2023 +0000\n\n    Initial (empty) version of cool_code.py"
  },
  {
    "objectID": "w03/index.html#but-why-these-diagrams",
    "href": "w03/index.html#but-why-these-diagrams",
    "title": "Week 3: Data Science Workflow",
    "section": "But Why These Diagrams?",
    "text": "But Why These Diagrams?\nEven the simplest projects can start to look like:\n\n\n\n\n\ngitGraph\n       commit id: \"537dd67\"\n       commit id: \"6639143\"\n       branch nice_feature\n       checkout nice_feature\n       commit id: \"937ded8\"\n       checkout main\n       commit id: \"9e6679c\"\n       checkout nice_feature\n       branch very_nice_feature\n       checkout very_nice_feature\n       commit id: \"7f4de03\"\n       checkout main\n       commit id: \"6df80c1\"\n       checkout nice_feature\n       commit id: \"bd0ebb8\"\n       checkout main\n       merge nice_feature id: \"9ff61cc\" tag: \"V 1.0.0\" type: HIGHLIGHT\n       checkout very_nice_feature\n       commit id: \"370613b\"\n       checkout main\n       commit id: \"9a07a97\""
  },
  {
    "objectID": "w03/index.html#the-github-side-remote",
    "href": "w03/index.html#the-github-side-remote",
    "title": "Week 3: Data Science Workflow",
    "section": "The GitHub Side: Remote",
    "text": "The GitHub Side: Remote"
  },
  {
    "objectID": "w03/index.html#an-empty-repo",
    "href": "w03/index.html#an-empty-repo",
    "title": "Week 3: Data Science Workflow",
    "section": "An Empty Repo",
    "text": "An Empty Repo"
  },
  {
    "objectID": "w03/index.html#refresh-after-git-push",
    "href": "w03/index.html#refresh-after-git-push",
    "title": "Week 3: Data Science Workflow",
    "section": "Refresh after git push",
    "text": "Refresh after git push"
  },
  {
    "objectID": "w03/index.html#commit-history",
    "href": "w03/index.html#commit-history",
    "title": "Week 3: Data Science Workflow",
    "section": "Commit History",
    "text": "Commit History"
  },
  {
    "objectID": "w03/index.html#checking-the-diff",
    "href": "w03/index.html#checking-the-diff",
    "title": "Week 3: Data Science Workflow",
    "section": "Checking the diff",
    "text": "Checking the diff"
  },
  {
    "objectID": "w03/index.html#web-development",
    "href": "w03/index.html#web-development",
    "title": "Week 3: Data Science Workflow",
    "section": "Web Development",
    "text": "Web Development\n\n\n\n\n\n\n\n\n\nFrontend   \nBackend   \n\n\n\n\nLow Level\nHTML/CSS/JavaScript\nGitHub Pages\n\n\nMiddle Level\nJS Libraries\nPHP, SQL\n\n\nHigh Level\nReact, Next.js\nNode.js, Vercel\n\n\n\n\nFrontend icons: UI+UI elements, what the user sees (on the screen), user experience (UX), data visualization Backend icons: Databases, Security"
  },
  {
    "objectID": "w03/index.html#getting-content-onto-the-internet",
    "href": "w03/index.html#getting-content-onto-the-internet",
    "title": "Week 3: Data Science Workflow",
    "section": "Getting Content onto the Internet",
    "text": "Getting Content onto the Internet\n\n\n\n\nStep 1: index.html\n\n\nStep 2: Create GitHub repository\n\n\nStep 3: git init, git add -A ., git push\n\n\nStep 4: Enable GitHub Pages in repo settings\n\n\nStep 5: &lt;username&gt;.github.io!"
  },
  {
    "objectID": "w03/index.html#deploying-from-a-branchfolder",
    "href": "w03/index.html#deploying-from-a-branchfolder",
    "title": "Week 3: Data Science Workflow",
    "section": "Deploying from a Branch/Folder",
    "text": "Deploying from a Branch/Folder"
  },
  {
    "objectID": "w03/index.html#lab-demonstration-1-transferring-files",
    "href": "w03/index.html#lab-demonstration-1-transferring-files",
    "title": "Week 3: Data Science Workflow",
    "section": "Lab Demonstration 1: Transferring Files",
    "text": "Lab Demonstration 1: Transferring Files"
  },
  {
    "objectID": "w03/index.html#lab-demonstration-2-quarto",
    "href": "w03/index.html#lab-demonstration-2-quarto",
    "title": "Week 3: Data Science Workflow",
    "section": "Lab Demonstration 2: Quarto",
    "text": "Lab Demonstration 2: Quarto"
  },
  {
    "objectID": "w03/index.html#lab-demonstration-3-git-and-github",
    "href": "w03/index.html#lab-demonstration-3-git-and-github",
    "title": "Week 3: Data Science Workflow",
    "section": "Lab Demonstration 3: Git and GitHub",
    "text": "Lab Demonstration 3: Git and GitHub"
  },
  {
    "objectID": "w03/index.html#assignment-overview",
    "href": "w03/index.html#assignment-overview",
    "title": "Week 3: Data Science Workflow",
    "section": "Assignment Overview",
    "text": "Assignment Overview\n\nCreate a repo on your private GitHub account called 5000-lab-1.2\nClone the repo to your local machine with git clone\nCreate a blank Quarto website project, then use a .bib file to add citations\nAdd content to index.qmd\nAdd content to about.ipynb\nBuild a simple presentation in slides/slides.ipynb using the revealjs format\nRender the website using quarto render\nSync your changes to GitHub\nUse rsync or scp to copy the _site directory to your GU domains server (within ~/public_html)\nCreate a Zotero (or Mendeley) account, download the software, and add at least one reference to your site by syncing the .bib file"
  },
  {
    "objectID": "w03/index.html#references",
    "href": "w03/index.html#references",
    "title": "Week 3: Data Science Workflow",
    "section": "References",
    "text": "References\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nMenczer, Filippo, Santo Fortunato, and Clayton A. Davis. 2020. A First Course in Network Science. Cambridge University Press."
  },
  {
    "objectID": "w03/index.html#footnotes",
    "href": "w03/index.html#footnotes",
    "title": "Week 3: Data Science Workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo see this, you can open your Terminal and run the ping command: ping georgetown.edu.↩︎\nIncredibly, despite the name, Javascript has absolutely nothing to do with the Java programming language…↩︎\nSorry for jargon: it just means using the same word for different levels of a system (dangerous when talking computers!)↩︎"
  },
  {
    "objectID": "w04/index.html",
    "href": "w04/index.html",
    "title": "Week 4: Data Gathering and APIs",
    "section": "",
    "text": "Open slides in new window →"
  },
  {
    "objectID": "w04/index.html#git-commands",
    "href": "w04/index.html#git-commands",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Git Commands",
    "text": "Git Commands\n\n\n\n\n\n\n\nCommand\nWhat It Does\n\n\n\n\ngit clone\nDownloads a repo from the web to our local computer\n\n\ngit init\nCreates a new, blank Git repository on our local computer (configuration/change-tracking stored in .git subfolder)\n\n\ngit add\nStages a file(s): Git will now track changes in this file(s)\n\n\ngit reset\nUndoes a git add\n\n\ngit status\nShows currently staged files and their status (created, modified, deleted)\n\n\ngit commit -m \"message\"\n“Saves” the current version of all staged files, ready to be pushed to a backup dir or remote server like GitHub\n\n\ngit push\nTransmits local commits to remote server\n\n\ngit pull\nDownloads commits from remote server to local computer\n\n\ngit merge\nMerges remote versions of files with local versions"
  },
  {
    "objectID": "w04/index.html#reproducible-docsliterate-programming",
    "href": "w04/index.html#reproducible-docsliterate-programming",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Reproducible Docs/Literate Programming",
    "text": "Reproducible Docs/Literate Programming\n\n1980s: \\(\\LaTeX\\) for \\(\\widehat{\\mathcal{T}}\\)ypesetting \\(\\sqrt{math}^2\\)\n1990s: Python and R as powerful scripting languages (no compilation required)\n2000s/2010s: Interactive Python via Jupyter, fancy IDE for R called RStudio\n2020s: Quarto (using pandoc under the hood) enables use of markdown for formatting, \\(\\LaTeX\\) for math, and both Python and R in same document, with choice of output formats (HTML, presentations, Word docs, …)"
  },
  {
    "objectID": "w04/index.html#preexisting-data-sources",
    "href": "w04/index.html#preexisting-data-sources",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Preexisting Data Sources",
    "text": "Preexisting Data Sources\n\nDepending on your field, or the type of data you’re looking for, there may be a “standard” data source! For example:\nEconomics:\n\nUS data: FRED\nGlobal data: World Bank Open Data, OECD Data, etc.\n\nPolitical Science:\n\nICPSR\n\nNetwork Science:\n\nStanford SNAP: Large Network Dataset Collection"
  },
  {
    "objectID": "w04/index.html#web-scraping",
    "href": "w04/index.html#web-scraping",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Web Scraping",
    "text": "Web Scraping\n\nFun fact: you can view a webpage’s HTML source code by right-clicking on the page and selecting “View Source”\n\nOn older websites, this means we can just request page and parse the returned HTML\n\nLess fun fact: modern web frameworks (React, Next.js) generate pages dynamically using JS, meaning that what you see on the page will not be visible in the HTML source\n\nData scraping still possible for these sites! Using browser automation tools like Selenium"
  },
  {
    "objectID": "w04/index.html#scraping-difficulty",
    "href": "w04/index.html#scraping-difficulty",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Scraping Difficulty",
    "text": "Scraping Difficulty\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow is data loaded?\nSolution\nExample\n\n\n\n\n😊\nEasy\nData in HTML source\n“View Source”\n\n\n\n😐\nMedium\nData loaded dynamically via API\n“View Source”, find API call, scrape programmatically\n\n\n\n😳\nHard\nData loaded dynamically [internally] via web framework\nUse Selenium"
  },
  {
    "objectID": "w04/index.html#data-structures-foundations",
    "href": "w04/index.html#data-structures-foundations",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Data Structures: Foundations",
    "text": "Data Structures: Foundations\n\nCould be (is) a whole class\nCould be (is) a whole class just for one type of data (geographic/spatial)\nFor this class: some foundational principles that should let you figure out fancier data structures you encounter"
  },
  {
    "objectID": "w04/index.html#opening-datasets-with-your-terminator-glasses-on",
    "href": "w04/index.html#opening-datasets-with-your-terminator-glasses-on",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Opening Datasets With Your Terminator Glasses On",
    "text": "Opening Datasets With Your Terminator Glasses On\n\n\n\n\nWhat does a row represent?\nWhat does a column represent?\nWhat does a value in a cell represent?\nAre there unique identifiers for the objects you care about?\n\n\n\n\n\n\nFigure 1: What you should see when you look at a new dataset"
  },
  {
    "objectID": "w04/index.html#from-raw-data-to-clean-data",
    "href": "w04/index.html#from-raw-data-to-clean-data",
    "title": "Week 4: Data Gathering and APIs",
    "section": "From Raw Data to Clean Data",
    "text": "From Raw Data to Clean Data"
  },
  {
    "objectID": "w04/index.html#data-structures-simple-rightarrow-complex",
    "href": "w04/index.html#data-structures-simple-rightarrow-complex",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Data Structures: Simple \\(\\rightarrow\\) Complex",
    "text": "Data Structures: Simple \\(\\rightarrow\\) Complex\n\n\n\n\n\n\n\nid\nname\nemail\n\n\n\n\n0\nK. Desbrow\nkd9@dailymail.com\n\n\n1\nD. Minall\ndminall1@wired.com\n\n\n2\nC. Knight\nck2@microsoft.com\n\n\n3\nM. McCaffrey\nmccaf4@nhs.uk\n\n\n\nFigure 2: Record Data\n\n\n\n\n\n\n\nyear\nmonth\npoints\n\n\n\n\n2023\nJan\n65\n\n\n2023\nFeb\n\n\n\n2023\nMar\n42\n\n\n2023\nApr\n11\n\n\n\nFigure 3: Time-Series Data\n\n\n\n\n\n\n\n\n\nid\ndate\nrating\nnum_rides\n\n\n\n\n0\n2023-01\n0.75\n45\n\n\n0\n2023-02\n0.89\n63\n\n\n0\n2023-03\n0.97\n7\n\n\n1\n2023-06\n0.07\n10\n\n\n\nFigure 4: Panel Data\n\n\n\n\n\n\n\nSource\nTarget\nWeight\n\n\n\n\nIGF2\nIGF1R\n1\n\n\nIGF1R\nTP53\n2\n\n\nTP53\nEGFR\n0.5\n\n\n\nFigure 5: Network Data\n\n\n\n\n\n\nFake data via Mockaroo and Random.org. Protein-protein interaction network from Agrawal, Zitnik, and Leskovec (2018)"
  },
  {
    "objectID": "w04/index.html#tabular-data-vs.-relational-data",
    "href": "w04/index.html#tabular-data-vs.-relational-data",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Tabular Data vs. Relational Data",
    "text": "Tabular Data vs. Relational Data\n\nAll of the datasets on the previous slide are tabular\nDatabases like SQLite, MySQL, require us to think about relationships within and between tabular datasets\nImagine you’re creating the backend for a social network. How would you record users and friendships? Your intuition may be record data:\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nfriends\n\n\n\n\n1\nPurna\n[2,3,4]\n\n\n2\nJeff\n[1,3,4,5,6]\n\n\n3\nJames\n[1,2,4,6]\n\n\n4\nNakul\n[1,2,3]\n\n\n5\nDr. Fauci\n[2,6]\n\n\n6\nPitbull\n[2,5]\n\n\n\nFigure 6: Our first attempt at a data structure for our social network app’s backend\n\n\n\nLong story short…\n\nThis doesn’t scale\nExtremely inefficient to find whether two users are friends\nRedundant information: Have to store friendship between A and B in both A’s row and B’s row"
  },
  {
    "objectID": "w04/index.html#a-better-approach",
    "href": "w04/index.html#a-better-approach",
    "title": "Week 4: Data Gathering and APIs",
    "section": "A Better Approach",
    "text": "A Better Approach\n\nMove the friendship data into its own table!\nThis table now represents relational data, (user table still corresponds to records):\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nname\n\n\n\n\n1\nPurna\n\n\n2\nJeff\n\n\n3\nJames\n\n\n4\nNakul\n\n\n5\nDr. Fauci\n\n\n6\nPitbull\n\n\n\nFigure 7: The user table in our relational structure\n\n\n\n\n\n\n\n\n\n\nid\nfriend_1\nfriend_2\nid\nfriend_1\nfriend_2\n\n\n\n\n1\n1\n2\n6\n2\n5\n\n\n2\n1\n3\n7\n2\n6\n\n\n3\n1\n4\n8\n3\n4\n\n\n4\n2\n3\n9\n3\n6\n\n\n5\n2\n4\n10\n5\n6\n\n\n\nFigure 8: The friendships table in our relational structure\n\n\n\n\n\nMay seem weird in terms of human readability, but think in terms of memory/computational efficiency: (a) Scalable, (b) Easy to find if two users are friends (via sorting/searching algorithms), (c) No redundant info"
  },
  {
    "objectID": "w04/index.html#dbs-relational-or-otherwise",
    "href": "w04/index.html#dbs-relational-or-otherwise",
    "title": "Week 4: Data Gathering and APIs",
    "section": "DBs: Relational or Otherwise",
    "text": "DBs: Relational or Otherwise\n\nFor rest of lecture we zoom in on cases where data comes as individual files\nBut on top of the relational format from previous slide, there are also non-relational database formats, like the document-based format used by e.g. MongoDB1\nIn either case, data is spread over many files, so that to obtain a single dataset we use queries.\n\n\n\n\n\n\ndigraph G {\n  rankdir=LR\n  file[label=\"File (.csv/.json/etc.)\"];\n  load[label=\"read_csv()\"];\n  dataset[label=\"Dataset\"];\n  file -&gt; load;\n  load -&gt; dataset;\n}\n\n\n\n\n\n\nG\n\n  \n\nfile\n\n File (.csv/.json/etc.)   \n\nload\n\n read_csv()   \n\nfile-&gt;load\n\n    \n\ndataset\n\n Dataset   \n\nload-&gt;dataset\n\n   \n\n\n\n\n\nFigure 9: Statically datasets (as individual files on disk)\n\n\n\n\n\ndigraph G {\n  rankdir=LR\n\n  subgraph cluster_00 {\n    label=\"Database\"\n    tab1[label=\"Table 1\"];\n    tab2[label=\"Table 2\"];\n    tabdots[label=\"...\", penwidth=0];\n    tabN[label=\"Table N\"];\n  }\n\n  query[label=\"Query\", style=\"dashed\"];\n  tab1 -&gt; query;\n  tab2 -&gt; query;\n  tabdots -&gt; query;\n  tabN -&gt; query\n\n  dataset[label=\"Dataset\"];\n\n  query -&gt; dataset;\n}\n\n\n\n\n\n\nG\n\n \n\ncluster_00\n\n Database   \n\ntab1\n\n Table 1   \n\nquery\n\n Query   \n\ntab1-&gt;query\n\n    \n\ntab2\n\n Table 2   \n\ntab2-&gt;query\n\n    \n\ntabdots\n\n …   \n\ntabdots-&gt;query\n\n    \n\ntabN\n\n Table N   \n\ntabN-&gt;query\n\n    \n\ndataset\n\n Dataset   \n\nquery-&gt;dataset\n\n   \n\n\n\n\n\nFigure 10: Datasets formed dynamically via database queries"
  },
  {
    "objectID": "w04/index.html#data-formats",
    "href": "w04/index.html#data-formats",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Data Formats",
    "text": "Data Formats\n\nThe most common formats, for most fields:\n\n.csv: Comma-Separated Values\n.tsv: Tab-Separated Values\n.json: JavaScript Object Notation\n.xls/.xlsx: Excel format\n.dta: Stata format"
  },
  {
    "objectID": "w04/index.html#csv-.tsv",
    "href": "w04/index.html#csv-.tsv",
    "title": "Week 4: Data Gathering and APIs",
    "section": ".csv / .tsv",
    "text": ".csv / .tsv\n\n\n👍\n\n\nmy_data.csv\n\nindex,var_1,var_2,var_3\nA,val_A1,val_A2,val_A3\nB,val_B1,val_B2,val_B3\nC,val_C1,val_C2,val_C3\nD,val_D1,val_D2,val_D3\n\n\n(👎)\n\n\nmy_data.tsv\n\nindex var_1 var_2 var_3\nA val_A1  val_A2  val_A3\nB val_B1  val_B2  val_B3\nC val_C1  val_C2  val_C3\nD val_D1  val_D2  val_D3\n\n\n\n→\n\n\n\nsource(\"../_globals.r\")\nlibrary(readr)\ndata &lt;- read_csv(\"assets/my_data.csv\")\n\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): index, var_1, var_2, var_3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndisp(data)\n\n\n\n\n\n# | index | var_1 | var_2 | var_3 |\n# | - | - | - | - |\n# | A | val_A1 | val_A2 | val_A3 |\n# | B | val_B1 | val_B2 | val_B3 |\n# | C | val_C1 | val_C2 | val_C3 |\n# | D | val_D1 | val_D2 | val_D3 | \n\n\n\n\n\nPython: pd.read_csv() (from Pandas library)\nR: read_csv() (from readr library)"
  },
  {
    "objectID": "w04/index.html#json",
    "href": "w04/index.html#json",
    "title": "Week 4: Data Gathering and APIs",
    "section": ".json",
    "text": ".json\n\n\n\ncourses.json\n\n{\n  \"dsan5000\": {\n    \"title\": \"Data Science and Analytics\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Tools and Workflow\"\n    ]\n  },\n  \"dsan5100\": {\n    \"title\": \"Probabilistic Modeling and Statistical Computing\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Conditional Probability\"\n    ]\n  }\n}\n\n\n\nPython: json (built-in library, import json)\nR: jsonlite (install.packages(jsonlite))\nHelpful validator (for when .json file won’t load)"
  },
  {
    "objectID": "w04/index.html#other-formats",
    "href": "w04/index.html#other-formats",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Other Formats",
    "text": "Other Formats\n\n.xls/.xlsx: Requires special libraries in Python/R\n\nPython: openpyxl\nR: readxl (part of tidyverse)\n\n.dta: Stata format, but can be read/written to in Python/R\n\nPython: Pandas has built-in pd.read_stata() and pd.to_stata()\nR: read_dta() from Haven library (part of tidyverse)"
  },
  {
    "objectID": "w04/index.html#scraping-html-with-requests-and-beautifulsoup",
    "href": "w04/index.html#scraping-html-with-requests-and-beautifulsoup",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Scraping HTML with requests and BeautifulSoup",
    "text": "Scraping HTML with requests and BeautifulSoup\nrequests Documentation | BeautifulSoup Documentation\n\n\nCode\n# Get HTML\nimport requests\n# Perform request\nresponse = requests.get(\"https://en.wikipedia.org/wiki/Data_science\")\n# Parse HTML\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.text, 'html.parser')\nall_headers = soup.find_all(\"h2\")\nsection_headers = [h.find(\"span\", {'class': 'mw-headline'}).text for h in all_headers[1:]]\nsection_headers\n\n\n['Foundations', 'Etymology', 'Data Science and Data Analysis', 'History', 'See also', 'References']"
  },
  {
    "objectID": "w04/index.html#navigating-html-with-beautifulsoup",
    "href": "w04/index.html#navigating-html-with-beautifulsoup",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Navigating HTML with BeautifulSoup",
    "text": "Navigating HTML with BeautifulSoup\n\nLet’s focus on this line from the previous slide:\n\nall_headers = soup.find_all(\"h2\")\n\nfind_all() is the key function for scraping!\nIf the HTML has a repeating structure (like rows in a table), find_all() can instantly parse this structure into a Python list."
  },
  {
    "objectID": "w04/index.html#the-power-of-find_all",
    "href": "w04/index.html#the-power-of-find_all",
    "title": "Week 4: Data Gathering and APIs",
    "section": "The Power of find_all()",
    "text": "The Power of find_all()\n\n\n\n\n\n\ndata_page.html\n\n&lt;div class=\"all-the-data\"&gt;\n    &lt;h4&gt;First Dataset&lt;/h4&gt;\n    &lt;div class=\"data-1\"&gt;\n        &lt;div class=\"dataval\"&gt;1&lt;/div&gt;\n        &lt;div class=\"dataval\"&gt;2&lt;/div&gt;\n        &lt;div class=\"dataval\"&gt;3&lt;/div&gt;\n    &lt;/div&gt;\n    &lt;h4&gt;Second Dataset&lt;/h4&gt;\n    &lt;div class=\"data-2\"&gt;\n        &lt;ul&gt;\n            &lt;li&gt;4.0&lt;/li&gt;\n            &lt;li&gt;5.5&lt;/li&gt;\n            &lt;li&gt;6.7&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\nFigure 11: Data in page elements (&lt;div&gt;, &lt;li&gt;)\n\n\n\n\n\n    First Dataset\n    \n        1\n        2\n        3\n    \n    Second Dataset\n        \n            4.0\n            5.5\n            6.7\n        \n\nFigure 12: The code from Figure 11, rendered by your browser\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(page_html, 'html.parser')\nds1_elt = soup.find(\"div\", class_='data-1')\nds1 = [e.text for e in ds1_elt.find_all(\"div\")]\nds2_elt = soup.find(\"div\", {'class': 'data-2'})\nds2 = [e.text for e in ds2_elt.find_all(\"li\")]\n\nFigure 13: The BeautifulSoup code used to parse the HTML\n\n\n\n\n\nprint(f\"dataset-1: {ds1}\\ndataset-2: {ds2}\")\n\ndataset-1: ['1', '2', '3']\ndataset-2: ['4.0', '5.5', '6.7']\n\n\nFigure 14: Contents of the Python variables holding the parsed data"
  },
  {
    "objectID": "w04/index.html#parsing-html-tables",
    "href": "w04/index.html#parsing-html-tables",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Parsing HTML Tables",
    "text": "Parsing HTML Tables\n\n\n\n\n\n\ntable_data.html\n\n&lt;table&gt;\n&lt;thead&gt;\n    &lt;tr&gt;\n        &lt;th&gt;X1&lt;/th&gt;&lt;th&gt;X2&lt;/th&gt;&lt;th&gt;X3&lt;/th&gt;\n    &lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n    &lt;tr&gt;\n        &lt;td&gt;1&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\nFigure 15: Data in HTML table format\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n1\n3\n5\n\n\n2\n4\n6\n\n\n\nFigure 16: The HTML table code, as rendered by your browser\n\n\n\n\n\n\n\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(table_html, 'html.parser')\nthead = soup.find(\"thead\")\nheaders = [e.text for e in thead.find_all(\"th\")]\ntbody = soup.find(\"tbody\")\nrows = tbody.find_all(\"tr\")\ndata = [[e.text for e in r.find_all(\"td\")]\n            for r in rows]\n\nFigure 17: The BeautifulSoup code used to parse the table HTML\n\n\n\n\n\nprint(f\"headers: {headers}\\ndata: {data}\")\n\nheaders: ['X1', 'X2', 'X3']\ndata: [['1', '3', '5'], ['2', '4', '6']]\n\n\nFigure 18: Contents of the Python variables holding the parsed data"
  },
  {
    "objectID": "w04/index.html#what-does-an-api-do",
    "href": "w04/index.html#what-does-an-api-do",
    "title": "Week 4: Data Gathering and APIs",
    "section": "What Does an API Do?",
    "text": "What Does an API Do?\nExposes endpoints for use by developers, without requiring them to know the nuts and bolts of your pipeline/service:\n\n\n\n\n\n\n\n\nExample\nEndpoint\nNot Exposed\n\n\n\n\nElectrical outlet\nSocket\nInternal wiring\n\n\nWater fountain\nAerator\nWater pump\n\n\nCar\nPedals, Steering wheel, etc.\nEngine\n\n\n\n\nWhen I’m teaching programming to students in refugee camps who may have never used a computer before, I try to use the idea of “robots”: a program is a robot trained to sit there and wait for inputs, then process them in some way and spit out some output. APIs really capture this notion, honestly."
  },
  {
    "objectID": "w04/index.html#example-math-api",
    "href": "w04/index.html#example-math-api",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Example: Math API",
    "text": "Example: Math API\n\nBase URL: https://newton.vercel.app/api/v2/\nThe endpoint: factor\nThe argument: \"x^2 - 1\"\nThe request: https://newton.vercel.app/api/v2/factor/x^2-1\n\n\n\nCode\nimport requests\nresponse = requests.get(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nprint(response.json())\n\n\n{'operation': 'factor', 'expression': 'x^2-1', 'result': '(x - 1) (x + 1)'}"
  },
  {
    "objectID": "w04/index.html#math-api-endpoints",
    "href": "w04/index.html#math-api-endpoints",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Math API Endpoints",
    "text": "Math API Endpoints\n\n\n\nOperation\nAPI Endpoint\nResult\n\n\n\n\nSimplify\n/simplify/2^2+2(2)\n8\n\n\nFactor\n/factor/x^2 + 2x\nx (x + 2)\n\n\nDerive\n/derive/x^2+2x\n2 x + 2\n\n\nIntegrate\n/integrate/x^2+2x\n1/3 x^3 + x^2 + C\n\n\nFind 0’s\n/zeroes/x^2+2x\n[-2, 0]\n\n\nFind Tangent\n/tangent/2|x^3\n12 x + -16\n\n\nArea Under Curve\n/area/2:4|x^3\n60\n\n\nCosine\n/cos/pi\n-1\n\n\nSine\n/sin/0\n0\n\n\nTangent\n/tan/0\n0"
  },
  {
    "objectID": "w04/index.html#authentication",
    "href": "w04/index.html#authentication",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication",
    "text": "Authentication\n\nUnlike the math API, most APIs do not allow requests to be made by anonymous requesters, and require authentication.\nFor example, you can access public GitHub repos anonymously, but to access private GitHub repos using GitHub’s API, you’ll need to authenticate that you are in fact the one making the request"
  },
  {
    "objectID": "w04/index.html#authentication-via-pygithub",
    "href": "w04/index.html#authentication-via-pygithub",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication via PyGithub",
    "text": "Authentication via PyGithub\n\n\n\n\n\n\nPyGithub Installation\n\n\n\nInstall using the following terminal/shell command [Documentation]\npip install PyGithub\n\n\nPyGithub can handle authentication for you. Example: this private repo in my account does not show up unless the request is authenticated (via a Personal Access Token)2:\n\n\n\n\n\n# import github\n# g = github.Github()\n# try:\n#   g.get_repo(\"jpowerj/private-repo-test\")\n# except Exception as e:\n#   print(e)\n\nFigure 19: Using the GitHub API without authentication\n\n\n\n\n\n# Load the access token securely\n# import os\n# my_access_token = os.getenv('GITHUB_TOKEN')\n# import github\n# # Use the access token to make an API request\n# auth = github.Auth.Token(my_access_token)\n# g = github.Github(auth=auth)\n# g.get_user().get_repo(\"private-repo-test\")\n\nFigure 20: Using the GitHub API with authentication"
  },
  {
    "objectID": "w04/index.html#references",
    "href": "w04/index.html#references",
    "title": "Week 4: Data Gathering and APIs",
    "section": "References",
    "text": "References\n\n\nAgrawal, Monica, Marinka Zitnik, and Jure Leskovec. 2018. “Large-Scale Analysis of Disease Pathways in the Human Interactome.” In PACIFIC SYMPOSIUM on BIOCOMPUTING 2018: Proceedings of the Pacific Symposium, 111–22. World Scientific.\n\n\nMenczer, Filippo, Santo Fortunato, and Clayton A. Davis. 2020. A First Course in Network Science. Cambridge University Press."
  },
  {
    "objectID": "w04/index.html#scraping-html-with-httr2-and-xml2",
    "href": "w04/index.html#scraping-html-with-httr2-and-xml2",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Scraping HTML with httr2 and xml2",
    "text": "Scraping HTML with httr2 and xml2\nhttr2 Documentation | xml2 Documentation\n\n\nCode\n# Get HTML\nlibrary(httr2)\nrequest_obj &lt;- request(\"https://en.wikipedia.org/wiki/Data_science\")\nresponse_obj &lt;- req_perform(request_obj)\n# Parse HTML\nlibrary(xml2)\n\n\n\nAttaching package: 'xml2'\n\n\nThe following object is masked from 'package:httr2':\n\n    url_parse\n\n\nCode\nhtml_obj &lt;- response_obj %&gt;% resp_body_html()\nhtml_obj %&gt;% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n\n\n{xml_nodeset (6)}\n[1] &lt;span class=\"mw-headline\" id=\"Foundations\"&gt;Foundations&lt;/span&gt;\n[2] &lt;span class=\"mw-headline\" id=\"Etymology\"&gt;Etymology&lt;/span&gt;\n[3] &lt;span class=\"mw-headline\" id=\"Data_Science_and_Data_Analysis\"&gt;Data Scienc ...\n[4] &lt;span class=\"mw-headline\" id=\"History\"&gt;History&lt;/span&gt;\n[5] &lt;span class=\"mw-headline\" id=\"See_also\"&gt;See also&lt;/span&gt;\n[6] &lt;span class=\"mw-headline\" id=\"References\"&gt;References&lt;/span&gt;\n\n\n\n\nNote: httr2 is a re-written version of the original httr package, which is now deprecated. You’ll still see lots of code using httr, however, so it’s good to know how both versions work. Click here for a helpful vignette on the original httr library."
  },
  {
    "objectID": "w04/index.html#navigating-html-with-xpath",
    "href": "w04/index.html#navigating-html-with-xpath",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Navigating HTML with XPath",
    "text": "Navigating HTML with XPath\nXPath Cheatsheet\n\nNotice the last line on the previous slide:\n\nhtml_obj %&gt;% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n\nThe string passed to xml_find_all() is an XPath selector\n\n\n\nXPath selectors are used by many different libraries, including Selenium (which we’ll look at very soon) and jQuery (a standard extension to plain JavaScript allowing easy searching/manipulation of the DOM), so it’s good to learn it now!"
  },
  {
    "objectID": "w04/index.html#xpath-i-selecting-elements",
    "href": "w04/index.html#xpath-i-selecting-elements",
    "title": "Week 4: Data Gathering and APIs",
    "section": "XPath I: Selecting Elements",
    "text": "XPath I: Selecting Elements\n\n\nmypage.html\n\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n\n\n'//div' matches all elements &lt;div&gt; in the document:\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n'//div//img' matches &lt;img&gt; elements which are children of &lt;div&gt; elements:\n&lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;"
  },
  {
    "objectID": "w04/index.html#xpath-ii-filtering-by-attributes",
    "href": "w04/index.html#xpath-ii-filtering-by-attributes",
    "title": "Week 4: Data Gathering and APIs",
    "section": "XPath II: Filtering by Attributes",
    "text": "XPath II: Filtering by Attributes\n\n\nmypage.html\n\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n\n\n'//p[id=\"page-content\"]' matches all &lt;p&gt; elements with id page-content3:\n&lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\nMatching classes is a bit trickier:\n'//img[contains(concat(\" \", normalize-space(@class), \" \"), \" footer-image \")]'\nmatches all &lt;img&gt; elements with page-content as one of their classes4\n&lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;"
  },
  {
    "objectID": "w04/index.html#example-math-api-1",
    "href": "w04/index.html#example-math-api-1",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Example: Math API",
    "text": "Example: Math API\n\nBase URL: https://newton.vercel.app/api/v2/\nThe endpoint: factor\nThe argument: \"x^2 - 1\"\nThe request: https://newton.vercel.app/api/v2/factor/x^2-1\n\n\n\nCode\nlibrary(httr2)\nrequest_obj &lt;- request(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nresponse_obj &lt;- req_perform(request_obj)\nwriteLines(response_obj %&gt;% resp_body_string())\n\n\n{\"operation\":\"factor\",\"expression\":\"x^2-1\",\"result\":\"(x - 1) (x + 1)\"}"
  },
  {
    "objectID": "w04/index.html#authentication-1",
    "href": "w04/index.html#authentication-1",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication",
    "text": "Authentication\n\nMost APIs don’t allow requests to be made by anonymous requesters, and require authentication.\nFor example, to access private GitHub repos using GitHub’s API, you’ll need to authenticate that you are in fact the one making the request"
  },
  {
    "objectID": "w04/index.html#authentication-via-gh",
    "href": "w04/index.html#authentication-via-gh",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication via GH",
    "text": "Authentication via GH\n\nThe GH library for R can handle this authentication process for you. For example, this private repo in my account does not show up if requested anonymously, but does show up when requested using GH with a Personal Access Token5:\n\n\n\nCode\nlibrary(gh)\nresult &lt;- gh(\"GET /repos/jpowerj/private-repo-test\")\nwriteLines(paste0(result$name, \": \",result$description))\n\n\nprivate-repo-test: Private repo example for DSAN5000"
  },
  {
    "objectID": "w04/index.html#footnotes",
    "href": "w04/index.html#footnotes",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor (much) more on this topic, see this page from Prisma, a high-level “wrapper” that auto-syncs your DB structure with a TypeScript schema, so your code knows exactly “what’s inside” a variable whose content was retrieved from the DB…↩︎\nYour code should 🚨never🚨 contain authentication info, especially when using GitHub. In this case, I created an OS environment variable called GITHUB_TOKEN containing my Personal Access Token, which I then loaded using os.getenv() and provided to PyGithub.↩︎\nIn HTML, ids are required to be unique to particular elements (and elements cannot have more than one id), meaning that this should only return a single element, for valid HTML code (not followed by all webpages!). Also note the double-quotes after id=, which are required in XPath.↩︎\nYour intuition may be to just use '//img[@class=\"footer-image\"]'. Sadly, however, this will match only elements with footer-image as their only class. i.e., it will match &lt;img class=\"footer-image\"&gt; but not &lt;img class=\"footer-image another-class\"&gt;. This will usually fail, since most elements on modern webpages have several classes. For example, if the site is using Bootstrap, &lt;p class=\"p-5 m-3\"&gt;&lt;/p&gt; creates a paragraph element with a padding of 5 pixels and a margin of 3 pixels.↩︎\nYour code should never contain authentication info, especially when using GitHub. In this case, I created an OS environment variable called GITHUB_TOKEN containing my Personal Access Token, which GH then uses to make authenticated requests.↩︎"
  },
  {
    "objectID": "w04/slides.html#git-commands",
    "href": "w04/slides.html#git-commands",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Git Commands",
    "text": "Git Commands\n\n\n\n\n\n\n\nCommand\nWhat It Does\n\n\n\n\ngit clone\nDownloads a repo from the web to our local computer\n\n\ngit init\nCreates a new, blank Git repository on our local computer (configuration/change-tracking stored in .git subfolder)\n\n\ngit add\nStages a file(s): Git will now track changes in this file(s)\n\n\ngit reset\nUndoes a git add\n\n\ngit status\nShows currently staged files and their status (created, modified, deleted)\n\n\ngit commit -m \"message\"\n“Saves” the current version of all staged files, ready to be pushed to a backup dir or remote server like GitHub\n\n\ngit push\nTransmits local commits to remote server\n\n\ngit pull\nDownloads commits from remote server to local computer\n\n\ngit merge\nMerges remote versions of files with local versions"
  },
  {
    "objectID": "w04/slides.html#reproducible-docsliterate-programming",
    "href": "w04/slides.html#reproducible-docsliterate-programming",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Reproducible Docs/Literate Programming",
    "text": "Reproducible Docs/Literate Programming\n\n1980s: \\(\\LaTeX\\) for \\(\\widehat{\\mathcal{T}}\\)ypesetting \\(\\sqrt{math}^2\\)\n1990s: Python and R as powerful scripting languages (no compilation required)\n2000s/2010s: Interactive Python via Jupyter, fancy IDE for R called RStudio\n2020s: Quarto (using pandoc under the hood) enables use of markdown for formatting, \\(\\LaTeX\\) for math, and both Python and R in same document, with choice of output formats (HTML, presentations, Word docs, …)"
  },
  {
    "objectID": "w04/slides.html#preexisting-data-sources",
    "href": "w04/slides.html#preexisting-data-sources",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Preexisting Data Sources",
    "text": "Preexisting Data Sources\n\nDepending on your field, or the type of data you’re looking for, there may be a “standard” data source! For example:\nEconomics:\n\nUS data: FRED\nGlobal data: World Bank Open Data, OECD Data, etc.\n\nPolitical Science:\n\nICPSR\n\nNetwork Science:\n\nStanford SNAP: Large Network Dataset Collection"
  },
  {
    "objectID": "w04/slides.html#web-scraping",
    "href": "w04/slides.html#web-scraping",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Web Scraping",
    "text": "Web Scraping\n\nFun fact: you can view a webpage’s HTML source code by right-clicking on the page and selecting “View Source”\n\nOn older websites, this means we can just request page and parse the returned HTML\n\nLess fun fact: modern web frameworks (React, Next.js) generate pages dynamically using JS, meaning that what you see on the page will not be visible in the HTML source\n\nData scraping still possible for these sites! Using browser automation tools like Selenium"
  },
  {
    "objectID": "w04/slides.html#scraping-difficulty",
    "href": "w04/slides.html#scraping-difficulty",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Scraping Difficulty",
    "text": "Scraping Difficulty\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow is data loaded?\nSolution\nExample\n\n\n\n\n😊\nEasy\nData in HTML source\n“View Source”\n\n\n\n😐\nMedium\nData loaded dynamically via API\n“View Source”, find API call, scrape programmatically\n\n\n\n😳\nHard\nData loaded dynamically [internally] via web framework\nUse Selenium"
  },
  {
    "objectID": "w04/slides.html#data-structures-foundations",
    "href": "w04/slides.html#data-structures-foundations",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Data Structures: Foundations",
    "text": "Data Structures: Foundations\n\nCould be (is) a whole class\nCould be (is) a whole class just for one type of data (geographic/spatial)\nFor this class: some foundational principles that should let you figure out fancier data structures you encounter"
  },
  {
    "objectID": "w04/slides.html#opening-datasets-with-your-terminator-glasses-on",
    "href": "w04/slides.html#opening-datasets-with-your-terminator-glasses-on",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Opening Datasets With Your Terminator Glasses On",
    "text": "Opening Datasets With Your Terminator Glasses On\n\n\n\n\nWhat does a row represent?\nWhat does a column represent?\nWhat does a value in a cell represent?\nAre there unique identifiers for the objects you care about?\n\n\n\n\n\n\nFigure 1: What you should see when you look at a new dataset"
  },
  {
    "objectID": "w04/slides.html#from-raw-data-to-clean-data",
    "href": "w04/slides.html#from-raw-data-to-clean-data",
    "title": "Week 4: Data Gathering and APIs",
    "section": "From Raw Data to Clean Data",
    "text": "From Raw Data to Clean Data"
  },
  {
    "objectID": "w04/slides.html#data-structures-simple-rightarrow-complex",
    "href": "w04/slides.html#data-structures-simple-rightarrow-complex",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Data Structures: Simple \\(\\rightarrow\\) Complex",
    "text": "Data Structures: Simple \\(\\rightarrow\\) Complex\n\n\n\n\n\n\n\nid\nname\nemail\n\n\n\n\n0\nK. Desbrow\nkd9@dailymail.com\n\n\n1\nD. Minall\ndminall1@wired.com\n\n\n2\nC. Knight\nck2@microsoft.com\n\n\n3\nM. McCaffrey\nmccaf4@nhs.uk\n\n\n\nFigure 2: Record Data\n\n\n\n\n\n\n\nyear\nmonth\npoints\n\n\n\n\n2023\nJan\n65\n\n\n2023\nFeb\n\n\n\n2023\nMar\n42\n\n\n2023\nApr\n11\n\n\n\nFigure 3: Time-Series Data\n\n\n\n\n\n\n\n\n\nid\ndate\nrating\nnum_rides\n\n\n\n\n0\n2023-01\n0.75\n45\n\n\n0\n2023-02\n0.89\n63\n\n\n0\n2023-03\n0.97\n7\n\n\n1\n2023-06\n0.07\n10\n\n\n\nFigure 4: Panel Data\n\n\n\n\n\n\n\nSource\nTarget\nWeight\n\n\n\n\nIGF2\nIGF1R\n1\n\n\nIGF1R\nTP53\n2\n\n\nTP53\nEGFR\n0.5\n\n\n\nFigure 5: Network Data\n\n\n\n\n\n\nFake data via Mockaroo and Random.org. Protein-protein interaction network from Agrawal, Zitnik, and Leskovec (2018)"
  },
  {
    "objectID": "w04/slides.html#tabular-data-vs.-relational-data",
    "href": "w04/slides.html#tabular-data-vs.-relational-data",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Tabular Data vs. Relational Data",
    "text": "Tabular Data vs. Relational Data\n\nAll of the datasets on the previous slide are tabular\nDatabases like SQLite, MySQL, require us to think about relationships within and between tabular datasets\nImagine you’re creating the backend for a social network. How would you record users and friendships? Your intuition may be record data:\n\n\n\n\n\n\n\n\n\n\n\nid\nname\nfriends\n\n\n\n\n1\nPurna\n[2,3,4]\n\n\n2\nJeff\n[1,3,4,5,6]\n\n\n3\nJames\n[1,2,4,6]\n\n\n4\nNakul\n[1,2,3]\n\n\n5\nDr. Fauci\n[2,6]\n\n\n6\nPitbull\n[2,5]\n\n\n\nFigure 6: Our first attempt at a data structure for our social network app’s backend\n\n\n\nLong story short…\n\nThis doesn’t scale\nExtremely inefficient to find whether two users are friends\nRedundant information: Have to store friendship between A and B in both A’s row and B’s row"
  },
  {
    "objectID": "w04/slides.html#a-better-approach",
    "href": "w04/slides.html#a-better-approach",
    "title": "Week 4: Data Gathering and APIs",
    "section": "A Better Approach",
    "text": "A Better Approach\n\nMove the friendship data into its own table!\nThis table now represents relational data, (user table still corresponds to records):\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nname\n\n\n\n\n1\nPurna\n\n\n2\nJeff\n\n\n3\nJames\n\n\n4\nNakul\n\n\n5\nDr. Fauci\n\n\n6\nPitbull\n\n\n\nFigure 7: The user table in our relational structure\n\n\n\n\n\n\n\n\n\n\nid\nfriend_1\nfriend_2\nid\nfriend_1\nfriend_2\n\n\n\n\n1\n1\n2\n6\n2\n5\n\n\n2\n1\n3\n7\n2\n6\n\n\n3\n1\n4\n8\n3\n4\n\n\n4\n2\n3\n9\n3\n6\n\n\n5\n2\n4\n10\n5\n6\n\n\n\nFigure 8: The friendships table in our relational structure\n\n\n\n\n\nMay seem weird in terms of human readability, but think in terms of memory/computational efficiency: (a) Scalable, (b) Easy to find if two users are friends (via sorting/searching algorithms), (c) No redundant info"
  },
  {
    "objectID": "w04/slides.html#dbs-relational-or-otherwise",
    "href": "w04/slides.html#dbs-relational-or-otherwise",
    "title": "Week 4: Data Gathering and APIs",
    "section": "DBs: Relational or Otherwise",
    "text": "DBs: Relational or Otherwise\n\nFor rest of lecture we zoom in on cases where data comes as individual files\nBut on top of the relational format from previous slide, there are also non-relational database formats, like the document-based format used by e.g. MongoDB1\nIn either case, data is spread over many files, so that to obtain a single dataset we use queries.\n\n\n\n\n\n\n\nCode\ndigraph G {\n  rankdir=LR\n  file[label=\"File (.csv/.json/etc.)\"];\n  load[label=\"read_csv()\"];\n  dataset[label=\"Dataset\"];\n  file -&gt; load;\n  load -&gt; dataset;\n}\n\n\n\n\n\n\n\nG\n\n  \n\nfile\n\n File (.csv/.json/etc.)   \n\nload\n\n read_csv()   \n\nfile-&gt;load\n\n    \n\ndataset\n\n Dataset   \n\nload-&gt;dataset\n\n   \n\n\n\n\n\nFigure 9: Statically datasets (as individual files on disk)\n\n\n\n\n\n\nCode\ndigraph G {\n  rankdir=LR\n\n  subgraph cluster_00 {\n    label=\"Database\"\n    tab1[label=\"Table 1\"];\n    tab2[label=\"Table 2\"];\n    tabdots[label=\"...\", penwidth=0];\n    tabN[label=\"Table N\"];\n  }\n\n  query[label=\"Query\", style=\"dashed\"];\n  tab1 -&gt; query;\n  tab2 -&gt; query;\n  tabdots -&gt; query;\n  tabN -&gt; query\n\n  dataset[label=\"Dataset\"];\n\n  query -&gt; dataset;\n}\n\n\n\n\n\n\n\nG\n\n \n\ncluster_00\n\n Database   \n\ntab1\n\n Table 1   \n\nquery\n\n Query   \n\ntab1-&gt;query\n\n    \n\ntab2\n\n Table 2   \n\ntab2-&gt;query\n\n    \n\ntabdots\n\n …   \n\ntabdots-&gt;query\n\n    \n\ntabN\n\n Table N   \n\ntabN-&gt;query\n\n    \n\ndataset\n\n Dataset   \n\nquery-&gt;dataset\n\n   \n\n\n\n\n\nFigure 10: Datasets formed dynamically via database queries\n\n\n\n\nFor (much) more on this topic, see this page from Prisma, a high-level “wrapper” that auto-syncs your DB structure with a TypeScript schema, so your code knows exactly “what’s inside” a variable whose content was retrieved from the DB…"
  },
  {
    "objectID": "w04/slides.html#data-formats",
    "href": "w04/slides.html#data-formats",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Data Formats",
    "text": "Data Formats\n\nThe most common formats, for most fields:\n\n.csv: Comma-Separated Values\n.tsv: Tab-Separated Values\n.json: JavaScript Object Notation\n.xls/.xlsx: Excel format\n.dta: Stata format"
  },
  {
    "objectID": "w04/slides.html#csv-.tsv",
    "href": "w04/slides.html#csv-.tsv",
    "title": "Week 4: Data Gathering and APIs",
    "section": ".csv / .tsv",
    "text": ".csv / .tsv\n\n\n👍\n\n\nmy_data.csv\n\nindex,var_1,var_2,var_3\nA,val_A1,val_A2,val_A3\nB,val_B1,val_B2,val_B3\nC,val_C1,val_C2,val_C3\nD,val_D1,val_D2,val_D3\n\n\n(👎)\n\n\nmy_data.tsv\n\nindex var_1 var_2 var_3\nA val_A1  val_A2  val_A3\nB val_B1  val_B2  val_B3\nC val_C1  val_C2  val_C3\nD val_D1  val_D2  val_D3\n\n\n\n→\n\n\n\n\nCode\nsource(\"../_globals.r\")\nlibrary(readr)\ndata &lt;- read_csv(\"assets/my_data.csv\")\ndisp(data)\n\n\n\n\n\n\n\nCode\n# | index | var_1 | var_2 | var_3 |\n# | - | - | - | - |\n# | A | val_A1 | val_A2 | val_A3 |\n# | B | val_B1 | val_B2 | val_B3 |\n# | C | val_C1 | val_C2 | val_C3 |\n# | D | val_D1 | val_D2 | val_D3 | \n\n\n\n\n\n\nPython: pd.read_csv() (from Pandas library)\nR: read_csv() (from readr library)"
  },
  {
    "objectID": "w04/slides.html#json",
    "href": "w04/slides.html#json",
    "title": "Week 4: Data Gathering and APIs",
    "section": ".json",
    "text": ".json\n\n\n\ncourses.json\n\n{\n  \"dsan5000\": {\n    \"title\": \"Data Science and Analytics\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Tools and Workflow\"\n    ]\n  },\n  \"dsan5100\": {\n    \"title\": \"Probabilistic Modeling and Statistical Computing\",\n    \"credits\": 3,\n    \"lectures\": [\n      \"Intro\",\n      \"Conditional Probability\"\n    ]\n  }\n}\n\n\n\nPython: json (built-in library, import json)\nR: jsonlite (install.packages(jsonlite))\nHelpful validator (for when .json file won’t load)"
  },
  {
    "objectID": "w04/slides.html#other-formats",
    "href": "w04/slides.html#other-formats",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Other Formats",
    "text": "Other Formats\n\n.xls/.xlsx: Requires special libraries in Python/R\n\nPython: openpyxl\nR: readxl (part of tidyverse)\n\n.dta: Stata format, but can be read/written to in Python/R\n\nPython: Pandas has built-in pd.read_stata() and pd.to_stata()\nR: read_dta() from Haven library (part of tidyverse)"
  },
  {
    "objectID": "w04/slides.html#scraping-html-with-requests-and-beautifulsoup",
    "href": "w04/slides.html#scraping-html-with-requests-and-beautifulsoup",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Scraping HTML with requests and BeautifulSoup",
    "text": "Scraping HTML with requests and BeautifulSoup\nrequests Documentation | BeautifulSoup Documentation\n\n\nCode\n# Get HTML\nimport requests\n# Perform request\nresponse = requests.get(\"https://en.wikipedia.org/wiki/Data_science\")\n# Parse HTML\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.text, 'html.parser')\nall_headers = soup.find_all(\"h2\")\nsection_headers = [h.find(\"span\", {'class': 'mw-headline'}).text for h in all_headers[1:]]\nsection_headers\n\n\n['Foundations', 'Etymology', 'Data Science and Data Analysis', 'History', 'See also', 'References']"
  },
  {
    "objectID": "w04/slides.html#navigating-html-with-beautifulsoup",
    "href": "w04/slides.html#navigating-html-with-beautifulsoup",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Navigating HTML with BeautifulSoup",
    "text": "Navigating HTML with BeautifulSoup\n\nLet’s focus on this line from the previous slide:\n\nall_headers = soup.find_all(\"h2\")\n\nfind_all() is the key function for scraping!\nIf the HTML has a repeating structure (like rows in a table), find_all() can instantly parse this structure into a Python list."
  },
  {
    "objectID": "w04/slides.html#the-power-of-find_all",
    "href": "w04/slides.html#the-power-of-find_all",
    "title": "Week 4: Data Gathering and APIs",
    "section": "The Power of find_all()",
    "text": "The Power of find_all()\n\n\n\n\n\n\ndata_page.html\n\n&lt;div class=\"all-the-data\"&gt;\n    &lt;h4&gt;First Dataset&lt;/h4&gt;\n    &lt;div class=\"data-1\"&gt;\n        &lt;div class=\"dataval\"&gt;1&lt;/div&gt;\n        &lt;div class=\"dataval\"&gt;2&lt;/div&gt;\n        &lt;div class=\"dataval\"&gt;3&lt;/div&gt;\n    &lt;/div&gt;\n    &lt;h4&gt;Second Dataset&lt;/h4&gt;\n    &lt;div class=\"data-2\"&gt;\n        &lt;ul&gt;\n            &lt;li&gt;4.0&lt;/li&gt;\n            &lt;li&gt;5.5&lt;/li&gt;\n            &lt;li&gt;6.7&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\nFigure 11: Data in page elements (&lt;div&gt;, &lt;li&gt;)\n\n\n\n\n\n    First Dataset\n    \n        1\n        2\n        3\n    \n    Second Dataset\n        \n            4.0\n            5.5\n            6.7\n        \n\nFigure 12: The code from Figure 11, rendered by your browser\n\n\n\n\n\n\n\n\nCode\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(page_html, 'html.parser')\nds1_elt = soup.find(\"div\", class_='data-1')\nds1 = [e.text for e in ds1_elt.find_all(\"div\")]\nds2_elt = soup.find(\"div\", {'class': 'data-2'})\nds2 = [e.text for e in ds2_elt.find_all(\"li\")]\n\n\nFigure 13: The BeautifulSoup code used to parse the HTML\n\n\n\n\n\n\nCode\nprint(f\"dataset-1: {ds1}\\ndataset-2: {ds2}\")\n\n\ndataset-1: ['1', '2', '3']\ndataset-2: ['4.0', '5.5', '6.7']\n\n\nFigure 14: Contents of the Python variables holding the parsed data"
  },
  {
    "objectID": "w04/slides.html#parsing-html-tables",
    "href": "w04/slides.html#parsing-html-tables",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Parsing HTML Tables",
    "text": "Parsing HTML Tables\n\n\n\n\n\n\ntable_data.html\n\n&lt;table&gt;\n&lt;thead&gt;\n    &lt;tr&gt;\n        &lt;th&gt;X1&lt;/th&gt;&lt;th&gt;X2&lt;/th&gt;&lt;th&gt;X3&lt;/th&gt;\n    &lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n    &lt;tr&gt;\n        &lt;td&gt;1&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;2&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\nFigure 15: Data in HTML table format\n\n\n\n\n\n\n\nX1\nX2\nX3\n\n\n\n\n1\n3\n5\n\n\n2\n4\n6\n\n\n\nFigure 16: The HTML table code, as rendered by your browser\n\n\n\n\n\n\n\n\nCode\nfrom bs4 import BeautifulSoup\nsoup = BeautifulSoup(table_html, 'html.parser')\nthead = soup.find(\"thead\")\nheaders = [e.text for e in thead.find_all(\"th\")]\ntbody = soup.find(\"tbody\")\nrows = tbody.find_all(\"tr\")\ndata = [[e.text for e in r.find_all(\"td\")]\n            for r in rows]\n\n\nFigure 17: The BeautifulSoup code used to parse the table HTML\n\n\n\n\n\n\nCode\nprint(f\"headers: {headers}\\ndata: {data}\")\n\n\nheaders: ['X1', 'X2', 'X3']\ndata: [['1', '3', '5'], ['2', '4', '6']]\n\n\nFigure 18: Contents of the Python variables holding the parsed data"
  },
  {
    "objectID": "w04/slides.html#what-does-an-api-do",
    "href": "w04/slides.html#what-does-an-api-do",
    "title": "Week 4: Data Gathering and APIs",
    "section": "What Does an API Do?",
    "text": "What Does an API Do?\nExposes endpoints for use by developers, without requiring them to know the nuts and bolts of your pipeline/service:\n\n\n\n\n\n\n\n\nExample\nEndpoint\nNot Exposed\n\n\n\n\nElectrical outlet\nSocket\nInternal wiring\n\n\nWater fountain\nAerator\nWater pump\n\n\nCar\nPedals, Steering wheel, etc.\nEngine\n\n\n\n\nWhen I’m teaching programming to students in refugee camps who may have never used a computer before, I try to use the idea of “robots”: a program is a robot trained to sit there and wait for inputs, then process them in some way and spit out some output. APIs really capture this notion, honestly."
  },
  {
    "objectID": "w04/slides.html#example-math-api",
    "href": "w04/slides.html#example-math-api",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Example: Math API",
    "text": "Example: Math API\n\nBase URL: https://newton.vercel.app/api/v2/\nThe endpoint: factor\nThe argument: \"x^2 - 1\"\nThe request: https://newton.vercel.app/api/v2/factor/x^2-1\n\n\n\nCode\nimport requests\nresponse = requests.get(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nprint(response.json())\n\n\n{'operation': 'factor', 'expression': 'x^2-1', 'result': '(x - 1) (x + 1)'}"
  },
  {
    "objectID": "w04/slides.html#math-api-endpoints",
    "href": "w04/slides.html#math-api-endpoints",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Math API Endpoints",
    "text": "Math API Endpoints\n\n\n\nOperation\nAPI Endpoint\nResult\n\n\n\n\nSimplify\n/simplify/2^2+2(2)\n8\n\n\nFactor\n/factor/x^2 + 2x\nx (x + 2)\n\n\nDerive\n/derive/x^2+2x\n2 x + 2\n\n\nIntegrate\n/integrate/x^2+2x\n1/3 x^3 + x^2 + C\n\n\nFind 0’s\n/zeroes/x^2+2x\n[-2, 0]\n\n\nFind Tangent\n/tangent/2|x^3\n12 x + -16\n\n\nArea Under Curve\n/area/2:4|x^3\n60\n\n\nCosine\n/cos/pi\n-1\n\n\nSine\n/sin/0\n0\n\n\nTangent\n/tan/0\n0"
  },
  {
    "objectID": "w04/slides.html#authentication",
    "href": "w04/slides.html#authentication",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication",
    "text": "Authentication\n\nUnlike the math API, most APIs do not allow requests to be made by anonymous requesters, and require authentication.\nFor example, you can access public GitHub repos anonymously, but to access private GitHub repos using GitHub’s API, you’ll need to authenticate that you are in fact the one making the request"
  },
  {
    "objectID": "w04/slides.html#authentication-via-pygithub",
    "href": "w04/slides.html#authentication-via-pygithub",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication via PyGithub",
    "text": "Authentication via PyGithub\n\n\n\nPyGithub Installation\n\n\nInstall using the following terminal/shell command [Documentation]\npip install PyGithub\n\n\n\nPyGithub can handle authentication for you. Example: this private repo in my account does not show up unless the request is authenticated (via a Personal Access Token)1:\n\n\n\n\n\n\nCode\n# import github\n# g = github.Github()\n# try:\n#   g.get_repo(\"jpowerj/private-repo-test\")\n# except Exception as e:\n#   print(e)\n\n\nFigure 19: Using the GitHub API without authentication\n\n\n\n\n\n\nCode\n# Load the access token securely\n# import os\n# my_access_token = os.getenv('GITHUB_TOKEN')\n# import github\n# # Use the access token to make an API request\n# auth = github.Auth.Token(my_access_token)\n# g = github.Github(auth=auth)\n# g.get_user().get_repo(\"private-repo-test\")\n\n\nFigure 20: Using the GitHub API with authentication\n\n\n\n\nYour code should 🚨never🚨 contain authentication info, especially when using GitHub. In this case, I created an OS environment variable called GITHUB_TOKEN containing my Personal Access Token, which I then loaded using os.getenv() and provided to PyGithub."
  },
  {
    "objectID": "w04/slides.html#references",
    "href": "w04/slides.html#references",
    "title": "Week 4: Data Gathering and APIs",
    "section": "References",
    "text": "References\n\n\nAgrawal, Monica, Marinka Zitnik, and Jure Leskovec. 2018. “Large-Scale Analysis of Disease Pathways in the Human Interactome.” In PACIFIC SYMPOSIUM on BIOCOMPUTING 2018: Proceedings of the Pacific Symposium, 111–22. World Scientific.\n\n\nMenczer, Filippo, Santo Fortunato, and Clayton A. Davis. 2020. A First Course in Network Science. Cambridge University Press."
  },
  {
    "objectID": "w04/slides.html#scraping-html-with-httr2-and-xml2",
    "href": "w04/slides.html#scraping-html-with-httr2-and-xml2",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Scraping HTML with httr2 and xml2",
    "text": "Scraping HTML with httr2 and xml2\nhttr2 Documentation | xml2 Documentation\n\n\nCode\n# Get HTML\nlibrary(httr2)\nrequest_obj &lt;- request(\"https://en.wikipedia.org/wiki/Data_science\")\nresponse_obj &lt;- req_perform(request_obj)\n# Parse HTML\nlibrary(xml2)\nhtml_obj &lt;- response_obj %&gt;% resp_body_html()\nhtml_obj %&gt;% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n\n\n{xml_nodeset (6)}\n[1] &lt;span class=\"mw-headline\" id=\"Foundations\"&gt;Foundations&lt;/span&gt;\n[2] &lt;span class=\"mw-headline\" id=\"Etymology\"&gt;Etymology&lt;/span&gt;\n[3] &lt;span class=\"mw-headline\" id=\"Data_Science_and_Data_Analysis\"&gt;Data Scienc ...\n[4] &lt;span class=\"mw-headline\" id=\"History\"&gt;History&lt;/span&gt;\n[5] &lt;span class=\"mw-headline\" id=\"See_also\"&gt;See also&lt;/span&gt;\n[6] &lt;span class=\"mw-headline\" id=\"References\"&gt;References&lt;/span&gt;\n\n\n\n\nNote: httr2 is a re-written version of the original httr package, which is now deprecated. You’ll still see lots of code using httr, however, so it’s good to know how both versions work. Click here for a helpful vignette on the original httr library."
  },
  {
    "objectID": "w04/slides.html#navigating-html-with-xpath",
    "href": "w04/slides.html#navigating-html-with-xpath",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Navigating HTML with XPath",
    "text": "Navigating HTML with XPath\nXPath Cheatsheet\n\nNotice the last line on the previous slide:\n\nhtml_obj %&gt;% xml_find_all('//h2//span[@class=\"mw-headline\"]')\n\nThe string passed to xml_find_all() is an XPath selector\n\n\n\nXPath selectors are used by many different libraries, including Selenium (which we’ll look at very soon) and jQuery (a standard extension to plain JavaScript allowing easy searching/manipulation of the DOM), so it’s good to learn it now!"
  },
  {
    "objectID": "w04/slides.html#xpath-i-selecting-elements",
    "href": "w04/slides.html#xpath-i-selecting-elements",
    "title": "Week 4: Data Gathering and APIs",
    "section": "XPath I: Selecting Elements",
    "text": "XPath I: Selecting Elements\n\n\nmypage.html\n\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n\n\n'//div' matches all elements &lt;div&gt; in the document:\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n'//div//img' matches &lt;img&gt; elements which are children of &lt;div&gt; elements:\n&lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;"
  },
  {
    "objectID": "w04/slides.html#xpath-ii-filtering-by-attributes",
    "href": "w04/slides.html#xpath-ii-filtering-by-attributes",
    "title": "Week 4: Data Gathering and APIs",
    "section": "XPath II: Filtering by Attributes",
    "text": "XPath II: Filtering by Attributes\n\n\nmypage.html\n\n&lt;div class=\"container\"&gt;\n  &lt;h1&gt;Header&lt;/h1&gt;\n  &lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\n  &lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n&lt;/div&gt;\n\n\n'//p[id=\"page-content\"]' matches all &lt;p&gt; elements with id page-content1:\n&lt;p id=\"page-content\"&gt;Content&lt;/p&gt;\nMatching classes is a bit trickier:\n'//img[contains(concat(\" \", normalize-space(@class), \" \"), \" footer-image \")]'\nmatches all &lt;img&gt; elements with page-content as one of their classes2\n&lt;img class=\"footer-image m-5\" src=\"footer.png\"&gt;\n\nIn HTML, ids are required to be unique to particular elements (and elements cannot have more than one id), meaning that this should only return a single element, for valid HTML code (not followed by all webpages!). Also note the double-quotes after id=, which are required in XPath.Your intuition may be to just use '//img[@class=\"footer-image\"]'. Sadly, however, this will match only elements with footer-image as their only class. i.e., it will match &lt;img class=\"footer-image\"&gt; but not &lt;img class=\"footer-image another-class\"&gt;. This will usually fail, since most elements on modern webpages have several classes. For example, if the site is using Bootstrap, &lt;p class=\"p-5 m-3\"&gt;&lt;/p&gt; creates a paragraph element with a padding of 5 pixels and a margin of 3 pixels."
  },
  {
    "objectID": "w04/slides.html#example-math-api-1",
    "href": "w04/slides.html#example-math-api-1",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Example: Math API",
    "text": "Example: Math API\n\nBase URL: https://newton.vercel.app/api/v2/\nThe endpoint: factor\nThe argument: \"x^2 - 1\"\nThe request: https://newton.vercel.app/api/v2/factor/x^2-1\n\n\n\nCode\nlibrary(httr2)\nrequest_obj &lt;- request(\"https://newton.vercel.app/api/v2/factor/x^2-1\")\nresponse_obj &lt;- req_perform(request_obj)\nwriteLines(response_obj %&gt;% resp_body_string())\n\n\n{\"operation\":\"factor\",\"expression\":\"x^2-1\",\"result\":\"(x - 1) (x + 1)\"}"
  },
  {
    "objectID": "w04/slides.html#authentication-1",
    "href": "w04/slides.html#authentication-1",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication",
    "text": "Authentication\n\nMost APIs don’t allow requests to be made by anonymous requesters, and require authentication.\nFor example, to access private GitHub repos using GitHub’s API, you’ll need to authenticate that you are in fact the one making the request"
  },
  {
    "objectID": "w04/slides.html#authentication-via-gh",
    "href": "w04/slides.html#authentication-via-gh",
    "title": "Week 4: Data Gathering and APIs",
    "section": "Authentication via GH",
    "text": "Authentication via GH\n\nThe GH library for R can handle this authentication process for you. For example, this private repo in my account does not show up if requested anonymously, but does show up when requested using GH with a Personal Access Token1:\n\n\n\nCode\nlibrary(gh)\nresult &lt;- gh(\"GET /repos/jpowerj/private-repo-test\")\nwriteLines(paste0(result$name, \": \",result$description))\n\n\nprivate-repo-test: Private repo example for DSAN5000\n\n\n \n\n\n\n\nYour code should never contain authentication info, especially when using GitHub. In this case, I created an OS environment variable called GITHUB_TOKEN containing my Personal Access Token, which GH then uses to make authenticated requests."
  },
  {
    "objectID": "w02/index.html",
    "href": "w02/index.html",
    "title": "Week 2: Data Science Fundamentals and Workflow",
    "section": "",
    "text": "Today’s Links\n\n\n\n\nWeek 2 Lecture Notes\nLab 1 Home\n\n\n\nToday’s Planned Schedule:\n\n\n\n\n\n\n\n\n\n\n\n\nStart\nEnd\nTopic\n\n\n\n\n\n\nLecture\n12:30pm\n12:35pm\nAbout Me\n\n\n\n\nLecture\n12:35pm\n12:50pm\nComputer Fundamentals\n\n\n\n\n\n12:50pm\n1:10pm\nCoding Fundamentals\n\n\n\n\n\n1:10pm\n1:30pm\nHTML and CSS\n\n\n\n\n\n1:30pm\n1:50pm\nObjects and Classes\n\n\n\n\nBreak!\n1:50pm\n2:00pm\n\n\n\n\n\nLab\n2:00pm\n2:25pm\nLab Part I: Coding Demonstration\n\n\n\n\n\n2:25pm\n2:50pm\nLab Part II: HTML/CSS Demonstration\n\n\n\n\n\n2:50pm\n3:00pm\nLab Assignment Overview"
  },
  {
    "objectID": "writeups/quiz-status/index.html",
    "href": "writeups/quiz-status/index.html",
    "title": "DSAN 5000 Quiz Status Sheet",
    "section": "",
    "text": "Order By\n       Default\n         \n          Quiz Link\n        \n         \n          Solutions Available (Canvas)\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nQuiz Link\n\n\nSolutions Available (Canvas)\n\n\n\n\n\n\nQuiz-3.1\n\n\n✅\n\n\n\n\nQuiz-2.2\n\n\n✅\n\n\n\n\nQuiz-2.1\n\n\n✅\n\n\n\n\nQuiz-1.2\n\n\n✅\n\n\n\n\nQuiz-1.1\n\n\n✅\n\n\n\n\nBootcamp Quiz\n\n\n✅\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/hw1-clarifications/index.html",
    "href": "writeups/hw1-clarifications/index.html",
    "title": "Homework 1 Clarifications",
    "section": "",
    "text": "Relevant Links\n\n\n\n\nMain Course Page:\n\nHomework 1 Assignment Page\nProject Topic Spreadsheet (Claim your project topic here! See Step 4 on the assignment page)\n\nOther Writeups:\n\nUsing Quarto’s Reference/Citation Manager"
  },
  {
    "objectID": "writeups/hw1-clarifications/index.html#due-date-clarification-pushed-to-sep-21",
    "href": "writeups/hw1-clarifications/index.html#due-date-clarification-pushed-to-sep-21",
    "title": "Homework 1 Clarifications",
    "section": "⚠️ Due Date Clarification: Pushed to Sep 21",
    "text": "⚠️ Due Date Clarification: Pushed to Sep 21\n\nImportant clarification for Jeff’s DSAN 5000 sections (02 and 03): Because of my delay in teaching you all the lab material, I am pushing the due date for Homework 1, for both Section 02 and Section 03, forward to Thursday, September 21st at 11:59pm EDT."
  },
  {
    "objectID": "writeups/hw1-clarifications/index.html#hw1-step-2-cloning-the-github-repo",
    "href": "writeups/hw1-clarifications/index.html#hw1-step-2-cloning-the-github-repo",
    "title": "Homework 1 Clarifications",
    "section": "(HW1 Step 2) Cloning the GitHub repo",
    "text": "(HW1 Step 2) Cloning the GitHub repo\n\nThe GitHub repository that is automatically created when you accept the GitHub Classroom assignment defaults to private. This may be an issue if you have not yet set up a Personal Access Token on your GitHub account, or if you have run into issues when trying to set up the Personal Access Token. See this page for GitHub’s documentation on how to set up and manage these tokens.\n\nOnce you have created a token, run the following command:\n\ngit clone https://github.com/anly501/dsan-5000-project-[your username].git\n\nThis command tells git that you’d like it to clone (i.e., copy) the automatically-created dsan-5000-project-[your username] repository to your local computer.\nBut, it may ask you for a password before it allows this downloading. If so, do not enter your “main” GitHub password: GitHub is phasing out the ability to use this password to clone repositories. Instead, just copy-and-paste the Personal Access Token you created into the terminal (as in, paste this Token instead of typing out your GitHub account password).\nIt should now download the remote (automatically-created) repository to your local drive. (If it doesn’t, or if you’re still experiencing issues, feel free to email me or sign up for an office hours slot!)"
  },
  {
    "objectID": "writeups/lab-1-2-clarifications/index.html",
    "href": "writeups/lab-1-2-clarifications/index.html",
    "title": "Lab 1.2 Clarifications",
    "section": "",
    "text": "Links\n\n\n\nLab 1.2 Link"
  },
  {
    "objectID": "writeups/lab-1-2-clarifications/index.html#component-3",
    "href": "writeups/lab-1-2-clarifications/index.html#component-3",
    "title": "Lab 1.2 Clarifications",
    "section": "Component-3",
    "text": "Component-3\n\nCreating a Quarto Website in VSCode\nThe beginning of the Component-3 description states that you should:\n\nMake sure the Quarto extension is installed in VS-Code.\nUsing VS-Code –&gt; New file –&gt; “Quarto project” –&gt; create a blank Quarto “Website” project in your 5000-lab-1.2 repo call simple_quarto_website\n\nTo clarify this portion: you can install the Quarto extension for VSCode by clicking on the “Extensions” panel on the left side of the VSCode interface (or pressing Cmd+Shift+X) and searching for “Quarto” in the extensions search box. The Quarto extension should be the first result:\n\nOnce you have this extension installed, there is a bit of a typo in the assignment description about how to create a Quarto project (specifically, a website project) within VSCode.\nRather than using VS-Code-&gt;New file-&gt;“Quarto project”, in the most recent version of the Quarto extension you should instead do the following:\n\nUse the keyboard shortcut Shift-Cmd-P (Shift-Ctrl-P on Windows) to open what’s called the Command Palette in VSCode. It is a box with a text field in it that should pop up near the top of your screen:\n\n\n\nWithin this Command Palette, start typing the word “Quarto”. As you type, it should filter out the irrelevant commands and just show the Quarto-related commands.\nThe first command in the list should now be Quarto: Create Project:\n\n\n\nClick this command, to begin the process of creating a Quarto project\nChoose “Quarto Website” on the next screen:\n\n\n\nChoose the folder on your computer where you would like Quarto to create the website (for the lab, for example, you could create a folder called simple_quarto_website, and tell Quarto that you’d like to create the website within that folder):\n\n\n\nVSCode will now create the website and display the created files “_quarto.yml”, “index.qmd”, “about.qmd”, and “style.css” in the “Explorer” pane on the left side of the VSCode interface:\n\n\nYou should now be able to proceed with the lab, by modifying and adding files to this newly-created Quarto website project.\n\n\n\n\n\n\nIf “Quarto: Create Project” Does Not Appear\n\n\n\nSome students are experiencing a bug where the “Quarto: Create Project” option does not appear in their Command Palette, after pressing Cmd+Shift+P. In this case, no need to worry: just create a new folder (called simple_quarto_website, for example), and within this folder you’ll need to create four files, which are the same four files that the “Quarto: Create Project” command creates:\n\n1. _quarto.yml\nCreate a file called _quarto.yml with the following contents:\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"simple_quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\n\n\n\n2. styles.css\nThen create a file called styles.css that can be empty for now.\n\n\n3. index.qmd\nThis file should have the following contents:\n---\ntitle: \"simple_quarto_website\"\n---\n\nThis is a Quarto website.\n\nTo learn more about Quarto websites visit &lt;https://quarto.org/docs/websites&gt;.\n\n\n4. about.qmd\nThis file should have the following contents:\n---\ntitle: \"About\"\n---\n\nAbout this site\nThese are the default website contents that the “Quarto: New Project” command creates in this _quarto.yml file, so for the most part you should be ready to work on the rest of the project even if this command did not appear when you typed “Quarto” into the Command Palette.\n\n\n\n\n\nMultiple Versions of the About Page\nThere is a step in Component-3 which asks you to:\n\nConvert about.qmd to about.ipynb with quarto convert about.qmd.\n\nAfter which (the last bullet in the section) it also asks you to\n\nmodify _quarto.yml accordingly - about.qmd -&gt; about.ipynb\n\nPlease note the following points about these two steps:\n\n\nMaking sure you have added both href and text data when updating _quarto.yml\n\nWhen you go to modify _quarto.yml, you’ll notice that the Home link is specified using two pieces of information, href and text:\n\n      - href: index.qmd\n        text: Home\n\nHowever, the About link is specified using a single bullet point, just indicating the name of the file:\n\n      - about.qmd\n\nSo, when you go to update _quarto.yml to change about.qmd to about.ipynb, make sure that you include both pieces of information given for the Home link, so that your new link information should now be:\n\n      - href: index.qmd\n        text: Home\n      - href: about.ipynb\n        text: About\n\n\nSpecifying the output format for about.ipynb\n\nAfter running quarto convert about.qmd, you can open the generated about.ipynb file to view the result.\nHowever, you’ll see that the metadata block at the top of the file (the block which starts and ends with ---) only contains the title attribute. That is, the top of the file will look like the following:\n\n---\ntitle: \"About\"\n---\n\nBut, for your website to correctly render this .ipynb file, you’ll need to also add information to this metadata block telling Quarto that you’d like it to output HTML code when it renders about.ipynb for your website.\nTo achieve this, add an additional line within the metadata block (i.e., between the starting --- and the ending ---), after the title: \"About\" line, specifying format: html. If you’ve done this correctly, your metadata block will now look like the following:\n\n---\ntitle: \"About\"\nformat: html\n---\n\nOnce you have added this additional piece of metadata, and saved your changes to the about.ipynb file, you will be halfway to successfully rendering this file as a page within your website. For the second half, see the next point.\n\n\n\nRemoving the about.qmd file\n\nIf you try to quarto render and/or quarto preview the website now, you may find that clicking the “About” link at the top of the page will show just the raw text content of the about.ipynb file, rather than the rendered, nicely-formatted and human-readable version. That is, when you click the “About” link, you may just see a plaintext file with the following contents:\n\n{\n  \"cells\": [\n    {\n      \"cell_type\": \"raw\",\n      \"metadata\": {},\n      \"source\": [\n        \"---\\n\",\n        \"title: \\\"About\\\"\\n\",\n        \"format: html\\n\",\n        \"---\"\n      ]\n    },\n    {\n      \"cell_type\": \"markdown\",\n      \"metadata\": {},\n      \"source\": [\n        \"About this site\"\n      ]\n    }\n  ],\n  \"metadata\": {\n    \"kernelspec\": {\n      \"display_name\": \"Python 3 (ipykernel)\",\n      \"language\": \"python\",\n      \"name\": \"python3\"\n    }\n  },\n  \"nbformat\": 4,\n  \"nbformat_minor\": 4\n}\n\nIf you encounter this issue, you can fix it by resolving the ambiguity that Quarto faces in having both about.qmd and about.ipynb files, and not knowing which one to choose as the file that should be rendered to create the final about.html file, the file that will be placed in the _site directory that quarto render creates.\nTo resolve this ambiguity,\n\nFirst, if you’re paranoid about losing the information in about.qmd, you can back this file up to another folder on your computer.\nThen (or, you can start here if you’re not worried about losing about.qmd), delete the original about.qmd file.\n\nDeleting the original about.qmd file will thus ensure that Quarto has a single about file (in this case, about.ipynb) that it knows is the file you want Quarto to render and show when a user clicks the “About” link."
  },
  {
    "objectID": "writeups/lab-1-2-clarifications/index.html#component-4-and-component-5-lorem-ipsum-text",
    "href": "writeups/lab-1-2-clarifications/index.html#component-4-and-component-5-lorem-ipsum-text",
    "title": "Lab 1.2 Clarifications",
    "section": "Component-4 and Component-5: Lorem Ipsum Text",
    "text": "Component-4 and Component-5: Lorem Ipsum Text\nIn one of the bullets in both the Component-4 section and Component-5 section of the lab, it mentions the following:\n\nNote: Use of ipsum lorem place-holder text is allowed\n\nWhat this means is just: If you would like “filler text” for your index.qmd page, to show what it will look like once it’s filled with content (but without having to write hundreds of words of content yourself), you can use a “Lorem Ipsum text generator” to auto-generate some number of paragraphs (filled with random Latin words), and place this auto-generated text in your index.qmd file.\nTo see why you might want to do this, consider the difference between the following structure with no filler text:\n\n\n\n\n\n\nSection Headers Without Filler Content\n\n\n\nMy Heading\n\nMy Subheading\n\n\nAnother Subheading\n\n\nThird Subheading\n\n\n\nAnd the same structure but “filled out” with paragraphs of randomly-generated text:\n\n\n\n\n\n\nSection Headers With Filler Content\n\n\n\nMy Heading\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Sed cras ornare arcu dui vivamus arcu felis. Dis parturient montes nascetur ridiculus mus. Tincidunt ornare massa eget egestas purus viverra. Porttitor massa id neque aliquam vestibulum morbi. Fringilla phasellus faucibus scelerisque eleifend. Faucibus nisl tincidunt eget nullam non nisi est sit amet. Urna id volutpat lacus laoreet non. Sed pulvinar proin gravida hendrerit lectus a. Sed odio morbi quis commodo odio aenean sed adipiscing. Bibendum at varius vel pharetra vel turpis nunc. Nec dui nunc mattis enim ut tellus elementum sagittis. Nisl rhoncus mattis rhoncus urna neque viverra. Elit at imperdiet dui accumsan.\n\nMy Subheading\nPorta non pulvinar neque laoreet suspendisse interdum consectetur libero id. Laoreet sit amet cursus sit amet dictum sit amet justo. Vitae semper quis lectus nulla at volutpat. Aliquet eget sit amet tellus cras adipiscing enim eu turpis. Tellus elementum sagittis vitae et leo duis ut. Neque vitae tempus quam pellentesque. Cursus eget nunc scelerisque viverra mauris. Morbi leo urna molestie at. Convallis posuere morbi leo urna molestie at elementum. Non consectetur a erat nam. Sagittis nisl rhoncus mattis rhoncus urna neque viverra. Lorem sed risus ultricies tristique.\n\n\nAnother Subheading\nTurpis massa sed elementum tempus egestas. Et netus et malesuada fames ac turpis. Diam maecenas ultricies mi eget mauris pharetra et ultrices neque. Felis imperdiet proin fermentum leo vel orci porta non pulvinar. Vulputate dignissim suspendisse in est. Ultricies mi quis hendrerit dolor magna eget est. Senectus et netus et malesuada fames. Aliquet porttitor lacus luctus accumsan tortor posuere ac. Et ligula ullamcorper malesuada proin libero nunc consequat. Nibh mauris cursus mattis molestie a iaculis. Sit amet commodo nulla facilisi nullam. Mollis nunc sed id semper risus in hendrerit. Sit amet massa vitae tortor condimentum lacinia. Ante in nibh mauris cursus. Aliquam purus sit amet luctus. Tincidunt arcu non sodales neque.\n\n\nThird Subheading\nLorem ipsum dolor sit amet consectetur adipiscing. Tortor pretium viverra suspendisse potenti. Felis eget nunc lobortis mattis aliquam faucibus. Morbi tincidunt ornare massa eget egestas purus viverra. Et netus et malesuada fames ac turpis. Nunc id cursus metus aliquam eleifend. Imperdiet massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada. Dignissim diam quis enim lobortis scelerisque. Vel pretium lectus quam id leo in vitae turpis. Venenatis cras sed felis eget velit aliquet. Eu feugiat pretium nibh ipsum. Eu tincidunt tortor aliquam nulla facilisi.\n\n\n\nIf you think this second version looks nicer than the first, and shows more clearly how the headers separate out different pieces of content, you can use this “lorem ipsum” text as filler (until you replace it with your own original writing!)."
  },
  {
    "objectID": "writeups/lab-1-2-clarifications/index.html#component-6",
    "href": "writeups/lab-1-2-clarifications/index.html#component-6",
    "title": "Lab 1.2 Clarifications",
    "section": "Component-6",
    "text": "Component-6\n\nLinking to the Slideshow From Your Main Page\nIn Component-6, the assignment asks you to create a slideshow, with the first two steps being:\n\nCreate a slides subfolder, within simple_quarto_website\nCreate a file within the slides subfolder called slides.ipynb\n\nAnd then it asks you to modify the yaml-format metadata block—this is the block of specially-formatted key-value pairs at the top of the first cell within the slides.ipynb file, that looks like the following (for example):\n---\ntitle: \"My Slideshow\"\n---\nTo specify to Quarto that you’d like it to render this slides.ipynb file specifically as a Reveal.js slideshow, you’ll need to add the following additional key-value pair to this metadata block (which, again, should be at the top of the first cell within slides.ipynb):\nformat: revealjs\nMeaning that, if your original metadata block just had the title, like in the example above, now it will have two pieces of information for Quarto:\n---\ntitle: \"My Slideshow\"\nformat: revealjs\n---\nOnce you’ve specified this format, though, it is still a bit tricky to preview the slideshow (to make sure that modifications you make are actually appearing in the slideshow, for example), since the Quarto extension for VSCode does not provide a nice “Render” button for .ipynb-format files like it does for .qmd-format files.\nSo, you can make your life slightly easier by adding a link to the slideshow from your website’s main navigation bar. To do that, you’ll need to modify your website’s _quarto.yml file, which contains the global settings for the website. If you open this file at this point in the project, it should look something like:\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"simple_quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.ipynb\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nNotice the portion within the value of website (the indented portions under website:) with the key navbar, and within that value, the key left. Every bullet point underneath this left key will correspond to a link on the top navbar of your website, where the left key indicates that these links will be left-aligned (you could add a new key right:, and some links underneath that key, and those links would be right-aligned on the top navbar).\nSo, to add a link to the slideshow you are creating in slides/slides.ipynb, we’ll need to modify two things in this _quarto.yml file. First, since Quarto will sometimes complain and/or get confused if some links have href and text tags while others do not have this tag, let’s change the link that just contains about.ipynb to have an href and label, like:\n- href: about.ipynb\n  text: About\nThe YAML format requires that elements of a list (like the list of left-aligned links under the left key) line up in terms of indentation, so make sure that the full _quarto.yml file now looks something like:\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"simple_quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: about.ipynb\n        text: About\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nNow we should be able to add another, third link to this list, allowing us to go directly to the slideshow we’re making in slides/slides.ipynb. To do this, add a new element to the left list by adding another “bullet point” (another - character), then set the href value to be exactly that path (slides/slides.ipynb) and the text value to be “My Slides” (or, any title you’d like). Your _quarto.yml file should now look something like:\n\n\n_quarto.yml\n\nproject:\n  type: website\n\nwebsite:\n  title: \"simple_quarto_website\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: about.ipynb\n        text: About\n      - href: slides/slides.ipynb\n        text: Slides\n\nformat:\n  html:\n    theme: cosmo\n    toc: true\n\nand now when you preview your website by running the quarto preview command in the terminal (from within the root directory of your website, which should be named simple_quarto_website), you should see three links in the top navbar: one to “Home”, one to the “About” page, and a third to “Slides”.\nNow, when you actually click the “Slides” link, it may not show anything, or it may show a bunch of non-human-readable raw code (which is actually the .json structure underlying a .ipynb file). This, however, is a separate issue, that you’ll need to tackle: how to get Quarto to know that it should show the Reveal.js presentation as the rendered form of the slides/slides.ipynb file. And that is part of the challenge of the assignment. As a hint, though, you should break it down into steps, to check exactly where this rendering is going wrong (if the slideshow doesn’t render as intended when you click the new “Slides” link):\n\nIf you run the quarto preview command from within the slides folder (rather than the root folder of your website, simple_quarto_website), does it display correctly as a slideshow?\nIs the first cell in your slides.ipynb file in the correct format for containing file metadata? (File metadata can only be entered in “raw” or “markdown” cell types. If the first cell is, for example, a “Python” cell type, Quarto will be unable to read your metadata block, as it will think it’s Python code rather than YAML code).\nDoes the file metadata actually exist in a block at the top of the first cell? As mentioned above, the top of the first cell in your .ipynb file (which should be a “raw” or “markdown” type cell) should look something like\n\n---\ntitle: \"Slides\"\nformat: revealjs\n---\nIf you don’t see text that looks like this at the top of the first cell in your slides.ipynb file, please add it and try to preview the file again (both from the root folder of your website and, for debugging purposes, also from the slides subfolder, as mentioned in step #1 above)"
  },
  {
    "objectID": "writeups/data-cleaning/clean_data.html",
    "href": "writeups/data-cleaning/clean_data.html",
    "title": "Data Cleaning with Python",
    "section": "",
    "text": "Links\n\n\n\n\nThis is the Jupyter notebook I created during the Video Walkthrough \nRun on Colab  to pause the video and edit interactively!\n\n\n\nFirst, since the .csv filename is super long, let’s just use one of my favorite Python libraries, glob, to automatically get a list of all the .csv files within the same folder as this notebook\n\nimport glob\n\nimport pandas as pd\nimport numpy as np\n\n\nglob.glob(\"*.csv\")\n\n['SHIP_Emergency_Department_Visits_Related_To_Mental_Health_Conditions_2008-2017.csv',\n 'PLACES__Local_Data_for_Better_Health__Place_Data_2022_release.csv',\n 'DASH_-_Youth_Risk_Behavior_Surveillance_System__YRBSS___High_School_-_Excluding_Sexual_Identity.csv']\n\n\n\nall_data_fnames = glob.glob(\"*.csv\") + glob.glob(\"*.xlsx\")\n# for fname in all_data_fnames:\n#     print(fname)\nfor fname_index, fname in enumerate(all_data_fnames):\n    print(fname_index, fname)\n\n0 SHIP_Emergency_Department_Visits_Related_To_Mental_Health_Conditions_2008-2017.csv\n1 PLACES__Local_Data_for_Better_Health__Place_Data_2022_release.csv\n2 DASH_-_Youth_Risk_Behavior_Surveillance_System__YRBSS___High_School_-_Excluding_Sexual_Identity.csv\n3 MD_CountyLevelSummary_2017.xlsx\n\n\n\nall_data_fnames[0]\n\n'SHIP_Emergency_Department_Visits_Related_To_Mental_Health_Conditions_2008-2017.csv'\n\n\nSHIP data: https://catalog.data.gov/dataset/ship-emergency-department-visits-related-to-mental-health-conditions-2008-2017\n\nship_fname = glob.glob(\"SHIP*.csv\")[0]\nship_df = pd.read_csv(ship_fname)\nship_df.head()\n\n\n\n\n\n\n\n\nJurisdiction\nValue\nRace/ ethnicity\nYear\nMeasure\n\n\n\n\n0\nState\n4291.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n1\nAllegany\n3309.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n2\nAnne Arundel\n5734.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n3\nBaltimore City\n10093.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n4\nBaltimore County\n4210.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n\n\n\n\n\n\n!pip install openpyxl\n\nCollecting openpyxl\n  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 250.0/250.0 kB 4.2 MB/s eta 0:00:00a 0:00:01\nCollecting et-xmlfile (from openpyxl)\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n\n\n\nplaces_df = pd.read_excel(\"MD_CountyLevelSummary_2017.xlsx\", skiprows=3)\nrename_map = {\n    'Unnamed: 0': 'fips',\n    'Unnamed: 1': 'county',\n    'Unnamed: 2': 'child_pop',\n    'Percent': 'pct_lead'\n}\nplaces_df.rename(columns=rename_map, inplace=True)\n\n\ncols_to_keep = list(rename_map.values())\nplaces_df = places_df[cols_to_keep]\n\n\nplaces_df\n\n\n\n\n\n\n\n\nfips\ncounty\nchild_pop\npct_lead\n\n\n\n\n0\n001\nAllegany County\n3986.0\n0.020888\n\n\n1\n003\nAnne Arundel County\n42211.0\n0.003220\n\n\n2\n005\nBaltimore County\n59089.0\n0.009980\n\n\n3\n009\nCalvert County\n6099.0\n0.006550\n\n\n4\n011\nCaroline County\n2428.0\n0.019973\n\n\n5\n013\nCarroll County\n10860.0\n0.006377\n\n\n6\n015\nCecil County\n7085.0\n0.006893\n\n\n7\n017\nCharles County\n11834.0\n0.002288\n\n\n8\n019\nDorchester County\n2215.0\n0.027439\n\n\n9\n021\nFrederick County\n18017.0\n0.003054\n\n\n10\n023\nGarrett County\n1717.0\nNaN\n\n\n11\n025\nHarford County\n17236.0\n0.003109\n\n\n12\n027\nHoward County\n23609.0\n0.008291\n\n\n13\n029\nKent County\n973.0\nNaN\n\n\n14\n031\nMontgomery County\n80503.0\n0.003826\n\n\n15\n033\nPrince George's County\n70775.0\n0.012376\n\n\n16\n035\nQueen Anne's County\n3034.0\nNaN\n\n\n17\n037\nSt. Mary's County\n8825.0\nNaN\n\n\n18\n039\nSomerset County\n1511.0\n0.015837\n\n\n19\n041\nTalbot County\n2068.0\n0.012442\n\n\n20\n043\nWashington County\n10569.0\n0.008881\n\n\n21\n045\nWicomico County\n7469.0\n0.013187\n\n\n22\n047\nWorcester County\n2762.0\n0.012959\n\n\n23\n510\nBaltimore (city) County\n46273.0\n0.041661\n\n\n24\nNaN\nUnknown\nNaN\nNaN\n\n\n25\nNaN\nNaN\nNaN\nNaN\n\n\n26\nNotes: 'N/A' indicates data are supressed when...\nNaN\nNaN\nNaN\n\n\n27\nPopulation estimates calculated as population ...\nNaN\nNaN\nNaN\n\n\n28\nData received and processed by CDC as of April...\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nplaces_df = places_df.iloc[0:24].copy()\n\n\nplaces_df\n\n\n\n\n\n\n\n\nfips\ncounty\nchild_pop\npct_lead\n\n\n\n\n0\n001\nAllegany\n3986.0\n0.020888\n\n\n1\n003\nAnne Arundel\n42211.0\n0.003220\n\n\n2\n005\nBaltimore\n59089.0\n0.009980\n\n\n3\n009\nCalvert\n6099.0\n0.006550\n\n\n4\n011\nCaroline\n2428.0\n0.019973\n\n\n5\n013\nCarroll\n10860.0\n0.006377\n\n\n6\n015\nCecil\n7085.0\n0.006893\n\n\n7\n017\nCharles\n11834.0\n0.002288\n\n\n8\n019\nDorchester\n2215.0\n0.027439\n\n\n9\n021\nFrederick\n18017.0\n0.003054\n\n\n10\n023\nGarrett\n1717.0\nNaN\n\n\n11\n025\nHarford\n17236.0\n0.003109\n\n\n12\n027\nHoward\n23609.0\n0.008291\n\n\n13\n029\nKent\n973.0\nNaN\n\n\n14\n031\nMontgomery\n80503.0\n0.003826\n\n\n15\n033\nPrince George's\n70775.0\n0.012376\n\n\n16\n035\nQueen Anne's\n3034.0\nNaN\n\n\n17\n037\nSt. Mary's\n8825.0\nNaN\n\n\n18\n039\nSomerset\n1511.0\n0.015837\n\n\n19\n041\nTalbot\n2068.0\n0.012442\n\n\n20\n043\nWashington\n10569.0\n0.008881\n\n\n21\n045\nWicomico\n7469.0\n0.013187\n\n\n22\n047\nWorcester\n2762.0\n0.012959\n\n\n23\n510\nBaltimore (city)\n46273.0\n0.041661\n\n\n\n\n\n\n\n\nplaces_df['county'] = places_df['county'].str.replace(\" County\",\"\")\n\n\nplaces_df\n\n\n\n\n\n\n\n\nfips\ncounty\nchild_pop\npct_lead\n\n\n\n\n0\n001\nAllegany\n3986.0\n0.020888\n\n\n1\n003\nAnne Arundel\n42211.0\n0.003220\n\n\n2\n005\nBaltimore\n59089.0\n0.009980\n\n\n3\n009\nCalvert\n6099.0\n0.006550\n\n\n4\n011\nCaroline\n2428.0\n0.019973\n\n\n5\n013\nCarroll\n10860.0\n0.006377\n\n\n6\n015\nCecil\n7085.0\n0.006893\n\n\n7\n017\nCharles\n11834.0\n0.002288\n\n\n8\n019\nDorchester\n2215.0\n0.027439\n\n\n9\n021\nFrederick\n18017.0\n0.003054\n\n\n10\n023\nGarrett\n1717.0\nNaN\n\n\n11\n025\nHarford\n17236.0\n0.003109\n\n\n12\n027\nHoward\n23609.0\n0.008291\n\n\n13\n029\nKent\n973.0\nNaN\n\n\n14\n031\nMontgomery\n80503.0\n0.003826\n\n\n15\n033\nPrince George's\n70775.0\n0.012376\n\n\n16\n035\nQueen Anne's\n3034.0\nNaN\n\n\n17\n037\nSt. Mary's\n8825.0\nNaN\n\n\n18\n039\nSomerset\n1511.0\n0.015837\n\n\n19\n041\nTalbot\n2068.0\n0.012442\n\n\n20\n043\nWashington\n10569.0\n0.008881\n\n\n21\n045\nWicomico\n7469.0\n0.013187\n\n\n22\n047\nWorcester\n2762.0\n0.012959\n\n\n23\n510\nBaltimore (city)\n46273.0\n0.041661\n\n\n\n\n\n\n\n\nplaces_df['county'] = places_df['county'].str.replace('(city)', 'City')\n\n\nplaces_df\n\n\n\n\n\n\n\n\nfips\ncounty\nchild_pop\npct_lead\n\n\n\n\n0\n001\nAllegany\n3986.0\n0.020888\n\n\n1\n003\nAnne Arundel\n42211.0\n0.003220\n\n\n2\n005\nBaltimore\n59089.0\n0.009980\n\n\n3\n009\nCalvert\n6099.0\n0.006550\n\n\n4\n011\nCaroline\n2428.0\n0.019973\n\n\n5\n013\nCarroll\n10860.0\n0.006377\n\n\n6\n015\nCecil\n7085.0\n0.006893\n\n\n7\n017\nCharles\n11834.0\n0.002288\n\n\n8\n019\nDorchester\n2215.0\n0.027439\n\n\n9\n021\nFrederick\n18017.0\n0.003054\n\n\n10\n023\nGarrett\n1717.0\nNaN\n\n\n11\n025\nHarford\n17236.0\n0.003109\n\n\n12\n027\nHoward\n23609.0\n0.008291\n\n\n13\n029\nKent\n973.0\nNaN\n\n\n14\n031\nMontgomery\n80503.0\n0.003826\n\n\n15\n033\nPrince George's\n70775.0\n0.012376\n\n\n16\n035\nQueen Anne's\n3034.0\nNaN\n\n\n17\n037\nSt. Mary's\n8825.0\nNaN\n\n\n18\n039\nSomerset\n1511.0\n0.015837\n\n\n19\n041\nTalbot\n2068.0\n0.012442\n\n\n20\n043\nWashington\n10569.0\n0.008881\n\n\n21\n045\nWicomico\n7469.0\n0.013187\n\n\n22\n047\nWorcester\n2762.0\n0.012959\n\n\n23\n510\nBaltimore City\n46273.0\n0.041661\n\n\n\n\n\n\n\n\nship_df['Year'] == 2017\n\n0        True\n1        True\n2        True\n3        True\n4        True\n        ...  \n1245    False\n1246    False\n1247    False\n1248    False\n1249    False\nName: Year, Length: 1250, dtype: bool\n\n\n\nship_df = ship_df.loc[ship_df['Year'] == 2017]\n\n\nship_df = ship_df[ship_df['Jurisdiction'] != \"State\"]\n\n\nship_df = ship_df[ship_df['Race/ ethnicity'] == \"All races/ ethnicities (aggregated)\"]\nship_df\n\n\n\n\n\n\n\n\nJurisdiction\nValue\nRace/ ethnicity\nYear\nMeasure\n\n\n\n\n0\nState\n4291.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n1\nAllegany\n3309.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n2\nAnne Arundel\n5734.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n3\nBaltimore City\n10093.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n4\nBaltimore County\n4210.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n5\nCalvert\n2999.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n6\nCaroline\n7556.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n7\nCarroll\n4216.0\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n8\nCecil\n9584.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n9\nCharles\n2817.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n10\nDorchester\n11251.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n11\nFrederick\n3064.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n12\nGarrett\n7967.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n13\nHarford\n3020.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n14\nHoward\n3082.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n15\nKent\n13662.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n16\nMontgomery\n2312.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n17\nPrince George's\n1955.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n18\nQueen Anne's\n6119.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n19\nSaint Mary's\n6173.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n20\nSomerset\n2696.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n21\nTalbot\n7661.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n22\nWashington\n5410.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n23\nWicomico\n2897.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n24\nWorcester\n3502.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n\n\n\n\n\n\nship_df\n\n\n\n\n\n\n\n\nJurisdiction\nValue\nRace/ ethnicity\nYear\nMeasure\n\n\n\n\n1\nAllegany\n3309.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n2\nAnne Arundel\n5734.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n3\nBaltimore City\n10093.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n4\nBaltimore County\n4210.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n5\nCalvert\n2999.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n6\nCaroline\n7556.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n7\nCarroll\n4216.0\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n8\nCecil\n9584.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n9\nCharles\n2817.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n10\nDorchester\n11251.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n11\nFrederick\n3064.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n12\nGarrett\n7967.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n13\nHarford\n3020.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n14\nHoward\n3082.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n15\nKent\n13662.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n16\nMontgomery\n2312.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n17\nPrince George's\n1955.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n18\nQueen Anne's\n6119.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n19\nSaint Mary's\n6173.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n20\nSomerset\n2696.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n21\nTalbot\n7661.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n22\nWashington\n5410.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n23\nWicomico\n2897.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n24\nWorcester\n3502.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n\n\n\n\n\n\nship_df['Jurisdiction'] = ship_df['Jurisdiction'].str.replace(\"Baltimore County\", \"Baltimore\")\n\n/var/folders/n2/m7_fj5vx6c50_yj7g23mwmq00000gn/T/ipykernel_76467/211774966.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ship_df['Jurisdiction'] = ship_df['Jurisdiction'].str.replace(\"Baltimore County\", \"Baltimore\")\n\n\n\nship_df\n\n\n\n\n\n\n\n\nJurisdiction\nValue\nRace/ ethnicity\nYear\nMeasure\n\n\n\n\n1\nAllegany\n3309.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n2\nAnne Arundel\n5734.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n3\nBaltimore City\n10093.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n4\nBaltimore\n4210.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n5\nCalvert\n2999.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n6\nCaroline\n7556.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n7\nCarroll\n4216.0\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n8\nCecil\n9584.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n9\nCharles\n2817.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n10\nDorchester\n11251.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n11\nFrederick\n3064.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n12\nGarrett\n7967.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n13\nHarford\n3020.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n14\nHoward\n3082.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n15\nKent\n13662.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n16\nMontgomery\n2312.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n17\nPrince George's\n1955.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n18\nQueen Anne's\n6119.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n19\nSaint Mary's\n6173.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n20\nSomerset\n2696.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n21\nTalbot\n7661.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n22\nWashington\n5410.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n23\nWicomico\n2897.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n24\nWorcester\n3502.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n\n\n\n\n\n\nship_df['Jurisdiction'] = ship_df['Jurisdiction'].str.replace(\"Saint\", \"St.\")\n\n/var/folders/n2/m7_fj5vx6c50_yj7g23mwmq00000gn/T/ipykernel_76467/2748728514.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  ship_df['Jurisdiction'] = ship_df['Jurisdiction'].str.replace(\"Saint\", \"St.\")\n\n\n\nship_df\n\n\n\n\n\n\n\n\nJurisdiction\nValue\nRace/ ethnicity\nYear\nMeasure\n\n\n\n\n1\nAllegany\n3309.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n2\nAnne Arundel\n5734.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n3\nBaltimore City\n10093.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n4\nBaltimore\n4210.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n5\nCalvert\n2999.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n6\nCaroline\n7556.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n7\nCarroll\n4216.0\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n8\nCecil\n9584.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n9\nCharles\n2817.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n10\nDorchester\n11251.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n11\nFrederick\n3064.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n12\nGarrett\n7967.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n13\nHarford\n3020.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n14\nHoward\n3082.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n15\nKent\n13662.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n16\nMontgomery\n2312.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n17\nPrince George's\n1955.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n18\nQueen Anne's\n6119.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n19\nSt. Mary's\n6173.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n20\nSomerset\n2696.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n21\nTalbot\n7661.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n22\nWashington\n5410.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n23\nWicomico\n2897.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n24\nWorcester\n3502.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n\n\n\n\n\n\nmerged_df = places_df.merge(ship_df, left_on='county', right_on='Jurisdiction', how='left', indicator=False)\n\n\nmerged_df\n\n\n\n\n\n\n\n\nfips\ncounty\nchild_pop\npct_lead\nJurisdiction\nValue\nRace/ ethnicity\nYear\nMeasure\n\n\n\n\n0\n001\nAllegany\n3986.0\n0.020888\nAllegany\n3309.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n1\n003\nAnne Arundel\n42211.0\n0.003220\nAnne Arundel\n5734.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n2\n005\nBaltimore\n59089.0\n0.009980\nBaltimore\n4210.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n3\n009\nCalvert\n6099.0\n0.006550\nCalvert\n2999.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n4\n011\nCaroline\n2428.0\n0.019973\nCaroline\n7556.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n5\n013\nCarroll\n10860.0\n0.006377\nCarroll\n4216.0\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n6\n015\nCecil\n7085.0\n0.006893\nCecil\n9584.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n7\n017\nCharles\n11834.0\n0.002288\nCharles\n2817.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n8\n019\nDorchester\n2215.0\n0.027439\nDorchester\n11251.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n9\n021\nFrederick\n18017.0\n0.003054\nFrederick\n3064.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n10\n023\nGarrett\n1717.0\nNaN\nGarrett\n7967.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n11\n025\nHarford\n17236.0\n0.003109\nHarford\n3020.2\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n12\n027\nHoward\n23609.0\n0.008291\nHoward\n3082.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n13\n029\nKent\n973.0\nNaN\nKent\n13662.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n14\n031\nMontgomery\n80503.0\n0.003826\nMontgomery\n2312.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n15\n033\nPrince George's\n70775.0\n0.012376\nPrince George's\n1955.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n16\n035\nQueen Anne's\n3034.0\nNaN\nQueen Anne's\n6119.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n17\n037\nSt. Mary's\n8825.0\nNaN\nSt. Mary's\n6173.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n18\n039\nSomerset\n1511.0\n0.015837\nSomerset\n2696.1\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n19\n041\nTalbot\n2068.0\n0.012442\nTalbot\n7661.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n20\n043\nWashington\n10569.0\n0.008881\nWashington\n5410.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n21\n045\nWicomico\n7469.0\n0.013187\nWicomico\n2897.6\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n22\n047\nWorcester\n2762.0\n0.012959\nWorcester\n3502.8\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n23\n510\nBaltimore City\n46273.0\n0.041661\nBaltimore City\n10093.5\nAll races/ ethnicities (aggregated)\n2017\nMental Health ED visits\n\n\n\n\n\n\n\n\nmerged_df[['pct_lead','Value']].corr()\n\n\n\n\n\n\n\n\npct_lead\nValue\n\n\n\n\npct_lead\n1.00000\n0.59498\n\n\nValue\n0.59498\n1.00000"
  },
  {
    "objectID": "writeups/regex/index.html",
    "href": "writeups/regex/index.html",
    "title": "Regular Expressions for Data Cleaning",
    "section": "",
    "text": "Since my Data Cleaning with Python video did not end up using regular expressions as part of the data cleaning process, in this writeup I want to quickly show you examples of how learning regular expressions can make your life 1000x easier when cleaning any data involving string variables."
  },
  {
    "objectID": "writeups/regex/index.html#pythons-re-library",
    "href": "writeups/regex/index.html#pythons-re-library",
    "title": "Regular Expressions for Data Cleaning",
    "section": "Python’s re Library",
    "text": "Python’s re Library\nSince regular expressions are the key building block for how programming language compilers/interpreters actually figure out what your code is telling it to do, most languages have a regular expression library built-in, and Python is no exception! To load Python’s regular expression library, you can add the following line of code to the top of your Python file/notebook:\n\nimport re"
  },
  {
    "objectID": "writeups/regex/index.html#pythons-r-syntax",
    "href": "writeups/regex/index.html#pythons-r-syntax",
    "title": "Regular Expressions for Data Cleaning",
    "section": "Python’s r'' Syntax",
    "text": "Python’s r'' Syntax\nAlthough regular expressions are ordinary string objects in Python, we’re using them in a different way from how we use ordinary strings: While you’ve seen ordinary non-RegEx strings like s = \"Hello\", x = \"abc123\", regular expressions are a bit confusing relative to these types of strings, since we’re using them as a sort of “meta-language” to search for patterns within other strings.\nThis means, for example, that if we include the special character \\t in a RegEx string, Python knows to interpret this as “I am looking for a tab character” rather than “Place four spaces here” as it would be interpreted in an ordinary non-RegEx string. To accomplish this separation, Python allows you to prefix regular expression strings with the letter r, like\n\nordinary_string = \"Hello\\tWorld\"\nregex_string = r'Hello\\tWorld'\n\nSo that now if we print the two strings we can see the difference:\n\nprint(ordinary_string)\nprint(regex_string)\n\nThis will become important as you start to use regular expressions to clean data: for example, if you happen to come across a dataset in .tsv format, and you want to convert it into .csv so it is more consistently read and displayed across different Operating Systems, you can use the following RegEx pattern to find all of the \\t characters and replace them with a comma (we will learn about each of the functions I’m using here in the sections below, so don’t worry if you don’t understand what’s happening here for the moment):\n\noriginal_file = \"\"\"id\\tvar1\\tvar2\n0\\tJeff\\t5\n1\\tJames\\t6\n2\\tNakul\\t7\"\"\"\ntab_regex_str = r'\\t'\ntab_regex = re.compile(tab_regex_str)\ncleaned_file = tab_regex.sub(\",\", original_file)\nprint(cleaned_file)\n\nOnce the library is imported, although there are lots of functions you could use, the following are the main ones you will use for data cleaning:\n\nre.compile()\nLong story short, although you could technically get away with using all the features of the re library without compiling your regular expression strings, I highly recommend always converting a “raw” regular expression string into a compiled re object, since compiling your RegEx string into a compiled object will make all the remaining functions run much faster.\nIn the above .tsv-to-.csv example, we took the raw RegEx string tab_regex_str and compiled it using tab_regex = re.compile(tab_regex_str), so that we could then use tab_regex as a regular expression object with all of the RegEx-related Pythons callable using the . operator:\n\nmatch_result = tab_regex.match(original_file)\nprint(match_result)\nsearch_result = tab_regex.search(original_file)\nprint(search_result)\nfindall_result = tab_regex.findall(original_file)\nprint(findall_result)\n\nIf, for whatever reason, you don’t want to compile your RegEx strings, you can still call all of these functions by using the re module directly and providing a RegEx string as the first argument to the function, like\n\nmatch_result_nocompile = re.match(\n    tab_regex_str,\n    original_file\n)\nprint(match_result)\nsearch_result_nocompile = re.search(tab_regex_str, original_file)\nprint(search_result)\nfindall_result_nocompile = re.findall(tab_regex_str, original_file)\nprint(findall_result)\n\nSince I want you to get in the habit of compiling your regular expression strings, I’m going to use the object-specific syntax (e.g., my_compiled_regex.match()) rather than the global syntax (re.match()), for this and all remaining functions.\nNow let’s look at what each of these three functions does!"
  },
  {
    "objectID": "writeups/regex/index.html#binary-accept-vs.-reject-match",
    "href": "writeups/regex/index.html#binary-accept-vs.-reject-match",
    "title": "Regular Expressions for Data Cleaning",
    "section": "Binary Accept vs. Reject: match()",
    "text": "Binary Accept vs. Reject: match()\nYou can think of match() as associated with the original purpose of regular expressions: to take in a string and accept or reject that string based on whether or not it matches the pattern described by the regular expression (see Week 05 for examples). This function will return information on the match if the match was successful, or the Python “null” value None otherwise1.\nIn the above examples, we saw that in fact our RegEx string did not match the provided string original_file: this is the expected behavior, and makes sense, since .match() will only be successful if the provided string EXACTLY matches the provided regular expression. Since original_file was a whole file, with lots of different characters, the regular expression r'\\t' (which matches only a single \\t character) will not match the file. The only string that our RegEx pattern r'\\t' would actually perfectly match would be the string \"\\t\":\n\nsingle_tab_character = \"\\t\"\nprint(tab_regex.match(single_tab_character))\n\nBut, even adding a single additional character before the \\t will cause this RegEx string to no longer match2:\n\ntwo_characters = \"s\\t\"\nprint(tab_regex.match(two_characters))\n\nThis can be extremely helpful when you want to (for example) validate a column in a dataset, like a phone number column: if you write a phone number RegEx string, you can use it in conjunction with match() and Pandas to check that every string in the column is a match:\n\nimport pandas as pd\nphone_reg_str = r'[0-9]{3}-[0-9]{3}-[0-9]{4}'\nphone_reg = re.compile(phone_reg_str)\nnumber_list = ['202-123-4567','202-999-9999','301x123x1234']\nphone_df = pd.DataFrame({'phone_num': number_list})\nphone_df['match'] = phone_df['phone_num'].apply(phone_reg.match)\nphone_df\n\nAlthough I do tend to just store the .match() result in a column like this, while cleaning, if you’re going to export the dataset it’s a bit sloppy to just put the re.match object itself into a column like this (it may cause issues if you try to save it as .csv and load it on a different operating system, for example). So, a cleaner version safe for export could look like:\n\nphone_df['match'] = phone_df['phone_num'].apply(lambda x: phone_reg.match(x) is not None)\nphone_df"
  },
  {
    "objectID": "writeups/regex/index.html#searching-a-string-for-matches",
    "href": "writeups/regex/index.html#searching-a-string-for-matches",
    "title": "Regular Expressions for Data Cleaning",
    "section": "Searching a String for Matches",
    "text": "Searching a String for Matches\nThe findall() function, unlike match(), does not require that the entire string is matched by the RegEx pattern. Instead, it finds all substrings within the bigger string that match the pattern (put another way: all the substrings for which re.match() would return a match).\nThis is helpful in a different data cleaning case, where for example you may have freeform text and you want to extract all of the phone numbers written in this freeform text. For example: Imagine a dataset containing the results from a survey with a freeform text field like “Introduce Yourself!”, and you want to go through this field and extract all of the phone numbers that have been entered in this field:\n\nresponses = [\n    \"Hello my number is 202-123-4567\",\n    \"Hi thanks for the survey call me at 301-111-1111 or 925-123-1111\",\n    \"I hate this survey don't ever call 240-999-9999\",\n    \"I don't have a phone number sorry\"\n]\nsurvey_df = pd.DataFrame({\n    'response': responses\n})\nsurvey_df\n\nWe can run the the findall() function on each entry in the response column here, to extract just the phone numbers:\n\nsurvey_df['matches'] = survey_df['response'].apply(phone_reg.findall)\nsurvey_df\n\nNote the fact that findall() returns a list of matches, rather than just one match, so that if you’re expecting only one you’ll have to handle the case of multiple matches (like in row 2 of this example) as well as the case of no matches (row 4)."
  },
  {
    "objectID": "writeups/regex/index.html#smart-substitution-sub",
    "href": "writeups/regex/index.html#smart-substitution-sub",
    "title": "Regular Expressions for Data Cleaning",
    "section": "Smart Substitution: sub()",
    "text": "Smart Substitution: sub()\nOftentimes you’re not worried about analyzing the re.match objects themselves, you just want to use a RegEx to do a fancier find and replace than what’s possible using .replace(x,y) (the Python string function that finds all instances of x in a string and replaces them with y).\nFor example, in the Data Cleaning with Python video/writeup, I found that one of the counties in one dataset did not match with the same county in the other dataset, because in one the county name was abbreviate to St. Mary's while in the other the name was fully written out as Saint Mary's.\nIn that case, I was able to just use .replace(), since there was only one instance and I knew exactly what I wanted to replace with what. But, for a trickier case that regular expressions can handle while .replace() can’t, image merging one dataset where two-character abbreviations like this are followed by a period like St. Marys, with another dataset where two-character abbreviations are not followed by a period, like St Marys (this is actually fairly common, as sometimes names like this are entered into old database systems used by government agencies that don’t allow entering commas and/or apostrophes).\nTo handle this, we can write code using regular expressions that finds all two-letter abbreviations—portions of a string starting with a capital letter, followed by a lowercase letter, followed by a period—and replaces them with the same string but without the period at the end.\n\nabbrev_reg_str = r'[A-Z][a-z]\\.'\nabbrev_reg = re.compile(abbrev_reg_str)\ncounty_list = [\n    'St. Marys',\n    'Ft. Worth',\n    'Montgomery',\n    'Mt. Everest',\n    'Mt. St. Vincent'\n]\ncounty_df = pd.DataFrame({'name': county_list})\ncounty_df\n\nLike before, we can use abbrev_reg.findall() to find instances of this type of abbreviation across the dataset:\n\ncounty_df['matches'] = county_df['name'].apply(abbrev_reg.findall)\ncounty_df\n\nHowever, to be able to automatically replace the matches with a given replacement, we’ll have to dive one level deeper into regular expressions. I will talk about this more in lecture, but it boils down to: you can use parentheses within your regular expression to indicate match groups: subsets of the regular expression that you would like re to keep track of when it finds a match, and then provide a replacement string that utilizes these match groups. It is easier to explain through example than through text. In the following code, we use match groups to indicate to Python that we want to extract just the country code from an entire phone number:\n\narea_code_reg_str = r'(?P&lt;county_code&gt;[+][0-9]{1,3})-[0-9]{3}-[0-9]{3}-[0-9]{4}'\narea_code_reg = re.compile(area_code_reg_str)\nmy_num = '+970-202-111-1111'\nmatch_result = area_code_reg.match(my_num)\nprint(match_result)\n\nSo, we see that we have a match, but we took the extra step of putting parentheses around the part of the match that we specifically wanted, and telling Python to call this part county_code, so now we can use the .groupdict() function of the re.match object to specifically extract just this subset of the full match:\n\nmatch_result.groupdict()\n\nWe can also directly use the captured groups within the sub() method, like\n\narea_code_reg.sub(r'Country code \\1', my_num)\n\nBy applying this same logic to the abbreviation matches above, therefore, we"
  },
  {
    "objectID": "writeups/regex/index.html#footnotes",
    "href": "writeups/regex/index.html#footnotes",
    "title": "Regular Expressions for Data Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou have to be extremely careful when running RegEx functions (or any functions/libraries where functions can return None) in Jupyter notebooks for this reason: the way Jupyter is set up, if the last line of code in a cell has the value None, Jupyter just won’t display anything at all when you run that cell, even if the execution of the cell was totally successful (i.e., even if there were no errors). That’s why—as in the above examples—I try to remember to always print() the results of RegEx functions in Python, rather than just expecting Jupyter to automatically display their result.↩︎\nTo reiterate the point about None: if hadn’t used print() in this case, to explicitly tell Jupyter to print the result of the last line, this cell wouldn’t have produced any output.↩︎"
  },
  {
    "objectID": "writeups/python-install/index.html",
    "href": "writeups/python-install/index.html",
    "title": "Managing Python Installations",
    "section": "",
    "text": "This writeup is intended to help students who are having issues with multiple Python installations existing on their computer at the same time. This can give rise to issues when trying to install and use third-party libraries (meaning, libraries which are not built into Python).\nFor example, you might encounter the following issue:\nIf you have encountered this issue, there is a short-term solution which can sometimes fix it temporarily (if you really need to run the Python code right now), but it indicates that something is wrong with the way Python, Conda, and/or Jupyter are set up on your system1, and you should really take the time to run through the sustainable solution later on in the writeup."
  },
  {
    "objectID": "writeups/python-install/index.html#the-short-term-temporary-solution",
    "href": "writeups/python-install/index.html#the-short-term-temporary-solution",
    "title": "Managing Python Installations",
    "section": "The Short-Term, Temporary Solution",
    "text": "The Short-Term, Temporary Solution\nIf you are panicking about this issue, and you really need to run the Python code right now, the following approach will temporarily “fix” the issue maybe 95% of the time:\nAdd a cell to your notebook (as in, don’t run this command in the terminal) containing the following line, above the cell where you are trying to import numpy for example, and execute this cell. In 95% of cases, this will allow you to import numpy as np and continue with your work:\n!pip install numpy"
  },
  {
    "objectID": "writeups/python-install/index.html#crawling-towards-a-longer-term-sustainable-solution",
    "href": "writeups/python-install/index.html#crawling-towards-a-longer-term-sustainable-solution",
    "title": "Managing Python Installations",
    "section": "Crawling Towards a Longer-Term, Sustainable Solution",
    "text": "Crawling Towards a Longer-Term, Sustainable Solution\nThis type of issue can be pretty tough to fix completely, so that it never happens again, but in most cases I think we can minimize the likelihood of it happening and affecting our work if we understand why it happens.\nIn general, the issue comes from some number of “mismatches” between versions of the Python interpreter, on the one hand (on Windows, for example, this is a program literally just called python.exe), and things that use versions of Python on the other (Jupyter, for example, and/or VSCode). Ensuring that all of the different versions of Python are “synced up” correctly with these things that use Python (Jupyter, VSCode) gets even exponentially trickier when you create environments within a given Python installation, since now you have to ensure that Jupyter is running the commands you type into a cell within the correct environment of the correct version of Python that you want it to use.\nSo, if you are just starting with Python, I highly recommend starting from the following barebones setup (we will learn what a lot of these terms mean, and how to “revert” to this setup if you already have a different one, in the next section):\n\nA single version of Python installed to a particular path on your computer that you write down somewhere, so that you know exactly which Python interpreter is being called when you type the python command in your terminal, for example.\n\nOn Unix/Linux/MacOS, you can find out what executable at what path the terminal is running when you issue the command python by issuing the following command:\n\nwhich python\nor (if that produces the output python not found)\nwhich python3\nThis should output a filepath which tells you the full pathway to the particular python executable that your terminal runs whenever you type python (or python3). When I use UTM to create a brand-new Mac OSX installation without changing any settings, for example, it says python not found if I execute which python in the Terminal, but produces the following output if I excute which python3:\n/usr/bin/python3\nThis says, literally, that whenever I type python3 into the Terminal, it immediately goes into the root directory of my computer (/), then goes into a subfolder called usr, then a subfolder of /usr called bin, within which there is an executable (program) called just python (without the .exe, which is what the program would be named on Windows).\nDon’t install conda until you are comfortable with the basics of (a) how to run .py files, (b) how to issue commands in an python shell (which is the program you see running, with a prompt at the bottom of your Terminal screen starting with &gt;&gt;&gt; into which you can type Python commands, if you just open the Terminal and execute the command python or python3 without any additional arguments—see Figure 1), and (c) how to link your Jupyter notebooks with the particular version of Python you have installed\n\nI know that this goes against what most data science courses/teachers suggest, and I’m happy to have my mind changed, but I suggest this because Installing conda means installing a second Python installation onto your computer: if you have a Mac this is for sure the case, since Macs actually come with Python pre-installed, and if you have Windows it is not always true but might be true, since some versions of Windows also come with Python pre-installed. For many of the students who I worked with on the above issues it turned out (after lots of digging) that their Jupyter or VSCode setup was using the Anaconda version of Python, while executing the python or pip command in their Terminal led OSX to run a completely different (non-conda) version of Python (usually the one that comes shipped with OSX).\n\n\n\n\n\nFigure 1: The python shell, which runs if you open a Terminal window and execute the command python or python3 without any additional arugments. To exit this shell and return to the “normal” Terminal prompt, execute the Python command quit() within this Python shell, as shown in Figure 2\n\n\n\n\n\nFigure 2: The same Terminal session as shown in Figure 1, but where I have now executed a Python command (1+1) and quit the Python shell using quit(), returning me back to my Terminal (shell), where I can issue Unix commands at the prompt (after the % portion at the bottom) like ls or cd."
  },
  {
    "objectID": "writeups/python-install/index.html#windows-specifics",
    "href": "writeups/python-install/index.html#windows-specifics",
    "title": "Managing Python Installations",
    "section": "Windows Specifics",
    "text": "Windows Specifics\nOpening my Windows 11 Virtual Machine, I find that Python does not seem to be pre-installed (again, it can differ based on what specific version of Windows you have, or what company you buy your PC from, in my experience), since neither the python or python3 commands work when I open PowerShell. So, I went ahead and installed Python directly from the Python website (rather than the Anaconda version, for example), and it installed Python in the following location (my username on the VM is dsanstudent, so on your installation wherever you see dsanstudent replace that with your Windows username):\nC:\\Users\\dsanstudent\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\nWhich is pretty long and difficult to remember, plus by default it did not add this to my Windows PATH Environment Variable, so I had to do it manually (tldr: press start, type “Environment Variables”, and use the UI that pops up to edit the PATH string, adding C:\\Users\\dsanstudent\\AppData\\Local\\Programs\\Python\\Python311 to the end of it). See Figure 3 for a screenshot of the contents of this location when I browse to this path using Windows Explorer.\n\n\n\nFigure 3: The path to which, by default, the Python installer installed the python.exe executable, when I downloaded and installed Python version 3.11 on a brand new Windows 11 Virtual Machine."
  },
  {
    "objectID": "writeups/python-install/index.html#so-what-are-all-these-different-moving-parts",
    "href": "writeups/python-install/index.html#so-what-are-all-these-different-moving-parts",
    "title": "Managing Python Installations",
    "section": "So What Are All These Different Moving Parts?",
    "text": "So What Are All These Different Moving Parts?\nA lot of confusion can also be avoided if we always keep in mind the basics of what each “piece” of the Python development environment actually is/actually does:\n\nPython\nAt its core, Python is just an executable program (hence the python.exe filename in Windows) that takes in Python files (.py files) and runs them. Anything beyond this can be thought of as an extension that is built on top of this core functionality.\n\n\npip\nAs external, third-party libraries like NumPy, Pandas, Jupyter, and so on became more and more central to the Python development process, the people who created Python also created pip, which stands for “pip Installs Packages”2.\npip, sadly, can also become “delinked” from the version of Python you want to use: if you have both Python 3.9 and Python 3.11 installed on your computer, for example, you need to be careful about which of these installations is having packages installed into it when you run pip install (and similarly for pip3).\n\n\nAnaconda\nAnaconda is a third-party package manager for Python, which works a lot better than pip in many ways, and comes with lots of nice data science packages built-in, but has the drawback that it exists somewhere on your computer separately from the “standard” Python installation if (a) you bought a Mac/PC with Python pre-installed, or (b) you downloaded Python from the python.org website.\n\n\nJupyter\nJupyter is essentially a wrapper around Python (whether “normal” Python or the Anaconda version of Python, or both), which takes the Python code that you enter into code cells and runs them within a Python kernel—basically an instance of the Python executable that sits and waits for commands to come in from Jupyter, runs them, and sends the output of the commands back to Jupyter so that Jupyter can show that output in your notebook, underneath the code cell.\n\n\nVSCode\nVSCode: Just like how Python is just a .py-file-runner at its core, with fancy stuff built on top of it, VSCode is just a text editor at its core, with fancy stuff built on top of it (especially once you add extensions, like the Python or Jupyter or Quarto extensions we’re using in this class)."
  },
  {
    "objectID": "writeups/python-install/index.html#summary",
    "href": "writeups/python-install/index.html#summary",
    "title": "Managing Python Installations",
    "section": "Summary",
    "text": "Summary\nThere is no deterministic fix-all algorithm for making sure that (e.g.) the version of Python used as a kernel in Jupyter is the same as the version of Python used when you click the “run file” button at the top of a .py file in VSCode, or that either of these are the same as the version of Python used when you execute the python command in your Terminal, for example, but I hope that maybe something I’ve included here can help you navigate this undeniably awful, headache-inducing, messy setup 🥴 Really, the best thing you can do if you keep running into these issues is to email me or schedule an office hours appointment, and we can work through the issue together 😸"
  },
  {
    "objectID": "writeups/python-install/index.html#footnotes",
    "href": "writeups/python-install/index.html#footnotes",
    "title": "Managing Python Installations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt doesn’t mean you did something wrong! It’s just a result of the way that programs/apps get installed on a computer: despite the fact that computers have now existed for many decades, there is still no easy way for operating systems to keep track of (e.g.) which version/type of Python you’re hoping to run when you type python in your terminal, or when you run a command in Jupyter.↩︎\nComputer scientists love recursive acronyms like this: if you’ve heard of the language PHP, that stands for PHP: Hypertext Preprocessor 🤓↩︎"
  },
  {
    "objectID": "writeups/columns/index.html",
    "href": "writeups/columns/index.html",
    "title": "Multiple-Column Layout in Quarto",
    "section": "",
    "text": "The following Quarto code:\n\n\n\n\n\n\nQuarto Code Using ::: columns\n\n\n\nHello this is my document\n\n::: columns\n\n::: {.column width=\"50%\"}\n\nHere is column 1\n\n:::\n::: {.column width=\"50%\"}\n\nHere is column 2\n\n:::\n\n:::\n\n\n\nProduces the following rendered output:\n\n\n\n\n\n\nRendered Output Using ::: columns\n\n\n\nHello this is my document\n\n\nHere is column 1\n\nHere is column 2"
  },
  {
    "objectID": "about/slides.html#prof.-jeff-introduction",
    "href": "about/slides.html#prof.-jeff-introduction",
    "title": "About Me",
    "section": "Prof. Jeff Introduction!",
    "text": "Prof. Jeff Introduction!\n\nBorn and raised in NW DC → high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "about/slides.html#grad-school",
    "href": "about/slides.html#grad-school",
    "title": "About Me",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/北大) → internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "about/slides.html#dissertation-political-science-history",
    "href": "about/slides.html#dissertation-political-science-history",
    "title": "About Me",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n“Our Word is Our Weapon”: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "about/slides.html#research-labor-economics",
    "href": "about/slides.html#research-labor-economics",
    "title": "About Me",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n“Monopsony in Online Labor Markets”: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n“Freedom as Non-Domination in the Labor Market”: Game-theoretic models of workers’ rights (monopsony vs. labor discipline)\n\n\n\n\n\n“Unsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements”: Linguistic (dependency) parses of contracts → time series of worker vs. employer rights and responsibilities over time"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About Me",
    "section": "",
    "text": "Born and raised in NW DC → high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "about/index.html#prof.-jeff-introduction",
    "href": "about/index.html#prof.-jeff-introduction",
    "title": "About Me",
    "section": "",
    "text": "Born and raised in NW DC → high school in Rockville, MD\nUniversity of Maryland: Computer Science, Math, Economics (2008-2012)"
  },
  {
    "objectID": "about/index.html#grad-school",
    "href": "about/index.html#grad-school",
    "title": "About Me",
    "section": "Grad School",
    "text": "Grad School\n\nStudied abroad in Beijing (Peking University/北大) → internship with Huawei in Hong Kong (HKUST)\n\n\n\n\nStanford for MS in Computer Science (2012-2014)\nResearch Economist at UC Berkeley (2014-2015)\n\n\n\n\n\n\nColumbia (NYC) for PhD[+Postdoc] in Political Science (2015-2023)"
  },
  {
    "objectID": "about/index.html#dissertation-political-science-history",
    "href": "about/index.html#dissertation-political-science-history",
    "title": "About Me",
    "section": "Dissertation (Political Science + History)",
    "text": "Dissertation (Political Science + History)\n“Our Word is Our Weapon”: Text-Analyzing Wars of Ideas from the French Revolution to the First Intifada"
  },
  {
    "objectID": "about/index.html#research-labor-economics",
    "href": "about/index.html#research-labor-economics",
    "title": "About Me",
    "section": "Research (Labor Economics)",
    "text": "Research (Labor Economics)\n\n\n\n“Monopsony in Online Labor Markets”: Machine Learning to enhance causal estimates of the effect of job description language on uptake rate\n\n\n\n“Freedom as Non-Domination in the Labor Market”: Game-theoretic models of workers’ rights (monopsony vs. labor discipline)\n\n\n\n\n\n“Unsupervised Extraction of Workplace Rights and Duties from Collective Bargaining Agreements”: Linguistic (dependency) parses of contracts → time series of worker vs. employer rights and responsibilities over time"
  },
  {
    "objectID": "extra-videos/index.html",
    "href": "extra-videos/index.html",
    "title": "Extra Videos",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Last Updated - Oldest\n        \n         \n          Last Updated - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nLast Updated\n\n\n\n\n\n\nExploratory Data Analysis of the THOR Dataset in Seaborn\n\n\nSunday Sep 30, 2029\n\n\n\n\nData Cleaning in Python\n\n\nTuesday Sep 19, 2023\n\n\n\n\nWindows: SSH and SCP on Georgetown Domains\n\n\nFriday Sep 15, 2023\n\n\n\n\nQuarto Websites\n\n\nThursday Sep 14, 2023\n\n\n\n\nSetting Up VSCode with Quarto\n\n\nThursday Sep 14, 2023\n\n\n\n\nSSH and SCP on Georgetown Domains\n\n\nThursday Sep 14, 2023\n\n\n\n\nGit and GitHub\n\n\nWednesday Sep 13, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "w01/index.html",
    "href": "w01/index.html",
    "title": "Week 1: Welcome to DSAN 5000!",
    "section": "",
    "text": "(Week 1 of DSAN 5000 was a joint session across all individual sections, taught on Zoom.)\n\n\n\n\n\n\nToday’s Links\n\n\n\n\nWeek 1 Lecture Notes\nLab 0 Instructions\nWeek 1 Lecture Recording"
  }
]