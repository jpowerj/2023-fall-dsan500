<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-11-03">

<title>DSAN 5000 – dsan5000_feature_selection_full</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-ETYTX4E16L"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-ETYTX4E16L', { 'anonymize_ip': true});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://code.jquery.com/jquery-3.7.1.slim.min.js"></script><script src="../../jjcustom.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-dataset" id="toc-the-dataset" class="nav-link active" data-scroll-target="#the-dataset">(1) The Dataset</a></li>
  <li><a href="#the-supervised-learning-task" id="toc-the-supervised-learning-task" class="nav-link" data-scroll-target="#the-supervised-learning-task">(2) The Supervised Learning Task</a></li>
  <li><a href="#machine-learning-without-feature-selection" id="toc-machine-learning-without-feature-selection" class="nav-link" data-scroll-target="#machine-learning-without-feature-selection">(3) Machine Learning <em>Without</em> Feature Selection</a></li>
  <li><a href="#baseline-1-random-guessing" id="toc-baseline-1-random-guessing" class="nav-link" data-scroll-target="#baseline-1-random-guessing">(4) Baseline 1: Random Guessing</a></li>
  <li><a href="#baseline-2-guessing-the-most-frequent-label" id="toc-baseline-2-guessing-the-most-frequent-label" class="nav-link" data-scroll-target="#baseline-2-guessing-the-most-frequent-label">(5) Baseline 2: Guessing the Most Frequent Label</a></li>
  <li><a href="#using-an-off-the-shelf-machine-learning-model" id="toc-using-an-off-the-shelf-machine-learning-model" class="nav-link" data-scroll-target="#using-an-off-the-shelf-machine-learning-model">(6) Using an “Off-The-Shelf” Machine Learning Model</a>
  <ul class="collapse">
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization">(6.1) Normalization</a></li>
  <li><a href="#training-data-vs.-test-data" id="toc-training-data-vs.-test-data" class="nav-link" data-scroll-target="#training-data-vs.-test-data">(6.2) Training Data vs.&nbsp;Test Data</a></li>
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">(6.3) Training the Model</a></li>
  <li><a href="#evaluating-the-model" id="toc-evaluating-the-model" class="nav-link" data-scroll-target="#evaluating-the-model">(6.4) Evaluating the Model</a></li>
  </ul></li>
  <li><a href="#augmenting-our-off-the-shelf-model-with-feature-selection" id="toc-augmenting-our-off-the-shelf-model-with-feature-selection" class="nav-link" data-scroll-target="#augmenting-our-off-the-shelf-model-with-feature-selection">(7) Augmenting our “Off-The-Shelf” Model With Feature Selection</a>
  <ul class="collapse">
  <li><a href="#the-base-merit-score-s_2" id="toc-the-base-merit-score-s_2" class="nav-link" data-scroll-target="#the-base-merit-score-s_2">(7.1) The Base Merit Score <span class="math inline">\(S_2\)</span></a></li>
  <li><a href="#the-augmented-merit-score-s_3" id="toc-the-augmented-merit-score-s_3" class="nav-link" data-scroll-target="#the-augmented-merit-score-s_3">(7.2) The Augmented Merit Score <span class="math inline">\(S'_3\)</span></a></li>
  <li><a href="#the-augmented-merit-score-s_5" id="toc-the-augmented-merit-score-s_5" class="nav-link" data-scroll-target="#the-augmented-merit-score-s_5">(7.3) The Augmented Merit Score <span class="math inline">\(S'_5\)</span></a></li>
  <li><a href="#putting-the-three-scores-together" id="toc-putting-the-three-scores-together" class="nav-link" data-scroll-target="#putting-the-three-scores-together">(7.4) Putting the Three Scores Together</a></li>
  </ul></li>
  <li><a href="#appendix-possibly-helpful-code" id="toc-appendix-possibly-helpful-code" class="nav-link" data-scroll-target="#appendix-possibly-helpful-code">Appendix: Possibly Helpful Code</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Feature Selection in Python</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Extra Writeups</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 3, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled" title="Colab Link">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Colab Link
</div>
</div>
<div class="callout-body-container callout-body">
<center>
<p><a href="https://colab.research.google.com/drive/125YQ7Wqo5vQG4ko3NqUs8EvYU6hmbEul" target="_blank"><img src="images/colab-badge.svg" class="img-fluid"><br>Click here to <strong>open in Colab</strong></a></p>
</center>
</div>
</div>
<p>Since my last two data adventures were focused on <a href="https://jjacobs.me/dsan5000/463a01339cf0f456ba54a1849df50d1a22c247e3/writeups/data-cleaning/clean_data.html" target="_blank">public health data</a> and <a href="https://jjacobs.me/dsan5000/463a01339cf0f456ba54a1849df50d1a22c247e3/writeups/eda-seaborn/THOR_EDA_with_Seaborn.html" target="_blank">international relations data</a>, this week I’m going to focus on the topic that is tied for the most common topic students are doing for their projects, namely, <strong>sports data</strong>!</p>
<p>This time, rather than worrying about cleaning the data or exploring the data, we will “fast forward” to a particular task: <strong>using</strong> the data to accomplish a task, namely, <strong>predicting success in the NBA (National Basketball Association)</strong>.</p>
<p>I chose the NBA because, unlike the NFL (National Football League) and the MLB (Major League Baseball) for example, the NBA is popular both inside <strong>and</strong> outside the US. But, if you don’t know anything about the NBA don’t worry, I will do my best to explain what’s going on at each point in the lab (and, as we’ll see in a moment, there is a <strong>codebook</strong> providing full explanations for each variable)</p>
<section id="the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-dataset">(1) The Dataset</h2>
<p>For this lab we’ll be analyzing a <strong>panel dataset</strong> where each row represents a <strong>(team, season) pair</strong>.</p>
<p>A complicating factor in analyzing NBA data is that, unlike other sports like baseball, each NBA season spans <strong>two years</strong>: meaning, for example, <a href="https://en.wikipedia.org/wiki/2022%E2%80%9323_NBA_season" target="_blank">the most recent season</a> began in 2022 but ended in 2023. To make things easier for us, then, rather than recording seasons as two-year spans, I have simplified the dataset so that the <strong>year</strong> for each row represents the year in which the <strong>playoffs</strong> for that season occurred. To make this as clear as possible when examining the dataset, I’ve named the year variable <code>po_year</code> (for <strong>playoff year</strong>).</p>
<p>Like we did for the <a href="https://jjacobs.me/dsan5000/463a01339cf0f456ba54a1849df50d1a22c247e3/writeups/eda-seaborn/THOR_EDA_with_Seaborn.html" target="_blank">exploratory data analysis lab</a>, let’s first <strong>download the dataset using the <code>requests</code> library</strong> to the same folder as this notebook, so that we don’t have to re-download the dataset every time we want to use it (meaning, once you run the following cell <strong>once</strong>, you don’t need to run it again, even if you restart the notebook)</p>
<div id="cell-4" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>data_fname <span class="op">=</span> <span class="st">"nba_team_data.csv"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nba_url <span class="op">=</span> <span class="ss">f"https://jpj.georgetown.domains/dsan5000-scratch/feature-selection/</span><span class="sc">{</span>data_fname<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(data_fname, <span class="st">'wb'</span>) <span class="im">as</span> outfile:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  data_content <span class="op">=</span> requests.get(nba_url, stream<span class="op">=</span><span class="va">True</span>).content</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  outfile.write(data_content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And one more cell, this time downloading the <strong>codebook</strong> for the dataset:</p>
<div id="cell-6" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>codebook_fname <span class="op">=</span> <span class="st">"nba_team_codebook.csv"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>codebook_url <span class="op">=</span> <span class="ss">f"https://jpj.georgetown.domains/dsan5000-scratch/feature-selection/</span><span class="sc">{</span>codebook_fname<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(codebook_fname, <span class="st">'wb'</span>) <span class="im">as</span> outfile:</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  codebook_content <span class="op">=</span> requests.get(codebook_url, stream<span class="op">=</span><span class="va">True</span>).content</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  outfile.write(codebook_content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>You should now have a file called <code>nba_team_data.csv</code> within the same folder as this notebook. Let’s open the data file using Pandas and examine the first few rows:</p>
<div id="cell-8" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-9" class="cell" data-outputid="02087e30-1e03-4728-b50c-db54a61b3413" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>nba_df <span class="op">=</span> pd.read_csv(<span class="st">"nba_team_data.csv"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>nba_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year_team</th>
<th data-quarto-table-cell-role="th">po_year</th>
<th data-quarto-table-cell-role="th">team_full</th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">Ast</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
<th data-quarto-table-cell-role="th">playoffs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998_Atlanta Hawks</td>
<td>1998</td>
<td>Atlanta Hawks</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>19.0</td>
<td>8.0</td>
<td>5.9</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1998_Boston Celtics</td>
<td>1998</td>
<td>Boston Celtics</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>22.1</td>
<td>12.0</td>
<td>4.5</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1998_Charlotte Hornets</td>
<td>1998</td>
<td>Charlotte Hornets</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>23.4</td>
<td>8.2</td>
<td>3.7</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1998_Chicago Bulls</td>
<td>1998</td>
<td>Chicago Bulls</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>23.1</td>
<td>8.6</td>
<td>4.3</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1998_Cleveland Cavaliers</td>
<td>1998</td>
<td>Cleveland Cavaliers</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>22.9</td>
<td>9.9</td>
<td>5.0</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
<td>True</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>As we’ve talked about in previous lectures, you can use your Data Terminator vision to begin understanding “what’s going on” in this dataset by</p>
<ol type="1">
<li>Sweeping your eyes over each <strong>row</strong>, to figure out what the rows represent,</li>
<li>Sweeping your eyes over each <strong>column</strong>, to figure out what the columns represent,</li>
<li>Examining whether there is a <strong>unique ID</strong> for each observation, and then</li>
<li>(If the data is already tidy) looking at individual <strong>values</strong> to determine what they represent: for example, by looking at a value within the <code>mean_age</code> column, can we infer what the <strong>units</strong> of this column are? (In this case, the answer is <strong>years</strong>).</li>
</ol>
<p>Now let’s look at the <strong>codebook</strong>:</p>
<div id="cell-12" class="cell" data-outputid="f03de50d-6c54-4d10-94a8-7ef633646cbc" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>codebook_df <span class="op">=</span> pd.read_csv(<span class="st">"nba_team_codebook.csv"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>codebook_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">variable</th>
<th data-quarto-table-cell-role="th">description</th>
<th data-quarto-table-cell-role="th">type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>year_team</td>
<td>Unique identifier of the form {po_year}_{team_...</td>
<td>str</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>po_year</td>
<td>The year that the season's playoffs took place...</td>
<td>int</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>team_full</td>
<td>Full name of the team</td>
<td>str</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>mean_age</td>
<td>Mean age of the team's members that year, in y...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>mean_height</td>
<td>Mean height of the team's members that year, i...</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>mean_weight</td>
<td>Mean weight of the team's members that year, i...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>PtsSeason</td>
<td>The team's average points per game that year</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>OppPtsSeason</td>
<td>The average number of points scored by the tea...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>PtsDiff</td>
<td>The difference between PtsSeason and OppPtsSeason</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>Reb</td>
<td>The team's average number of rebounds per game...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>Ast</td>
<td>The team's average number of assists per game ...</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>Stl</td>
<td>The team's average number of steals per game t...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>Blk</td>
<td>The team's average number of blocks per game t...</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>To</td>
<td>The team's average number of turnovers per gam...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>Pf</td>
<td>The team's average number of personal fouls pe...</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>Dreb</td>
<td>The team's average number of defensive rebound...</td>
<td>float</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>Oreb</td>
<td>The team's average number of offensive rebound...</td>
<td>float</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>playoffs</td>
<td>TRUE if the team made the playoffs that year, ...</td>
<td>bool</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>We can use this codebook to obtain a <strong>description</strong> for any variable we don’t understand. For example, if we don’t know what the <code>OppPtsSeason</code> variable means, we can use Pandas’ <code>.loc[]</code> syntax to obtain just the row in the codebook corresponding to this variable, and examine the full <code>description</code> value in this row:</p>
<div id="cell-14" class="cell" data-outputid="d889e6c1-1a9a-4761-a516-56d7f1b33796" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>var_row <span class="op">=</span> codebook_df.loc[codebook_df[<span class="st">'variable'</span>] <span class="op">==</span> <span class="st">"OppPtsSeason"</span>,<span class="st">"description"</span>]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>var_row</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>7    The average number of points scored by the tea...
Name: description, dtype: object</code></pre>
</div>
</div>
<p>Note that this is not entirely satisfactory, since the full <code>description</code> value is cut off—this happens because the <code>.loc[]</code> syntax returns a <code>pd.Series</code> object containing <strong>all matches</strong>. In this case, since we know there is only going to be <strong>one</strong> match, we can take the result of the <code>.loc[]</code> call and extract just the first (and only) match by extracting <code>.iloc[0]</code>:</p>
<div id="cell-16" class="cell" data-outputid="2a6cb56e-c82e-44d0-ce5e-c3463de7bbb2" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>var_row.iloc[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>"The average number of points scored by the team's opponents that year"</code></pre>
</div>
</div>
<p>And we see that now, rather than a <code>pd.Series</code> object, we have the full <strong>string</strong> description of the variable we wanted a description for.</p>
<blockquote class="blockquote">
<p><strong>Quick aside: <code>DataFrame</code> to <code>dict</code></strong><br><br>Given how much work it took just now to extract the description for <code>OppPtsSeason</code>, you may have a hunch that a <code>pd.DataFrame</code> is probably <strong>not</strong> the most efficient format for storing a codebook.<br><br>For this reason, when I am working with codebooks, I usually convert the <code>pd.DataFrame</code> containing the codebook into a more straightforward <strong>Python dictionary</strong> object.</p>
</blockquote>
<div id="cell-19" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>codebook_dict <span class="op">=</span> codebook_df.set_index(<span class="st">'variable'</span>).to_dict(orient<span class="op">=</span><span class="st">'index'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<p>Now we can more easily check the information we have about a variable: <code>codebook_dict</code> is a set of key-value pairs, where the keys are the <strong>variable names</strong> and the values are the <strong>information</strong> we have about that variable. Now, for example, to access all of the information we have about the <code>Blk</code> variable, we can just run the following line of code:</p>
</blockquote>
<div id="cell-21" class="cell" data-outputid="069af334-1f88-4045-b865-4551b9b4e195" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>codebook_dict[<span class="st">'Blk'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>{'description': "The team's average number of blocks per game that year",
 'type': 'float'}</code></pre>
</div>
</div>
</section>
<section id="the-supervised-learning-task" class="level2">
<h2 class="anchored" data-anchor-id="the-supervised-learning-task">(2) The Supervised Learning Task</h2>
<p>Now that we have a feel for the data, I can reveal the <strong>task</strong> we’re hoping to accomplish by examining this data:</p>
<blockquote class="blockquote">
<p><strong>Our aim is to develop a <em>model</em> of team-level “success” in the NBA, by finding what <em>properties</em> of a (team,season) pair best predict whether or not the team <em>makes the playoffs</em> that year.</strong></p>
</blockquote>
<p>We can gain some initial intuition around this task by, for example, creating a <strong>cross-tabulation</strong> showing how particular variables differ between teams that <strong>did</strong> and <strong>did not</strong> make the playoffs in a given year.</p>
<p><em>(Note that this is <strong>not</strong> a very effective way of using cross-tabulation, since it is very rare for two (team,season) pairs to have the exact same average number of points, but it suffices here to show a general pattern)</em></p>
<div id="cell-23" class="cell" data-outputid="85ca8dfd-9b77-490d-ab0e-a0d44fe77f3b" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pd.crosstab(nba_df[<span class="st">'PtsSeason'</span>], nba_df[<span class="st">'playoffs'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">81.9</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">84.2</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">84.8</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">85.4</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">85.6</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">117.0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">117.7</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">117.8</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">118.1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">118.7</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>245 rows × 2 columns</p>
</div>
</div>
</div>
</div>
<p>From this cross-tabulation, we can already see an interesting aspect of the data: looking at the <strong>first five rows</strong> of the cross-tabulation, we can see that</p>
<blockquote class="blockquote">
<p>For every (team,season) pair where the team scored <strong>85.6 points per game or less</strong>, that team <strong>failed to make the playoffs</strong> in that season.</p>
</blockquote>
<p>Then, looking at the <strong>last five rows</strong> of the cross-tabulation, we can see that</p>
<blockquote class="blockquote">
<p>For every (team,season) pair where the team scored <strong>117.0 points per game or more</strong>, that team <strong>successfully made the playoffs</strong> in that season.</p>
</blockquote>
<p>Therefore, if we consider this as an EDA step, we may start to formulate a hypothesis that <strong>scoring more points per game increases the likelihood of making the playoffs</strong>. This may seem like an intuitive/obvious point, but remember that the point of EDA is for us to <strong>probe</strong> and <strong>challenge</strong> our preexisting (<strong>prior!</strong>) intuitions, seeing whether or not they hold or fail to hold when we actually look at the data, and updating our priors accordingly.</p>
<p>Now, to make sure that we’re not relying on <strong>engineer’s induction</strong> here—meaning, to check whether this pattern that we see in the first 5 and last 5 rows actually holds across the entire dataset—let’s make a new dummy variable <code>above_mean_pts</code> which is equal to <strong>1</strong> if the team’s points per game in a given season was <strong>above</strong> the mean points per game across all teams in that season, and equal to <strong>0</strong> otherwise.</p>
<p>As a way to help us compute this variable, and to show the usefulness of a topic from lecture, let’s first compute the <strong><span class="math inline">\(z\)</span>-score</strong> for each (team,season) pair: that is, compute a new column containing the score <span class="math inline">\(z_{i,t}\)</span> for team <span class="math inline">\(i\)</span> in season <span class="math inline">\(t\)</span> as</p>
<p><span class="math display">\[
z_{i,t} = \frac{\texttt{pts}_{i,t} - \mu_t}{\sigma_t},
\]</span></p>
<p>where <span class="math inline">\(\mu_t\)</span> represents the <strong>mean</strong> of all teams’ points per game stat in season <span class="math inline">\(t\)</span>, and <span class="math inline">\(\sigma_t\)</span> represents the <strong>standard deviation</strong> of all teams’ points per game stat in season <span class="math inline">\(t\)</span>:</p>
<div id="cell-25" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>compute_z_score <span class="op">=</span> <span class="kw">lambda</span> x: (x <span class="op">-</span> x.mean()) <span class="op">/</span> x.std()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'season_pts_z_score'</span>] <span class="op">=</span> nba_df.groupby(<span class="st">'po_year'</span>)[<span class="st">'PtsSeason'</span>].transform(compute_z_score)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-26" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">#compute_z_score = lambda x: (x - x.mean()) / x.std()</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co">#nba_df.insert(0, 'pts_z_score', nba_df.groupby('po_year')['PtsSeason'].transform(compute_z_score))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-27" class="cell" data-outputid="7563f70c-1e7e-45ac-f85f-60343174c9fa" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>nba_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year_team</th>
<th data-quarto-table-cell-role="th">po_year</th>
<th data-quarto-table-cell-role="th">team_full</th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">Ast</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">season_pts_z_score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998_Atlanta Hawks</td>
<td>1998</td>
<td>Atlanta Hawks</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>19.0</td>
<td>8.0</td>
<td>5.9</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
<td>True</td>
<td>0.082901</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1998_Boston Celtics</td>
<td>1998</td>
<td>Boston Celtics</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>22.1</td>
<td>12.0</td>
<td>4.5</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
<td>False</td>
<td>0.082901</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1998_Charlotte Hornets</td>
<td>1998</td>
<td>Charlotte Hornets</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>23.4</td>
<td>8.2</td>
<td>3.7</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
<td>True</td>
<td>0.261932</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1998_Chicago Bulls</td>
<td>1998</td>
<td>Chicago Bulls</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>23.1</td>
<td>8.6</td>
<td>4.3</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
<td>True</td>
<td>0.287508</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1998_Cleveland Cavaliers</td>
<td>1998</td>
<td>Cleveland Cavaliers</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>22.9</td>
<td>9.9</td>
<td>5.0</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
<td>True</td>
<td>-0.786678</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Now that we’ve applied this transformation to <code>PtsSeason</code>, given what we know about the <span class="math inline">\(z\)</span>-score, we know that teams whose points per game stat was <strong>above the average</strong> for all teams in that season will have a <strong>positive</strong> <span class="math inline">\(z\)</span>-score, and teams whose points per game stat was <strong>below the average</strong> will have a <strong>negative</strong> <span class="math inline">\(z\)</span>-score. Therefore, we can compute our desired <code>above_mean_pts</code> variable by just checking whether or not the <span class="math inline">\(z\)</span>-score is positive:</p>
<div id="cell-29" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'above_mean_pts'</span>] <span class="op">=</span> nba_df[<span class="st">'season_pts_z_score'</span>] <span class="op">&gt;</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-30" class="cell" data-outputid="202c36ea-dce4-4980-a101-9c9e98d6527a" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>nba_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year_team</th>
<th data-quarto-table-cell-role="th">po_year</th>
<th data-quarto-table-cell-role="th">team_full</th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">Ast</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">season_pts_z_score</th>
<th data-quarto-table-cell-role="th">above_mean_pts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998_Atlanta Hawks</td>
<td>1998</td>
<td>Atlanta Hawks</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>19.0</td>
<td>8.0</td>
<td>5.9</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
<td>True</td>
<td>0.082901</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1998_Boston Celtics</td>
<td>1998</td>
<td>Boston Celtics</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>22.1</td>
<td>12.0</td>
<td>4.5</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
<td>False</td>
<td>0.082901</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1998_Charlotte Hornets</td>
<td>1998</td>
<td>Charlotte Hornets</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>23.4</td>
<td>8.2</td>
<td>3.7</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
<td>True</td>
<td>0.261932</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1998_Chicago Bulls</td>
<td>1998</td>
<td>Chicago Bulls</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>23.1</td>
<td>8.6</td>
<td>4.3</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
<td>True</td>
<td>0.287508</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1998_Cleveland Cavaliers</td>
<td>1998</td>
<td>Cleveland Cavaliers</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>22.9</td>
<td>9.9</td>
<td>5.0</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
<td>True</td>
<td>-0.786678</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">678</td>
<td>2020_Sacramento Kings</td>
<td>2020</td>
<td>Sacramento Kings</td>
<td>25.117647</td>
<td>199.763529</td>
<td>98.242691</td>
<td>110.1</td>
<td>112.1</td>
<td>-2.0</td>
<td>42.6</td>
<td>23.8</td>
<td>7.7</td>
<td>4.1</td>
<td>13.7</td>
<td>22.2</td>
<td>32.9</td>
<td>9.7</td>
<td>False</td>
<td>-0.411031</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">679</td>
<td>2020_San Antonio Spurs</td>
<td>2020</td>
<td>San Antonio Spurs</td>
<td>25.750000</td>
<td>199.390000</td>
<td>99.223250</td>
<td>114.1</td>
<td>115.2</td>
<td>-1.1</td>
<td>44.6</td>
<td>24.7</td>
<td>7.3</td>
<td>5.5</td>
<td>12.2</td>
<td>19.4</td>
<td>35.6</td>
<td>9.0</td>
<td>False</td>
<td>0.627363</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">680</td>
<td>2020_Toronto Raptors</td>
<td>2020</td>
<td>Toronto Raptors</td>
<td>25.555556</td>
<td>198.261111</td>
<td>96.539497</td>
<td>112.8</td>
<td>106.5</td>
<td>6.3</td>
<td>45.3</td>
<td>25.2</td>
<td>8.6</td>
<td>5.0</td>
<td>13.8</td>
<td>21.8</td>
<td>36.0</td>
<td>9.3</td>
<td>True</td>
<td>0.289885</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">681</td>
<td>2020_Utah Jazz</td>
<td>2020</td>
<td>Utah Jazz</td>
<td>25.666667</td>
<td>196.285556</td>
<td>98.353865</td>
<td>111.3</td>
<td>108.8</td>
<td>2.5</td>
<td>44.6</td>
<td>22.3</td>
<td>6.1</td>
<td>4.0</td>
<td>14.2</td>
<td>20.3</td>
<td>35.4</td>
<td>9.1</td>
<td>True</td>
<td>-0.099513</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">682</td>
<td>2020_Washington Wizards</td>
<td>2020</td>
<td>Washington Wizards</td>
<td>25.210526</td>
<td>197.718947</td>
<td>96.925448</td>
<td>114.4</td>
<td>119.1</td>
<td>-4.7</td>
<td>42.1</td>
<td>25.0</td>
<td>8.0</td>
<td>4.3</td>
<td>13.5</td>
<td>22.7</td>
<td>31.9</td>
<td>10.2</td>
<td>False</td>
<td>0.705243</td>
<td>True</td>
</tr>
</tbody>
</table>

<p>683 rows × 20 columns</p>
</div>
</div>
</div>
</div>
<p>And, finally, we can generate a much more useful cross-tabulation of our variables:</p>
<div id="cell-32" class="cell" data-outputid="262269d0-1993-4057-fdc7-62ca64fd3220" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>pts_playoffs_count <span class="op">=</span> pd.crosstab(nba_df[<span class="st">'above_mean_pts'</span>], nba_df[<span class="st">'playoffs'</span>])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>pts_playoffs_count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">False</td>
<td>209</td>
<td>137</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">True</td>
<td>106</td>
<td>231</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>And then <strong>plot</strong> the values in this cross-tabulation for an even more intuitive representation, where for each value of <code>above_mean_pts</code> we will have a bar representing the <strong>total</strong> number of (team,season) pairs in our dataset where the team had above-the-mean points per game in that season, and each bar will be <strong>further split</strong> into two “sub-rectangles” representing the <strong>proportions</strong> of these counts representing teams which <strong>also</strong> made the playoffs in that season:</p>
<div id="cell-34" class="cell" data-outputid="d7ac1fe1-313c-43e5-859d-893ddde2126a" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>crosstab_plot <span class="op">=</span> pts_playoffs_count.plot(kind<span class="op">=</span><span class="st">'bar'</span>, stacked<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>crosstab_plot.grid(<span class="va">False</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Playoffs vs. Points-Per-Game'</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Above-Mean Points Per Game'</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Number of Teams'</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="DSAN5000_Feature_Selection_Full_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice how, in both the cross-tabulation and plot above, having raw counts on their own was a bit of a nuisance, in the sense that we really care about their <strong>proportions</strong> to one another. The plot made this a bit easier, since we could compare the areas of the two sub-rectangles within each bar, but it’d be more helpful if we could read these totals directly off of the cross-tabulation. To accomplish this, we can include the <code>margins=True</code> argument to <code>pd.crosstab()</code> and it will produce, in addition to the output above, a <strong>totals column</strong> and <strong>totals row</strong>:</p>
<div id="cell-36" class="cell" data-outputid="5ddf3f4c-cce2-453d-ec7c-01a8c9d7c2ea" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>pts_playoffs_count <span class="op">=</span> pd.crosstab(nba_df[<span class="st">'above_mean_pts'</span>], nba_df[<span class="st">'playoffs'</span>], margins<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>pts_playoffs_count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
<th data-quarto-table-cell-role="th">All</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">False</td>
<td>209</td>
<td>137</td>
<td>346</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">True</td>
<td>106</td>
<td>231</td>
<td>337</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">All</td>
<td>315</td>
<td>368</td>
<td>683</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>We can see from this table that, among all <strong>337 (team,season) pairs</strong> where the team had <strong>above average</strong> points-per-game in that season, the team made the playoffs <strong>231 times</strong> and failed to make the playoffs <strong>106 times</strong>. On the other hand, of the <strong>346 (team,season) pairs</strong> where the team had <strong>below average</strong> points-per-game in that season, that team made the playoffs only <strong>137 times</strong> and failed to make the playoffs in <strong>209 cases</strong>.</p>
<p>Notice how I’m giving raw counts here, but contextualizing them by also providing the <strong>total number of cases</strong> in each row. The <code>pd.crosstab()</code> function has another optional parameter <code>normalize</code> (on top of the <code>margins=True</code> option we provided above) which will do this <strong>normalization</strong> for us, thus producing actual full-on <strong>probability distributions</strong>:</p>
<ul>
<li>By default, the produced table contains <strong>counts</strong>, not probabilities</li>
<li>If we include <code>normalize='all'</code>, Pandas will generate a table representing the <strong>joint distribution</strong> of the pair of variables (<code>above_mean_points</code>,<code>playoffs</code>):</li>
</ul>
<div id="cell-38" class="cell" data-outputid="f5034104-c658-477e-d382-632e5d298eca" data-execution_count="19">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>pts_playoffs_joint <span class="op">=</span> pd.crosstab(nba_df[<span class="st">'above_mean_pts'</span>], nba_df[<span class="st">'playoffs'</span>], normalize<span class="op">=</span><span class="st">'all'</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>pts_playoffs_joint</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">False</td>
<td>0.306003</td>
<td>0.200586</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">True</td>
<td>0.155198</td>
<td>0.338214</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>So that from this table we can see that, for example, about 20% of all (team,season) pairs represent teams that made the playoffs despite having below-the-mean points per game in that season.</p>
<ul>
<li>If we include <code>normalize='all'</code> <strong>in addition to</strong> <code>margins=True</code>, Pandas will produce a table representing both the <strong>joint distribution</strong> (within the non-total cells) and the <strong>marginal distributions</strong> (in the row/column labeled <code>All</code>):</li>
</ul>
<div id="cell-41" class="cell" data-outputid="6f2a730a-345d-4f94-fd91-0c13859fbd6d" data-execution_count="20">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>pd.crosstab(nba_df[<span class="st">'above_mean_pts'</span>], nba_df[<span class="st">'playoffs'</span>], margins<span class="op">=</span><span class="va">True</span>, normalize<span class="op">=</span><span class="st">'all'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
<th data-quarto-table-cell-role="th">All</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">False</td>
<td>0.306003</td>
<td>0.200586</td>
<td>0.506589</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">True</td>
<td>0.155198</td>
<td>0.338214</td>
<td>0.493411</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">All</td>
<td>0.461201</td>
<td>0.538799</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>So that from the row labeled <code>All</code> in this table we can see that, for example, approximately 46% of all (team, season) pairs in the dataset correspond to teams who made the playoffs in that season.</p>
<ul>
<li>If we include <code>normalize='index'</code>, Pandas will generate a table representing the <strong>conditional distribution</strong> of <code>playoffs</code> given <code>above_mean_points</code>:</li>
</ul>
<div id="cell-44" class="cell" data-outputid="87918e41-32be-4b8e-886e-2dc568e89425" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>pd.crosstab(nba_df[<span class="st">'above_mean_pts'</span>], nba_df[<span class="st">'playoffs'</span>], normalize<span class="op">=</span><span class="st">'index'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">False</td>
<td>0.604046</td>
<td>0.395954</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">True</td>
<td>0.314540</td>
<td>0.685460</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Notice that the <strong>rows</strong> here (but <strong>not</strong> the columns) sum to 1. This table therefore tells us, for example, that across all (team,season) pairs where the team had <strong>above-the-mean</strong> points per game in that season (<code>above_mean_points</code> is <code>True</code>), about 69% of these teams made the playoffs in that season.</p>
<ul>
<li>Finally, if we include <code>normalize='columns'</code>, Pandas will produce a table representing the <strong>conditional distribution</strong> of <code>above_mean_pts</code> given <code>playoffs</code>:</li>
</ul>
<div id="cell-47" class="cell" data-outputid="d67ffaf9-06d9-4b16-a520-443000a49754" data-execution_count="22">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>pd.crosstab(nba_df[<span class="st">'above_mean_pts'</span>], nba_df[<span class="st">'playoffs'</span>], normalize<span class="op">=</span><span class="st">'columns'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">False</th>
<th data-quarto-table-cell-role="th">True</th>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">False</td>
<td>0.663492</td>
<td>0.372283</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">True</td>
<td>0.336508</td>
<td>0.627717</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<p>Notice that now the <strong>columns</strong> (but <strong>not</strong> the rows) sum to 1. This table therefore tells us, for example, that among (team,season) pairs where the team made the playoffs in that season (<code>playoffs</code> is <code>True</code>), about <strong>63%</strong> of these pairs correspond to teams which had <strong>above-the-mean</strong> points per game in that season.</p>
</section>
<section id="machine-learning-without-feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning-without-feature-selection">(3) Machine Learning <em>Without</em> Feature Selection</h2>
<p>While the cross-tabulations/plots in the previous section can help us think about the relationships between our <strong>features</strong> (all of the non-ID variables besides <code>playoffs</code>) and our <strong>label</strong> (the <code>playoffs</code> variable), and which features might be more or less helpful for <strong>predicting</strong> the labels for each (team,season) pair, we’re going to need to have a set of <strong>metrics</strong> for measuring <strong>how well</strong> we’re doing at this prediction task, as well as a <strong>range of values</strong> that these metrics may take on telling us whether we’re doing “not very well” vs.&nbsp;“very well” on the task.</p>
<p>In general, a very good rule-of-thumb to use in machine learning is to come up with a <strong>baseline value</strong> and <strong>gold standard value</strong> for the metric. In our case, for example, we can quickly think about generating the following two values:</p>
<ol type="1">
<li>A <strong>baseline value</strong> of our success metric, which tells us how well an algorithm <strong>randomly guessing</strong> would do on the task—in other words, an algorithm that is <strong>not learning anything at all</strong>. If we just write a function that randomly guesses <code>False</code> or <code>True</code> as the <code>playoff</code> value for a given (team, season) pair, how well does that function do on our task?</li>
<li>Another <strong>baseline value</strong> which is slightly smarter, but only slightly: it should guess the <strong>most common value</strong> of the label <strong>every single time</strong>. In our case, since there are more (team,season) pairs representing teams which <strong>made the playoffs</strong> in that season than there are (team,season) pairs representing teams which <strong>failed</strong> to make the playoffs, this baseline value would be derived from a function that always guesses the value <code>True</code> for our <code>playoffs</code> label.</li>
<li>Finally, since here we’re trying to look at the efficacy of <strong>feature selection</strong>, we should take an “off-the-shelf” machine learning method and see how well it does <strong>without performing any feature selection</strong>.</li>
</ol>
</section>
<section id="baseline-1-random-guessing" class="level2">
<h2 class="anchored" data-anchor-id="baseline-1-random-guessing">(4) Baseline 1: Random Guessing</h2>
<p>Although it may seem weird at first to sit down and purposefully code a “bad” model, you will thank yourself for it later, since it can serve as a “meter stick” to help you keep track of progress that you’re making as you improve/tweak your <strong>real</strong> model’s parameters.</p>
<p>So, making sure that we <strong>set the seed</strong> on NumPy’s random number generator so that results are replicable from here on out, let’s create this random-guessing baseline model:</p>
<div id="cell-53" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here we set the seed by creating a "random generator object"</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># in NumPy, which we'll use from here onwards when we need to</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># generate random numbers</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">5000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As we move from data exploration to actual machine learning, also, we’ll want to convert our <strong>boolean</strong> <code>playoffs</code> label into a <strong>numeric value</strong>, since many of the ML algorithms and evaluation functions in Scikit-learn require fully-numeric values (vectors and/or matrices, for example) as inputs.</p>
<p>To do this, we’ll use what I think is the easiest way to convert <strong>booleans</strong> into <strong>integers</strong> in Pandas, which is just to <code>apply()</code> Python’s <code>int()</code> conversion function to the entire column:</p>
<div id="cell-55" class="cell" data-outputid="430231ea-6b6a-422b-a0e2-a0b3b8640064" data-execution_count="24">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'playoffs'</span>] <span class="op">=</span> nba_df[<span class="st">'playoffs'</span>].<span class="bu">apply</span>(<span class="bu">int</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>nba_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year_team</th>
<th data-quarto-table-cell-role="th">po_year</th>
<th data-quarto-table-cell-role="th">team_full</th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">Ast</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">season_pts_z_score</th>
<th data-quarto-table-cell-role="th">above_mean_pts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998_Atlanta Hawks</td>
<td>1998</td>
<td>Atlanta Hawks</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>19.0</td>
<td>8.0</td>
<td>5.9</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
<td>1</td>
<td>0.082901</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1998_Boston Celtics</td>
<td>1998</td>
<td>Boston Celtics</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>22.1</td>
<td>12.0</td>
<td>4.5</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
<td>0</td>
<td>0.082901</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1998_Charlotte Hornets</td>
<td>1998</td>
<td>Charlotte Hornets</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>23.4</td>
<td>8.2</td>
<td>3.7</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
<td>1</td>
<td>0.261932</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1998_Chicago Bulls</td>
<td>1998</td>
<td>Chicago Bulls</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>23.1</td>
<td>8.6</td>
<td>4.3</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
<td>1</td>
<td>0.287508</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1998_Cleveland Cavaliers</td>
<td>1998</td>
<td>Cleveland Cavaliers</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>22.9</td>
<td>9.9</td>
<td>5.0</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
<td>1</td>
<td>-0.786678</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">678</td>
<td>2020_Sacramento Kings</td>
<td>2020</td>
<td>Sacramento Kings</td>
<td>25.117647</td>
<td>199.763529</td>
<td>98.242691</td>
<td>110.1</td>
<td>112.1</td>
<td>-2.0</td>
<td>42.6</td>
<td>23.8</td>
<td>7.7</td>
<td>4.1</td>
<td>13.7</td>
<td>22.2</td>
<td>32.9</td>
<td>9.7</td>
<td>0</td>
<td>-0.411031</td>
<td>False</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">679</td>
<td>2020_San Antonio Spurs</td>
<td>2020</td>
<td>San Antonio Spurs</td>
<td>25.750000</td>
<td>199.390000</td>
<td>99.223250</td>
<td>114.1</td>
<td>115.2</td>
<td>-1.1</td>
<td>44.6</td>
<td>24.7</td>
<td>7.3</td>
<td>5.5</td>
<td>12.2</td>
<td>19.4</td>
<td>35.6</td>
<td>9.0</td>
<td>0</td>
<td>0.627363</td>
<td>True</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">680</td>
<td>2020_Toronto Raptors</td>
<td>2020</td>
<td>Toronto Raptors</td>
<td>25.555556</td>
<td>198.261111</td>
<td>96.539497</td>
<td>112.8</td>
<td>106.5</td>
<td>6.3</td>
<td>45.3</td>
<td>25.2</td>
<td>8.6</td>
<td>5.0</td>
<td>13.8</td>
<td>21.8</td>
<td>36.0</td>
<td>9.3</td>
<td>1</td>
<td>0.289885</td>
<td>True</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">681</td>
<td>2020_Utah Jazz</td>
<td>2020</td>
<td>Utah Jazz</td>
<td>25.666667</td>
<td>196.285556</td>
<td>98.353865</td>
<td>111.3</td>
<td>108.8</td>
<td>2.5</td>
<td>44.6</td>
<td>22.3</td>
<td>6.1</td>
<td>4.0</td>
<td>14.2</td>
<td>20.3</td>
<td>35.4</td>
<td>9.1</td>
<td>1</td>
<td>-0.099513</td>
<td>False</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">682</td>
<td>2020_Washington Wizards</td>
<td>2020</td>
<td>Washington Wizards</td>
<td>25.210526</td>
<td>197.718947</td>
<td>96.925448</td>
<td>114.4</td>
<td>119.1</td>
<td>-4.7</td>
<td>42.1</td>
<td>25.0</td>
<td>8.0</td>
<td>4.3</td>
<td>13.5</td>
<td>22.7</td>
<td>31.9</td>
<td>10.2</td>
<td>0</td>
<td>0.705243</td>
<td>True</td>
</tr>
</tbody>
</table>

<p>683 rows × 20 columns</p>
</div>
</div>
</div>
</div>
<p>We can now see in the above table that (scrolling horizontally to the <code>playoffs</code> column) entries where the <code>playoffs</code> value was the boolean <code>True</code> now have the integer value <code>1</code>, and entries where the <code>playoffs</code> value was the boolean <code>False</code> now have the integer value <code>0</code>.</p>
<p>Given our <code>nba_df</code> dataset with the now-numeric label, we can make a new column called <code>random_guess</code> which just generates a random boolean value for each observation, then compute how well this random-guess model is doing by comparing the <code>random_guess</code> model with the true values in <code>playoffs</code>.</p>
<blockquote class="blockquote">
<p><strong>Imporant Aside: Accuracy vs.&nbsp;F1 Score!</strong><br><br>You have maybe noticed that I’m being purposefully vague about measuring <strong>how well</strong> a given classifier is doing: that’s because, while it may feel instinctively “right”/natural to say “I’ll see how well my classifier is doing by measuring its <strong>accuracy</strong> on a task”, the notion of <strong>accuracy</strong> in machine learning is (sadly but truly) a <strong>🚨RED FLAG🚨</strong> that you should learn to be wary of! Let’s quickly see why, by going over to the<br><br><strong><a href="https://jjacobs.me/dsan5000/463a01339cf0f456ba54a1849df50d1a22c247e3/writeups/machine-learning/slides.html#/title-slide" target="_blank">Extra slides on Machine Learning</a></strong><br><br>that are now posted in the “Extra Writeups” section of the website for my sections.<br><br> Specifically, you should look at them from the beginning (they overlap, purposefully, with the slides from this week where I started talking about <strong>Supervised</strong> vs.&nbsp;<strong>Unsupervised</strong> learning on the liked-and-disliked-houses dataset) up to the <strong>Measuring Errors: F1 Score</strong> slide, which will introduce you to the <strong>F1 Score</strong> that we will be using here to quantify how well/not-well our random-guessing approach is doing at our task.</p>
</blockquote>
<p>Now that we have seen why the F1 score is preferable to accuracy as a measurement of the performance of our algorithms, let’s generate guesses and evaluate our random-guess model using the F1 score:</p>
<div id="cell-60" class="cell" data-outputid="e2e83ce7-9fc6-4cff-b93a-9f40248aa6e8" data-execution_count="25">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate one random guess (a random choice of an element from</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the python list [0,1]) per observation in the dataset</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>num_obs <span class="op">=</span> <span class="bu">len</span>(nba_df)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>random_guesses <span class="op">=</span> rng.choice([<span class="dv">0</span>,<span class="dv">1</span>], num_obs)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'random_guess'</span>] <span class="op">=</span> random_guesses</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>nba_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year_team</th>
<th data-quarto-table-cell-role="th">po_year</th>
<th data-quarto-table-cell-role="th">team_full</th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">season_pts_z_score</th>
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th">random_guess</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998_Atlanta Hawks</td>
<td>1998</td>
<td>Atlanta Hawks</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>...</td>
<td>8.0</td>
<td>5.9</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
<td>1</td>
<td>0.082901</td>
<td>True</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1998_Boston Celtics</td>
<td>1998</td>
<td>Boston Celtics</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>...</td>
<td>12.0</td>
<td>4.5</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
<td>0</td>
<td>0.082901</td>
<td>True</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1998_Charlotte Hornets</td>
<td>1998</td>
<td>Charlotte Hornets</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>...</td>
<td>8.2</td>
<td>3.7</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
<td>1</td>
<td>0.261932</td>
<td>True</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1998_Chicago Bulls</td>
<td>1998</td>
<td>Chicago Bulls</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>...</td>
<td>8.6</td>
<td>4.3</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
<td>1</td>
<td>0.287508</td>
<td>True</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1998_Cleveland Cavaliers</td>
<td>1998</td>
<td>Cleveland Cavaliers</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>...</td>
<td>9.9</td>
<td>5.0</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
<td>1</td>
<td>-0.786678</td>
<td>False</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">678</td>
<td>2020_Sacramento Kings</td>
<td>2020</td>
<td>Sacramento Kings</td>
<td>25.117647</td>
<td>199.763529</td>
<td>98.242691</td>
<td>110.1</td>
<td>112.1</td>
<td>-2.0</td>
<td>42.6</td>
<td>...</td>
<td>7.7</td>
<td>4.1</td>
<td>13.7</td>
<td>22.2</td>
<td>32.9</td>
<td>9.7</td>
<td>0</td>
<td>-0.411031</td>
<td>False</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">679</td>
<td>2020_San Antonio Spurs</td>
<td>2020</td>
<td>San Antonio Spurs</td>
<td>25.750000</td>
<td>199.390000</td>
<td>99.223250</td>
<td>114.1</td>
<td>115.2</td>
<td>-1.1</td>
<td>44.6</td>
<td>...</td>
<td>7.3</td>
<td>5.5</td>
<td>12.2</td>
<td>19.4</td>
<td>35.6</td>
<td>9.0</td>
<td>0</td>
<td>0.627363</td>
<td>True</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">680</td>
<td>2020_Toronto Raptors</td>
<td>2020</td>
<td>Toronto Raptors</td>
<td>25.555556</td>
<td>198.261111</td>
<td>96.539497</td>
<td>112.8</td>
<td>106.5</td>
<td>6.3</td>
<td>45.3</td>
<td>...</td>
<td>8.6</td>
<td>5.0</td>
<td>13.8</td>
<td>21.8</td>
<td>36.0</td>
<td>9.3</td>
<td>1</td>
<td>0.289885</td>
<td>True</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">681</td>
<td>2020_Utah Jazz</td>
<td>2020</td>
<td>Utah Jazz</td>
<td>25.666667</td>
<td>196.285556</td>
<td>98.353865</td>
<td>111.3</td>
<td>108.8</td>
<td>2.5</td>
<td>44.6</td>
<td>...</td>
<td>6.1</td>
<td>4.0</td>
<td>14.2</td>
<td>20.3</td>
<td>35.4</td>
<td>9.1</td>
<td>1</td>
<td>-0.099513</td>
<td>False</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">682</td>
<td>2020_Washington Wizards</td>
<td>2020</td>
<td>Washington Wizards</td>
<td>25.210526</td>
<td>197.718947</td>
<td>96.925448</td>
<td>114.4</td>
<td>119.1</td>
<td>-4.7</td>
<td>42.1</td>
<td>...</td>
<td>8.0</td>
<td>4.3</td>
<td>13.5</td>
<td>22.7</td>
<td>31.9</td>
<td>10.2</td>
<td>0</td>
<td>0.705243</td>
<td>True</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>683 rows × 21 columns</p>
</div>
</div>
</div>
</div>
<p>We could compute the F1 score <strong>manually</strong> here (and it would be good practice/coding work for you!), but instead now we will finally import and use <strong>Scikit-learn</strong>, which has a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" target="_blank"><strong>built-in</strong> function for computing F-scores</a>. To use it, we just need to provide the first two arguments, <code>y_true</code> and <code>y_pred</code>:</p>
<div id="cell-62" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-63" class="cell" data-outputid="2cf8facc-ca4d-4544-9d0d-e697818e1bc9" data-execution_count="27">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>f1_score(nba_df[<span class="st">'playoffs'</span>], nba_df[<span class="st">'random_guess'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>0.5313351498637602</code></pre>
</div>
</div>
<p>We can now also compute accuracy as</p>
<p><span class="math display">\[
\frac{\#\text{ Correct guesses}}{\#\text{ Total guesses}}
\]</span></p>
<p>to see the difference between F1 score and accuracy:</p>
<div id="cell-65" class="cell" data-outputid="bc26be6b-b172-4278-d34e-e6e140d03026" data-execution_count="28">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'random_guess_correct'</span>] <span class="op">=</span> nba_df[<span class="st">'playoffs'</span>] <span class="op">==</span> nba_df[<span class="st">'random_guess'</span>]</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>correct_random_guess_rows <span class="op">=</span> nba_df[nba_df[<span class="st">'random_guess_correct'</span>] <span class="op">==</span> <span class="va">True</span>]</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>random_guess_accuracy <span class="op">=</span> <span class="bu">len</span>(correct_random_guess_rows) <span class="op">/</span> <span class="bu">len</span>(nba_df)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>random_guess_accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>0.49633967789165445</code></pre>
</div>
</div>
</section>
<section id="baseline-2-guessing-the-most-frequent-label" class="level2">
<h2 class="anchored" data-anchor-id="baseline-2-guessing-the-most-frequent-label">(5) Baseline 2: Guessing the Most Frequent Label</h2>
<p>The two metrics (accuracy and F1 score) don’t seem so different in this case, but now let’s implement the <strong>second baseline model</strong> mentioned above, where we <strong>always guess the most frequent label</strong> no matter what observation we’re looking at.</p>
<p>Notice how, just as in the random-guessing case, here we are <strong>not even looking at the features of the individual observations at all</strong>. And yet, here we will almost always achieve a <strong>higher accuracy</strong> than the random-guessing approach, essentially because we are allowing a small “bit” of information about the labels to enter our algorithm here: whereas the random-guess model truly knows nothing about the distribution of labels in the dataset, the guess-most-frequent-label approach <strong>does</strong> know that there are more observations with a <code>playoffs</code> value of <code>1</code> than with a <code>playoffs</code> value of <code>0</code>, and it is able to take advantage of this information:</p>
<div id="cell-68" class="cell" data-outputid="f877cd38-5565-4caf-f0fb-43edc6246144" data-execution_count="29">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Since datasets can technically have more than one mode, the</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="co"># mode() function in Pandas returns a pd.Series object containing</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co"># all of the mode values. Since in this case we know that there is</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># only **one** mode (the value 1), we can just convert this</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># one-item pd.Series directly into an integer value using Python's</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># int() method:</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>most_frequent_label <span class="op">=</span> <span class="bu">int</span>(nba_df[<span class="st">'playoffs'</span>].mode())</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>most_frequent_label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/n2/m7_fj5vx6c50_yj7g23mwmq00000gn/T/ipykernel_10671/1516993167.py:7: FutureWarning: Calling int on a single element Series is deprecated and will raise a TypeError in the future. Use int(ser.iloc[0]) instead
  most_frequent_label = int(nba_df['playoffs'].mode())</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>1</code></pre>
</div>
</div>
<div id="cell-69" class="cell" data-outputid="1fd9aabb-b76c-4d3e-8071-e5e31b411d58" data-execution_count="30">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'most_freq_guess'</span>] <span class="op">=</span> most_frequent_label</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>nba_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year_team</th>
<th data-quarto-table-cell-role="th">po_year</th>
<th data-quarto-table-cell-role="th">team_full</th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
<th data-quarto-table-cell-role="th">playoffs</th>
<th data-quarto-table-cell-role="th">season_pts_z_score</th>
<th data-quarto-table-cell-role="th">above_mean_pts</th>
<th data-quarto-table-cell-role="th">random_guess</th>
<th data-quarto-table-cell-role="th">random_guess_correct</th>
<th data-quarto-table-cell-role="th">most_freq_guess</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>1998_Atlanta Hawks</td>
<td>1998</td>
<td>Atlanta Hawks</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>...</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
<td>1</td>
<td>0.082901</td>
<td>True</td>
<td>0</td>
<td>False</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>1998_Boston Celtics</td>
<td>1998</td>
<td>Boston Celtics</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>...</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
<td>0</td>
<td>0.082901</td>
<td>True</td>
<td>0</td>
<td>True</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>1998_Charlotte Hornets</td>
<td>1998</td>
<td>Charlotte Hornets</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>...</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
<td>1</td>
<td>0.261932</td>
<td>True</td>
<td>0</td>
<td>False</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>1998_Chicago Bulls</td>
<td>1998</td>
<td>Chicago Bulls</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>...</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
<td>1</td>
<td>0.287508</td>
<td>True</td>
<td>0</td>
<td>False</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>1998_Cleveland Cavaliers</td>
<td>1998</td>
<td>Cleveland Cavaliers</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>...</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
<td>1</td>
<td>-0.786678</td>
<td>False</td>
<td>1</td>
<td>True</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>5 rows × 23 columns</p>
</div>
</div>
</div>
</div>
<p>And now we can use Scikit-learn’s <code>f1_score()</code> function to compute the F1 score in this case as well:</p>
<div id="cell-71" class="cell" data-outputid="e26820a5-6df2-442e-f15d-631aeefc365e" data-execution_count="31">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>f1_score(nba_df[<span class="st">'playoffs'</span>], nba_df[<span class="st">'most_freq_guess'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>0.7002854424357755</code></pre>
</div>
</div>
<p>And the accuracy of this most-frequent-label guess approach as well, for comparison:</p>
<div id="cell-73" class="cell" data-outputid="840c1976-6111-4c78-9e16-9ad6747b4a92" data-execution_count="32">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>nba_df[<span class="st">'most_freq_guess_correct'</span>] <span class="op">=</span> nba_df[<span class="st">'most_freq_guess'</span>] <span class="op">==</span> nba_df[<span class="st">'playoffs'</span>]</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>correct_most_freq_rows <span class="op">=</span> nba_df[nba_df[<span class="st">'most_freq_guess_correct'</span>] <span class="op">==</span> <span class="va">True</span>]</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>most_freq_accuracy <span class="op">=</span> <span class="bu">len</span>(correct_most_freq_rows) <span class="op">/</span> <span class="bu">len</span>(nba_df)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>most_freq_accuracy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>0.5387994143484627</code></pre>
</div>
</div>
<p>Note that here, unlike in the liked-and-disliked-houses example from the slides, guessing the most-frequent label every time actually does produce a fairly high <strong>F1 score</strong> of about 0.7. So, if we were doing all this as research to put into a paper and submit to a CS journal, we would now have our set of <strong>baselines</strong> that we should be able to <strong>beat</strong> in order to show that our fancier machine-learning approach was <strong>actually <em>learning</em> something</strong> about the data, in a deeper sense than just e.g.&nbsp;learning what the most-frequent label was and always guessing that. In a Machine Learning paper, generally these two baseline approaches would appear in a table like the following (where I’ve left blanks in the slots for our fancy algorithm that we’re hoping can soundly “beat out” these baseline approaches):</p>
<table class="table">
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Accuracy</th>
<th>F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ML Algorithm + <strong>Feature Selection</strong></td>
<td>?</td>
<td><strong>?</strong></td>
</tr>
<tr class="even">
<td>ML Algorithm <strong>without</strong> Feature Selection</td>
<td>?</td>
<td><strong>?</strong></td>
</tr>
<tr class="odd">
<td>Baseline: Guess Most Frequent Label</td>
<td>0.539</td>
<td><strong>0.700</strong></td>
</tr>
<tr class="even">
<td>Baseline: Random Guessing</td>
<td>0.496</td>
<td><strong>0.531</strong></td>
</tr>
</tbody>
</table>
<p>Where here I have bolded the <strong>F1 Score</strong> column to indicate that, as is standard practice in most CS journals, we are <strong>sorting the results by F1 score</strong> rather than accuracy or any other metric which might be “misled” by fake not-actually-learning-anything approaches like guessing the most frequent label.</p>
<p>In the next and final part, we will fill in these final “?” cells in the table, seeing what we are able to accomplish using a standard ML algorithm <strong>without</strong> feature selection, and then seeing how applying an additional feature selection step can <strong>improve upon</strong> the performance (in terms of F1 score) of the ML algorithm without this feature selection step.</p>
<p>To illustrate how standard the F1 score is, I went on the CS Arxiv (which I get email updates about, and check every day, incidentally!) and looked at the papers posted today in the Computation and Language category: I opened <a href="" target="_blank">the very first paper I saw on the list</a> and scrolled down without reading anything at all, and alas! There was an F1 score table right at the top of page 6 of the PDF. Which is just an example of how, the more familiar and comfortable you are with the F1 score and the two quantities it is built upon (<strong>precision</strong> and <strong>recall</strong>), the easier it will become for you to keep up with the Computer Science literature!</p>
</section>
<section id="using-an-off-the-shelf-machine-learning-model" class="level2">
<h2 class="anchored" data-anchor-id="using-an-off-the-shelf-machine-learning-model">(6) Using an “Off-The-Shelf” Machine Learning Model</h2>
<p>Since the Lab 3.2 Assignment asks you to train a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" target="_blank">Gaussian Naïve Bayes classifier</a>, here I will instead use Scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" target="_blank">Linear Support Vector Machine (SVM) Classifier</a>. But, since Scikit-learn’s various classes and functions often work very similarly to one another, this will give you a good starting point to take what you learn here and apply it to the <code>GaussianNB</code> classifier on the assignment!</p>
<blockquote class="blockquote">
<p><strong>Quick Aside: Classification vs.&nbsp;Regression</strong><br><br>If you take a quick look at the Scikit-learn documentation’s <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm" target="_blank">section on Support Vector Machines</a>, you’ll see that it contains both <strong>Classification</strong> and <strong>Regression</strong> versions of the SVM algorithm:</p>
</blockquote>
<p><img src="https://jpj.georgetown.domains/dsan5000-scratch/feature-selection/sklearn_svm.jpeg" class="img-fluid"></p>
<blockquote class="blockquote">
<p>This relates to what I mentioned in class, that when you’re first learning about ML algorithms, you can usually worry less about the distinction between <strong>discrete</strong> and <strong>continuous</strong> data than we’ve been worrying in class up to this point. Once you get more comfortable with how these ML algorithms work in general, <strong>then</strong> you can think more deeply about the difference (for example, in terms of the number of datapoints that will be needed for the ML algorithm to learn the relationship between features and labels), but for now I just wanted to point out that we could have done this entire lab with a <strong>continuous label</strong> rather than a <strong>discrete label</strong>, without much difference in the Scikit-learn code that we’re using!</p>
</blockquote>
<p>So, to get started (and, Scikit-learn’s <a href="https://scikit-learn.org/stable/modules/svm.html" target="_blank">tutorial page</a> is a good place to look for how to get started as well, more generally), we import the <code>svm</code> module within <code>sklearn</code>:</p>
<div id="cell-78" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, since (as mentioned above) Scikit-learn is set up to work with <strong>numeric</strong> NumPy vectors/matrices rather than Pandas <code>DataFrame</code> objects, we’ll need to convert our <code>nba_df</code> object into a NumPy array. This can be done very quickly and straightforwardly, in most cases, by just accessing the <code>.values</code> attribute on the <code>DataFrame</code> itself, or on specific columns of the <code>DataFrame</code>:</p>
<div id="cell-80" class="cell" data-outputid="4e85fa7a-8f12-491e-ae71-afd72e4f1e9b" data-execution_count="34">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the columns we created in the previous sections (e.g., for the</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="co"># random-guessing and guess-most-frequent-label models above), where we specify</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="co"># `errors='ignore'` so that this line doesn't crash if we've already dropped</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># them at some point</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>prev_cols <span class="op">=</span> [</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'season_pts_z_score'</span>,<span class="st">'above_mean_pts'</span>,<span class="st">'random_guess'</span>,<span class="st">'random_guess_correct'</span>,</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'most_freq_guess'</span>, <span class="st">'most_freq_guess_correct'</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>nba_df.drop(columns <span class="op">=</span> prev_cols, inplace<span class="op">=</span><span class="va">True</span>, errors<span class="op">=</span><span class="st">'ignore'</span>)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="co"># And now we specify the id columns (which sklearn can't use) and the label</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="co"># column, so that the feature columns are just all of the remaining columns in</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="co"># the DataFrame</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>id_cols <span class="op">=</span> [<span class="st">'year_team'</span>,<span class="st">'po_year'</span>,<span class="st">'team_full'</span>]</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>label_col <span class="op">=</span> <span class="st">'playoffs'</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>feature_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> nba_df.columns <span class="cf">if</span> c <span class="op">!=</span> label_col <span class="kw">and</span> c <span class="kw">not</span> <span class="kw">in</span> id_cols]</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>feature_cols</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>['mean_age',
 'mean_height',
 'mean_weight',
 'PtsSeason',
 'OppPtsSeason',
 'PtsDiff',
 'Reb',
 'Ast',
 'Stl',
 'Blk',
 'To',
 'Pf',
 'Dreb',
 'Oreb']</code></pre>
</div>
</div>
<p>And now we’re ready to extract the subsets of <code>nba_df</code> that we’ll use as the <strong>feature matrix</strong> and <strong>label vector</strong> that we provide to Scikit-learn:</p>
<div id="cell-82" class="cell" data-outputid="e702a220-3c3b-4be2-f878-a8ef1004df88" data-execution_count="35">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>feature_matrix <span class="op">=</span> nba_df[feature_cols].copy()</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>feature_matrix</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">Ast</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>28.250000</td>
<td>199.231250</td>
<td>99.166551</td>
<td>95.9</td>
<td>92.3</td>
<td>3.6</td>
<td>42.8</td>
<td>19.0</td>
<td>8.0</td>
<td>5.9</td>
<td>14.0</td>
<td>20.5</td>
<td>29.5</td>
<td>13.4</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>25.857143</td>
<td>200.297143</td>
<td>95.027524</td>
<td>95.9</td>
<td>98.5</td>
<td>-2.6</td>
<td>39.5</td>
<td>22.1</td>
<td>12.0</td>
<td>4.5</td>
<td>15.6</td>
<td>26.9</td>
<td>24.9</td>
<td>14.6</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>29.733333</td>
<td>200.829333</td>
<td>101.755805</td>
<td>96.6</td>
<td>94.6</td>
<td>2.0</td>
<td>40.1</td>
<td>23.4</td>
<td>8.2</td>
<td>3.7</td>
<td>14.4</td>
<td>21.4</td>
<td>28.4</td>
<td>11.7</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>30.600000</td>
<td>201.337333</td>
<td>103.691131</td>
<td>96.7</td>
<td>89.6</td>
<td>7.1</td>
<td>44.1</td>
<td>23.1</td>
<td>8.6</td>
<td>4.3</td>
<td>13.0</td>
<td>21.1</td>
<td>29.2</td>
<td>14.9</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>26.375000</td>
<td>199.231250</td>
<td>96.756844</td>
<td>92.5</td>
<td>89.8</td>
<td>2.7</td>
<td>39.8</td>
<td>22.9</td>
<td>9.9</td>
<td>5.0</td>
<td>16.6</td>
<td>23.7</td>
<td>28.2</td>
<td>11.6</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">678</td>
<td>25.117647</td>
<td>199.763529</td>
<td>98.242691</td>
<td>110.1</td>
<td>112.1</td>
<td>-2.0</td>
<td>42.6</td>
<td>23.8</td>
<td>7.7</td>
<td>4.1</td>
<td>13.7</td>
<td>22.2</td>
<td>32.9</td>
<td>9.7</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">679</td>
<td>25.750000</td>
<td>199.390000</td>
<td>99.223250</td>
<td>114.1</td>
<td>115.2</td>
<td>-1.1</td>
<td>44.6</td>
<td>24.7</td>
<td>7.3</td>
<td>5.5</td>
<td>12.2</td>
<td>19.4</td>
<td>35.6</td>
<td>9.0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">680</td>
<td>25.555556</td>
<td>198.261111</td>
<td>96.539497</td>
<td>112.8</td>
<td>106.5</td>
<td>6.3</td>
<td>45.3</td>
<td>25.2</td>
<td>8.6</td>
<td>5.0</td>
<td>13.8</td>
<td>21.8</td>
<td>36.0</td>
<td>9.3</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">681</td>
<td>25.666667</td>
<td>196.285556</td>
<td>98.353865</td>
<td>111.3</td>
<td>108.8</td>
<td>2.5</td>
<td>44.6</td>
<td>22.3</td>
<td>6.1</td>
<td>4.0</td>
<td>14.2</td>
<td>20.3</td>
<td>35.4</td>
<td>9.1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">682</td>
<td>25.210526</td>
<td>197.718947</td>
<td>96.925448</td>
<td>114.4</td>
<td>119.1</td>
<td>-4.7</td>
<td>42.1</td>
<td>25.0</td>
<td>8.0</td>
<td>4.3</td>
<td>13.5</td>
<td>22.7</td>
<td>31.9</td>
<td>10.2</td>
</tr>
</tbody>
</table>

<p>683 rows × 14 columns</p>
</div>
</div>
</div>
</div>
<div id="cell-83" class="cell" data-outputid="21d88b6f-da62-4213-cb80-0fb89ac048d3" data-execution_count="36">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>feature_matrix.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>(683, 14)</code></pre>
</div>
</div>
<div id="cell-84" class="cell" data-outputid="21cd4a2e-de14-43ba-c872-798a459b89f9" data-execution_count="37">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>label_vec <span class="op">=</span> nba_df[label_col].copy()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>label_vec</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>0      1
1      0
2      1
3      1
4      1
      ..
678    0
679    0
680    1
681    1
682    0
Name: playoffs, Length: 683, dtype: int64</code></pre>
</div>
</div>
<div id="cell-85" class="cell" data-outputid="7110dc46-c52d-4cd7-a31a-44c6448677cb" data-execution_count="38">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>label_vec.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>(683,)</code></pre>
</div>
</div>
<p>So we can see that our <strong>feature matrix</strong> in this case is a 683 x 14 matrix, while our <strong>label vector</strong> is a 683-dimensional <strong>column vector</strong></p>
<p>First things first, we create an <code>SVC</code> <strong>object</strong>, which we will use for <strong>fitting</strong> the model to data, for <strong>evaluating</strong> the model, and then for using the trained model to make <strong>predictions</strong> given new not-seen-before datapoints. Typically we call this object <code>clf</code> to identify that it is a <strong>classifier</strong> object (like how we use <code>df</code> to identify that this is a <code>DataFrame</code> object)</p>
<div id="cell-88" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.SVC()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, since we have a classifier object <code>clf</code>, we could immediately train an SVM on all of our data, as-is, using the following code:</p>
<div id="cell-90" class="cell" data-outputid="3d6627fc-bfdd-4c3a-91ee-825900249600" data-execution_count="40">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>clf.fit(feature_matrix, label_vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC()</pre></div></div></div></div></div>
</div>
</div>
<p>However, this approach <strong>will not perform very well on the training data</strong>, and it will also have a set of <strong>biases</strong> that will prevent it from <strong>generalizing to new data</strong> (for example, if we wanted to use the trained classifier to make predictions about next season). Before we can get a reasonably well-performing classifier, we have to quickly talk about two considerations that we’ll always have to take into account before we train a Machine Learning algorithm!</p>
<section id="normalization" class="level3">
<h3 class="anchored" data-anchor-id="normalization">(6.1) Normalization</h3>
<p>There’s a reason we spent so much time talking about normalization in class! In the case of SVM algorithms (as is discussed in the <a href="https://scikit-learn.org/stable/modules/svm.html#tips-on-practical-use" target="_blank">“Tips on Practical Use” section</a> of Scikit-learn’s SVM tutorial), these models are <strong>not scale-invariant</strong>. This means, for example, that if you had a feature <span class="math inline">\(X \sim \mathcal{N}(5,1)\)</span>, and you decided to <strong>center</strong> this variable by e.g.&nbsp;subtracting all values by 5 to obtain <span class="math inline">\(X' \sim \mathcal{N}(0, 1)\)</span>, the result of an SVM trained on <span class="math inline">\(X\)</span> is <strong>not guaranteed to be the same</strong> as the SVM trained on <span class="math inline">\(X'\)</span>, despite the fact that these represent the exact same underlying data!</p>
<p>So, to handle this undesirable property, before training an SVM we typically <strong>normalize</strong> all of the features to have a mean of 0 and a standard deviation of 1 (that is, we transform all values into <strong><span class="math inline">\(z\)</span>-scores</strong>). While we know how to do this mathematically, so that we could go through and apply the mathematical transformation to each column manually, Scikit-learn provides a nice <strong>pipeline functionality</strong> which can be used to ensure that this transformation is <strong>applied automatically</strong> before training the algorithm. So, rather than using the <code>clf</code> object we created earlier, we could replace it with a two-step <strong>pipeline</strong> which always transforms the data in this way before training, evaluating, or making predictions:</p>
<div id="cell-93" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> make_pipeline(StandardScaler(), svm.SVC())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>So, with this new two-step pipeline in hand, we could now tell Scikit-learn to <strong>fit</strong> the parameters of the model (in other words, to <strong>train</strong> the model on the training data), using the classifier object’s <code>fit()</code> function, and it would perform much better in terms of achieving a high F1 score on our data:</p>
<div id="cell-95" class="cell" data-outputid="4de2da76-bf2e-4395-d3f3-77636109beec" data-execution_count="42">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>clf.fit(feature_matrix, label_vec)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC())])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC()</pre></div></div></div></div></div></div></div>
</div>
</div>
<p>However, there is <strong>one more extremely important consideration</strong> that we’ll need to take into account before we can fully dive into what the algorithm has learned—namely, splitting our full dataset into <strong>training</strong> and <strong>test</strong> data!</p>
</section>
<section id="training-data-vs.-test-data" class="level3">
<h3 class="anchored" data-anchor-id="training-data-vs.-test-data">(6.2) Training Data vs.&nbsp;Test Data</h3>
<p>A crucial consideration in Machine Learning, which I also have been conspicuously avoiding until now, can be summarized as follows:</p>
<p><strong>Our goal in training a Machine Learning algorithm is <em>not</em> do well at classifying data we’ve <em>already seen</em>, but rather to <em>learn</em> from the data we’ve already seen to do well at <em>predicting future data</em></strong></p>
<p>Therefore, when we’re <strong>training</strong> a machine learning algorithm, we <strong>do not</strong> want to include <strong>all of the data that we have</strong>. Instead, we should provide a <strong>portion</strong> of the data to the algorithm during the <strong>training</strong> stage, then <strong>evaluate</strong> the algorithm on the basis of its performance <strong>predicting labels</strong> for the “held-out” portion of the dataset that it <strong>did not see</strong> during the training stage!</p>
<p>A widely-followed standard in Machine learning is the <strong>80-20 rule</strong>: provide the algorithm with <strong>80% of the data we have</strong> at training time (hence we call this portion <strong>training data</strong>), then evaluate the algorithm’s performance on <strong>the remaining 20% of the data</strong> (hence we call this portion <strong>test data</strong>). The following figure from the slides linked above illustrates this split, where thinking of this 80-20 split in terms of chunks of 20% will be helpful for future, fancier approaches:</p>
<p><img src="https://kroki.io/graphviz/svg/eNptUEFqwzAQPEuvEIJAC6Fx3AZ60aUPyKW9BVNkayuLbCQjyaEl5O-VZROnpnsQmllpZme1l13LtDeKXihLpTNxyPeh3Bk8yk5E30Mmq3xapyBAJ4qnImMv7XGJlfGCf7zx24c72RB_EAT_Moig-PrGNw6dFzXK5jiTw6uxgUa3scYe5mZoZQeidt_TdJRsNqy3jTudwEYWHWtNso4tjDEJKJ0mGQcw9mxCRQkloa_H6A32IYL_LLbsQsmgibIGTEm8NNZYzd4hsofXYvXIKdlvD1O7LFa8YvtygZ8X-OUPpuT6n3W5tIYQR9tyst3dy6znBbm0eA1Z90p_AbxGh5g=.png" class="img-fluid"></p>
<p>Helpfully, Scikit-learn also provides a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank">built-in function</a> which we can use to carry out this <strong>train-test split</strong> before we train our algorithm:</p>
<div id="cell-99" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, since the variable names would start to get long if we kept using <code>feature_matrix</code> and <code>label_vec</code>, I’m just going to call the feature matrix <code>X</code> and the label vector <code>y</code>, so that we can use notation like <code>X_train</code> to denote the <strong>training data portion</strong> of our full matrix <code>X</code></p>
<p><em>(Also note that this function takes a <code>random_state</code> parameter, which here I’m setting to <code>5000</code> just like we did above for our NumPy random number generator, to ensure the same results across different runs)</em></p>
<div id="cell-101" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> feature_matrix</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> label_vec</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">5000</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And now we can look at the <code>.shape</code> attribute on our newly-split data to verify that it has split it in the proportions we expected (80% for training data, 20% for test data):</p>
<div id="cell-103" class="cell" data-outputid="a81b4f70-8460-47ed-d030-70a1b3accd28" data-execution_count="45">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>X_train.shape, X_test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>((546, 14), (137, 14))</code></pre>
</div>
</div>
<div id="cell-104" class="cell" data-outputid="cfd34e76-6593-485e-aa01-f5d30387d211" data-execution_count="46">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>y_train.shape, y_test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>((546,), (137,))</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Final Aside: Why Do We Have To Do This Split?</strong><br><br>As a quick tldr here, the reason we have to do this train-test split (at least, going one level deeper than the reason given above about the need for generalization) is because an algorithm which is <strong>not restricted by the need to generalize</strong> can often simply “memorize” every bit of information in the training data, so that if it is <strong>evaluated</strong> on the same data that it is <strong>trained on</strong>, then (with enough parameters to fit) it can always achieve a <strong>perfect score</strong>. And yet, despite achieving this perfect score, the algorithm will typically do <strong>worse</strong> than an algorithm which did <strong>not</strong> achieve a perfect score but was trained using a train-test split, because in the perfect-score case the algorithm is typically <strong>overfitting</strong>!<br><br>To see what I mean, pictorally, consider the following plots from the extra slides linked above (run the Python code cell to generate the images side-by-side):</p>
</blockquote>
<div id="cell-106" class="cell" data-outputid="23e28779-207d-437f-d9f8-8440abaf227f" data-execution_count="47">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML, Image</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>style <span class="op">=</span> <span class="st">"&lt;style&gt;#output-body{display:flex; flex-direction: row;}&lt;/style&gt;"</span></span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>display(HTML(style))</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>base_url <span class="op">=</span> <span class="st">"https://jpj.georgetown.domains/dsan5000-scratch/feature-selection"</span></span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>tr_data_fname <span class="op">=</span> <span class="st">"overfitting_training_data.png"</span></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>model_fname <span class="op">=</span> <span class="st">"overfitting_models.png"</span></span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>display(Image(<span class="ss">f"</span><span class="sc">{</span>base_url<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>tr_data_fname<span class="sc">}</span><span class="ss">"</span>))</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>display(Image(<span class="ss">f"</span><span class="sc">{</span>base_url<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>model_fname<span class="sc">}</span><span class="ss">"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<style>#output-body{display:flex; flex-direction: row;}</style>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="DSAN5000_Feature_Selection_Full_files/figure-html/cell-48-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="DSAN5000_Feature_Selection_Full_files/figure-html/cell-48-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p>Here we can see the issue, when it comes to <strong>performance on the training data</strong> vs.&nbsp;<strong>generalization</strong>: the <strong>polynomial</strong> model, in this case, had many parameters that it could fit, such that it was able to produce a curve perfectly passing through every point in the training data, achieving 100% accuracy (here we’re dealing with <strong>continuous</strong> rather than <strong>discrete</strong> data, so we can’t use the F1 score, but the same intuition holds for how we evaluate regression tasks, which we’ll learn soon 🙂)<br><br>However, when it comes time for our two trained models to try and predict the <strong>label</strong> (in this case, the <strong>y value</strong>) for a <strong>new data point</strong> which it has never seen before, we can see the issue with overfitting:</p>
</blockquote>
<p><img src="https://jpj.georgetown.domains/dsan5000-scratch/feature-selection/overfitting_test_data.png" width="40%"></p>
<blockquote class="blockquote">
<p>We see that now, since the <strong>underlying Data-Generating Process (DGP)</strong> was <strong>linear</strong>, the polynomial model now does <strong>worse</strong> than the linear model, despite its perfect score on the training data. This is because, rather than trying to learn a <a href="https://en.wikipedia.org/wiki/Occam%27s_razor" target="_blank"><strong>parsimonious</strong> model</a>—that is, a model that represents the data sufficiently well under the constraint of being as “simple” as possible—the polynomial model simply “memorized” the training data, in the sense of constructing an erratic line with lots of oscillation in order to perfectly hit every point.<br><br>Quantitatively, we can see the effects of overfitting by <strong>comparing</strong> the performance of the linear (<span class="math inline">\(\textsf{Lin}\)</span>) and polynomial (<span class="math inline">\(\textsf{Poly}\)</span>) models, not on the <strong>training data</strong> but on the unseen <strong>test data</strong>. Computing the <span class="math inline">\(R^2\)</span> value for both models, we get:</p>
</blockquote>
<p><span class="math display">\[
\begin{align*}
R^2(\textsf{Lin}) &amp;\approx 0.841 \\
R^2(\textsf{Poly}) &amp;\approx 0.186
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p>and if we use a different metric which we’ll see a lot more often for regression tasks, the <strong>Residual Sum of Squares</strong> (literally, the sum of the squared distances between each point and the prediction made by the algorithm, so that in this case higher values indicate worse performance), we get:</p>
</blockquote>
<p><span class="math display">\[
\begin{align*}
RSS(\textsf{Lin}) &amp;\approx 0.186 \\
RSS(\textsf{Poly}) &amp;\approx 0.843
\end{align*}
\]</span></p>
<blockquote class="blockquote">
<p><em>(You’ll notice that, with respect to our data here, one of these measures seems to just be 1 minus the other measure—until we get to these measures in the class, you can dive into this and try to figure out if this will always be the case, and/or, why it is the case here!)</em></p>
</blockquote>
</section>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">(6.3) Training the Model</h3>
<p>Now that we understand these two important factors we’ll need to worry about (<strong>normalizing</strong> the data and performing the <strong>train-test split</strong>) in any Machine Learning task, we can now finally train our SVM classifier on the <strong>normalized training data</strong>, using the <code>.fit()</code> function like we saw before, but this time on just the <strong>training</strong> data:</p>
<div id="cell-111" class="cell" data-outputid="78feecc0-22d1-4811-b30d-4edc995b3bf2" data-execution_count="48">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Here I re-initialize the `clf` object, so that sklearn "starts from scratch"</span></span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rather than thinking we want it to update the previous model (the model that</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co"># was trained on the full dataset rather than just the training data). Also,</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="co"># since we're going to want to use the scaled data explicitly later on, here I</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="co"># do *not* use the make_pipeline() function like we did above, but manually</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="co"># apply the two steps (scaling and then training the SVC) separately</span></span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb63-8"><a href="#cb63-8" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train)</span>
<span id="cb63-9"><a href="#cb63-9" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.SVC()</span>
<span id="cb63-10"><a href="#cb63-10" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train_scaled, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox" checked=""><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC()</pre></div></div></div></div></div>
</div>
</div>
</section>
<section id="evaluating-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model">(6.4) Evaluating the Model</h3>
<p>And now we can <strong>evaluate</strong> our trained model by seeing how well it does at predicting the <strong>test data labels</strong> when given the <strong>test data features</strong>:</p>
<div id="cell-114" class="cell" data-outputid="c9bb0727-0d5f-4e34-eb0e-92d5d982f1d5" data-execution_count="49">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>X_test_scaled <span class="op">=</span> scaler.transform(X_test)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>test_predictions <span class="op">=</span> clf.predict(X_test_scaled)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>test_predictions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>array([1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,
       1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,
       0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
       1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,
       0, 1, 0, 1, 0])</code></pre>
</div>
</div>
<div id="cell-115" class="cell" data-outputid="b06b836d-7146-42e9-a5d2-1c419c6c0171" data-execution_count="50">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>test_predictions.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>(137,)</code></pre>
</div>
</div>
<p>And we see that the <code>.predict()</code> function has given us an array of values representing the predictions made by our trained classifier given the <strong>test data features</strong> in <code>X_test</code>. To evaluate these predictions, just like we did before in the random-guessing and guess-most-frequent-label cases, we can compute the <strong>F1 score</strong> using the Scikit-learn function:</p>
<div id="cell-117" class="cell" data-outputid="5a13522a-e23f-4083-e6dc-ed585a4eed5b" data-execution_count="51">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>f1_score(</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> y_test,</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> test_predictions</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>0.8783783783783783</code></pre>
</div>
</div>
<p>And we are happy now since we see that, by actually <strong>learning</strong> statistical relationships between the features and the labels in the training data, we have been able to <strong>beat both of our baseline models</strong> in terms of F1 score! Our updated algorithm performance table now looks like (where I have removed the accuracy column, since that was just for demonstration of the earlier point about why we instead use F1 score):</p>
<table class="table">
<thead>
<tr class="header">
<th>Algorithm</th>
<th>F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ML Algorithm + <strong>Feature Selection</strong></td>
<td><strong>?</strong></td>
</tr>
<tr class="even">
<td>ML Algorithm <strong>without</strong> Feature Selection</td>
<td><strong>0.878</strong></td>
</tr>
<tr class="odd">
<td>Baseline: Guess Most Frequent Label</td>
<td><strong>0.700</strong></td>
</tr>
<tr class="even">
<td>Baseline: Random Guessing</td>
<td><strong>0.531</strong></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="augmenting-our-off-the-shelf-model-with-feature-selection" class="level2">
<h2 class="anchored" data-anchor-id="augmenting-our-off-the-shelf-model-with-feature-selection">(7) Augmenting our “Off-The-Shelf” Model With Feature Selection</h2>
<p>Finally, we now not only have a pair of “overall” baselines (random-guessing and guess-most-frequent-label), but a baseline representing the performance of our classifier <strong>without</strong> any feature selection being performed, which we can use to quantify any <strong>improvement</strong> we’re able to achieve over this base ML model by employing feature selection techniques.</p>
<p>Now, whereas for your Lab 3.2 Assignment you will be conducting a <strong>grid search</strong> over all possible subsets of the full set of features, to get you started here we will just take a few of the variables we talked about in the beginning of the lab (or in class):</p>
<ul>
<li>Some of which we think may be extremely helpful for predicting playoff success: e.g., <code>PtsSeason</code>, and</li>
<li>Some of which we think may be redundant and unhelpful: e.g., including all three of <code>Reb</code>, <code>Oreb</code> and <code>Dreb</code> in our model, since mathematically <code>Reb</code> is just the sum of <code>Oreb</code> and <code>Dreb</code>.</li>
</ul>
<p>So, using the definition of the <strong>merit score</strong> given to you at the beginning of the Lab 3.2 Assignment, let’s take a player-related variable like <code>mean_height</code> and a team-related variable like <code>To</code> as our two “base” variables we’d like to include in our model, and compute a “base” merit score for this subset <span class="math inline">\(S_2 = \{\texttt{mean\_height}, \texttt{To}\}\)</span>.</p>
<p>As a reminder, using our dictionary-format codebook, here are the descriptions and types of these two base variables:</p>
<div id="cell-121" class="cell" data-outputid="e19207d4-fdea-44af-fb9d-7b31a26039e5" data-execution_count="52">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>codebook_dict[<span class="st">'mean_height'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>{'description': "Mean height of the team's members that year, in cm",
 'type': 'float'}</code></pre>
</div>
</div>
<div id="cell-122" class="cell" data-outputid="121c6879-c2d8-4b8f-bcee-9dafa1635b1a" data-execution_count="53">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>codebook_dict[<span class="st">'To'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>{'description': "The team's average number of turnovers per game that year",
 'type': 'float'}</code></pre>
</div>
</div>
<p>Once we have this base merit score <span class="math inline">\(S_2\)</span> for these two features, we can examine how the merit score <strong>changes</strong> when:</p>
<ol type="1">
<li>We add <code>PtsSeason</code> to the base subset to form a new subset <span class="math inline">\(S'_3 = \{\texttt{mean\_height}, \texttt{To}, \texttt{PtsSeason}\}\)</span>, or</li>
<li>We add the three variables we think may be unhelpful (<code>Reb</code>, <code>Oreb</code>, and <code>Dreb</code>) to the base subset to form a new subset <span class="math inline">\(S'_5 = \{\texttt{mean\_height}, \texttt{To}, \texttt{Reb}, \texttt{Oreb}, \texttt{Dreb}\}\)</span></li>
</ol>
<p>So, let’s start by computing our base merit score. We’ll use the SciPy library here, since it comes with a nice <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html" target="_blank"><code>spearmanr()</code> function</a> for computing the correlations we want. Note that this function generates an output containing <strong>two</strong> pieces of information: the info that we want, the correlation coefficient specifically, can be obtained by accessing the <code>.statistic</code> attribute on the result. That is, the following code will extract the correlation coefficient specifically from the object produced by <code>spearmanr()</code>:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>spearman_result <span class="op">=</span> scipy.stats.spearmanr(vec1, vec2)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>spearman_corr_coef <span class="op">=</span> spearman_result.statistic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Or, to combine this into one line, you can use</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>spearman_corr_coef <span class="op">=</span> scipy.stats.spearmanr(vec1, vec2).statistic</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Also, to make our lives easier, we’ll quickly make a <code>DataFrame</code> object which contains the <strong>scaled feature data</strong> but with columns representing the <strong>names</strong> of each feature, so that we can easily go from the name of the feature that we want to the column index of that feature in <code>X_train_scaled</code>:</p>
<div id="cell-124" class="cell" data-outputid="9df18c27-7a27-4dd6-afca-f77d260ebb4e" data-execution_count="54">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>X_train_scaled_df <span class="op">=</span> pd.DataFrame(X_train_scaled, columns<span class="op">=</span>feature_cols)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>X_test_scaled_df <span class="op">=</span> pd.DataFrame(X_test_scaled, columns<span class="op">=</span>feature_cols)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>X_train_scaled_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mean_age</th>
<th data-quarto-table-cell-role="th">mean_height</th>
<th data-quarto-table-cell-role="th">mean_weight</th>
<th data-quarto-table-cell-role="th">PtsSeason</th>
<th data-quarto-table-cell-role="th">OppPtsSeason</th>
<th data-quarto-table-cell-role="th">PtsDiff</th>
<th data-quarto-table-cell-role="th">Reb</th>
<th data-quarto-table-cell-role="th">Ast</th>
<th data-quarto-table-cell-role="th">Stl</th>
<th data-quarto-table-cell-role="th">Blk</th>
<th data-quarto-table-cell-role="th">To</th>
<th data-quarto-table-cell-role="th">Pf</th>
<th data-quarto-table-cell-role="th">Dreb</th>
<th data-quarto-table-cell-role="th">Oreb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>0.868557</td>
<td>-0.833381</td>
<td>-0.670704</td>
<td>0.449983</td>
<td>0.261535</td>
<td>0.258920</td>
<td>1.103405</td>
<td>0.130919</td>
<td>-0.716876</td>
<td>0.361359</td>
<td>0.681978</td>
<td>-1.959417</td>
<td>0.998052</td>
<td>0.001158</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>0.432615</td>
<td>0.129837</td>
<td>0.339992</td>
<td>-1.327738</td>
<td>-1.554359</td>
<td>0.347875</td>
<td>0.808684</td>
<td>-0.907914</td>
<td>1.088957</td>
<td>-0.622078</td>
<td>0.326078</td>
<td>-0.754511</td>
<td>-0.119384</td>
<td>1.405650</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>0.884333</td>
<td>-1.188430</td>
<td>-0.578330</td>
<td>-0.042065</td>
<td>0.308498</td>
<td>-0.497191</td>
<td>-0.370201</td>
<td>-0.611105</td>
<td>1.314686</td>
<td>-0.745007</td>
<td>-1.008545</td>
<td>0.220889</td>
<td>-0.352183</td>
<td>0.001158</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>-0.001146</td>
<td>-0.720411</td>
<td>0.774420</td>
<td>-0.216663</td>
<td>-0.286364</td>
<td>0.103250</td>
<td>-0.812282</td>
<td>1.169751</td>
<td>0.524634</td>
<td>-0.622078</td>
<td>0.148128</td>
<td>-1.672534</td>
<td>0.206535</td>
<td>-1.473560</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>0.404715</td>
<td>-0.637567</td>
<td>-0.560475</td>
<td>-0.010320</td>
<td>0.167610</td>
<td>-0.252567</td>
<td>-0.812282</td>
<td>0.130919</td>
<td>-0.265418</td>
<td>-1.359655</td>
<td>-0.296746</td>
<td>1.597924</td>
<td>-1.004021</td>
<td>0.422505</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<section id="the-base-merit-score-s_2" class="level3">
<h3 class="anchored" data-anchor-id="the-base-merit-score-s_2">(7.1) The Base Merit Score <span class="math inline">\(S_2\)</span></h3>
<div id="cell-126" class="cell" data-outputid="95706fad-b30a-472e-fd84-25090ff3c801" data-execution_count="55">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> spearmanr</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Computing the base merit score S_2. Here we're going to use Spearman correlation coefficients,</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a><span class="co"># though remember that your assignment code needs to work for *either* Pearson</span></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a><span class="co"># or Spearman correlation coefficients</span></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 1: Compute the between-feature correlations</span></span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>included_vars_s2 <span class="op">=</span> [<span class="st">'mean_height'</span>, <span class="st">'To'</span>]</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>included_vars_df <span class="op">=</span> X_train_scaled_df[included_vars_s2].copy()</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>height_turnover_corr <span class="op">=</span> spearmanr(included_vars_df[<span class="st">'mean_height'</span>], included_vars_df[<span class="st">'To'</span>]).statistic</span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>height_turnover_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<pre><code>-0.073349157425512</code></pre>
</div>
</div>
<div id="cell-127" class="cell" data-outputid="76c81dbf-4e3a-4a84-b717-c7a9e50ecd0e" data-execution_count="56">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Compute the correlations between each feature and the label</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>height_playoff_corr <span class="op">=</span> spearmanr(included_vars_df[<span class="st">'mean_height'</span>], y_train).statistic</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>height_playoff_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>0.01256038600061839</code></pre>
</div>
</div>
<div id="cell-128" class="cell" data-outputid="096fa862-066e-42ec-f8c3-8ce2f871bc3d" data-execution_count="57">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>turnover_playoff_corr <span class="op">=</span> spearmanr(included_vars_df[<span class="st">'To'</span>], y_train).statistic</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>turnover_playoff_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<pre><code>-0.30211010989735243</code></pre>
</div>
</div>
<div id="cell-129" class="cell" data-outputid="6e089c74-4c6c-41ad-8f74-2505145f53c8" data-execution_count="58">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Since we only have two features here, we have only one correlation value, so</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="co"># that the mean is just this one value</span></span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>mean_xx_corr <span class="op">=</span> height_turnover_corr</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="co"># mean_xy_corr is the mean of the two feature-vs-label correlations we computed</span></span>
<span id="cb83-6"><a href="#cb83-6" aria-hidden="true" tabindex="-1"></a><span class="co"># above</span></span>
<span id="cb83-7"><a href="#cb83-7" aria-hidden="true" tabindex="-1"></a>mean_xy_corr <span class="op">=</span> np.mean([height_playoff_corr, turnover_playoff_corr])</span>
<span id="cb83-8"><a href="#cb83-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of features: </span><span class="sc">{</span>k<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb83-9"><a href="#cb83-9" aria-hidden="true" tabindex="-1"></a>merit_score_numer <span class="op">=</span> k <span class="op">*</span> np.absolute(mean_xy_corr)</span>
<span id="cb83-10"><a href="#cb83-10" aria-hidden="true" tabindex="-1"></a>merit_score_denom <span class="op">=</span> np.sqrt(k <span class="op">+</span> k <span class="op">*</span> (k <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> np.absolute(mean_xx_corr))</span>
<span id="cb83-11"><a href="#cb83-11" aria-hidden="true" tabindex="-1"></a>merit_score_s2 <span class="op">=</span> merit_score_numer <span class="op">/</span> merit_score_denom</span>
<span id="cb83-12"><a href="#cb83-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Merit score: </span><span class="sc">{</span>merit_score_s2<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of features: 2
Merit score: 0.18536160983305905</code></pre>
</div>
</div>
</section>
<section id="the-augmented-merit-score-s_3" class="level3">
<h3 class="anchored" data-anchor-id="the-augmented-merit-score-s_3">(7.2) The Augmented Merit Score <span class="math inline">\(S'_3\)</span></h3>
<p>Next we can compute the merit score for <span class="math inline">\(S'_{3}\)</span>: here we’re including <code>mean_height</code>, <code>To</code>, and <code>PtsSeason</code>, so that the math will get a bit more complicated, since we have to consider all possible pairs of features. So, to handle this and to prepare us for future cases, we’ll make a function which lets us compute the correlation coefficients for general subsets of the features:</p>
<p><em>(Note that this is <strong>not</strong> the most efficient way to do this! Loops almost never are. But in this case I’m writing the function using a loop to make it a bit more straightforward how the function works / how it makes sure to consider all possible pairs)</em></p>
<div id="cell-132" class="cell" data-outputid="f5ea05e5-d4ee-4ddd-9c2a-7ba45fad85b0" data-execution_count="59">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A quick illustration of how the itertools.combinations() function works</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="st">'a'</span>,<span class="st">'b'</span>,<span class="st">'c'</span>]</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(itertools.combinations(x, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>[('a', 'b'), ('a', 'c'), ('b', 'c')]</code></pre>
</div>
</div>
<div id="cell-133" class="cell" data-outputid="c07912a0-2cff-44c1-fd10-13d761d399c0" data-execution_count="60">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a><span class="co"># And while we're looking at helpful Python utilities for making nice variables</span></span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a><span class="co"># to loop over: how Python's built-in zip() function works</span></span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="st">'a'</span>,<span class="st">'b'</span>,<span class="st">'c'</span>]</span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(<span class="bu">zip</span>(x,y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>[('a', 1), ('b', 2), ('c', 3)]</code></pre>
</div>
</div>
<div id="cell-134" class="cell" data-outputid="3326d34c-7b76-4b7b-dcf7-580d6369d65c" data-execution_count="61">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># And finally, how itertools.product() works to create cartesian products</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="st">'a'</span>,<span class="st">'b'</span>,<span class="st">'c'</span>]</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> [<span class="dv">10</span>]</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(itertools.product(x,c))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>[('a', 10), ('b', 10), ('c', 10)]</code></pre>
</div>
</div>
<div id="cell-135" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mean_xx_corr(x_df):</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>  df_colnames <span class="op">=</span> x_df.columns</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This will contain our final set of x&lt;-&gt;x correlations</span></span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>  xx_corrs <span class="op">=</span> []</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Now we use itertools to iterate over all possible *pairs* of</span></span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># elements from df_cols</span></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>  df_colname_pairs <span class="op">=</span> itertools.combinations(df_colnames, <span class="dv">2</span>)</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> colname1, colname2 <span class="kw">in</span> df_colname_pairs:</span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the first column we're considering</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>    col1 <span class="op">=</span> x_df[colname1]</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract the second column</span></span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>    col2 <span class="op">=</span> x_df[colname2]</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># And compute the correlation</span></span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>    xx_pair_corr <span class="op">=</span> spearmanr(col1, col2).statistic</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    xx_corrs.append(xx_pair_corr)</span>
<span id="cb91-16"><a href="#cb91-16" aria-hidden="true" tabindex="-1"></a>  <span class="co"># And now that the loop has finished running, we can return the **mean**</span></span>
<span id="cb91-17"><a href="#cb91-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># of the correlation values we've accumulated in the `xx_corrs` list</span></span>
<span id="cb91-18"><a href="#cb91-18" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.mean(xx_corrs)</span>
<span id="cb91-19"><a href="#cb91-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-20"><a href="#cb91-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_mean_xy_corr(x_df, y_vec):</span>
<span id="cb91-21"><a href="#cb91-21" aria-hidden="true" tabindex="-1"></a>  df_colnames <span class="op">=</span> x_df.columns</span>
<span id="cb91-22"><a href="#cb91-22" aria-hidden="true" tabindex="-1"></a>  xy_corrs <span class="op">=</span> []</span>
<span id="cb91-23"><a href="#cb91-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> colname <span class="kw">in</span> df_colnames:</span>
<span id="cb91-24"><a href="#cb91-24" aria-hidden="true" tabindex="-1"></a>    x_col <span class="op">=</span> x_df[colname]</span>
<span id="cb91-25"><a href="#cb91-25" aria-hidden="true" tabindex="-1"></a>    xy_pair_corr <span class="op">=</span> spearmanr(x_col, y_vec)</span>
<span id="cb91-26"><a href="#cb91-26" aria-hidden="true" tabindex="-1"></a>    xy_corrs.append(xy_pair_corr)</span>
<span id="cb91-27"><a href="#cb91-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># And return the mean</span></span>
<span id="cb91-28"><a href="#cb91-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.mean(xy_corrs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-136" class="cell" data-outputid="7ad1957b-b432-40d5-fee7-3317f1309c59" data-execution_count="63">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>included_vars_sp3 <span class="op">=</span> [<span class="st">'mean_height'</span>, <span class="st">'To'</span>, <span class="st">'PtsSeason'</span>]</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>included_vars_df <span class="op">=</span> X_train_scaled_df[included_vars_sp3].copy()</span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>mean_xx_corr <span class="op">=</span> compute_mean_xx_corr(included_vars_df)</span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a>mean_xy_corr <span class="op">=</span> compute_mean_xy_corr(included_vars_df, y_train)</span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a>mean_xx_corr, mean_xy_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>(-0.117371681544074, 0.1169299925506893)</code></pre>
</div>
</div>
<p>So that now we can create a simple <code>compute_merit_score()</code> function that takes these outputs from <code>compute_mean_xx_corr()</code> and <code>compute_mean_xy_corr()</code> and uses them to produce a merit score:</p>
<div id="cell-138" class="cell" data-outputid="fc777385-c2d1-4a18-ead2-5afd551b7b3e" data-execution_count="64">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_merit_score(num_features, mean_xx_corr, mean_xy_corr):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>  merit_score_numer <span class="op">=</span> k <span class="op">*</span> np.absolute(mean_xy_corr)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>  merit_score_denom <span class="op">=</span> np.sqrt(k <span class="op">+</span> k <span class="op">*</span> (k <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> np.absolute(mean_xx_corr))</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>  merit_score <span class="op">=</span> merit_score_numer <span class="op">/</span> merit_score_denom</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> merit_score</span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>merit_score_sp3 <span class="op">=</span> compute_merit_score(<span class="dv">3</span>, mean_xx_corr, mean_xy_corr)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Merit score (S'_3): </span><span class="sc">{</span>merit_score_sp3<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Merit score (S'_3): 0.14221129699638796</code></pre>
</div>
</div>
</section>
<section id="the-augmented-merit-score-s_5" class="level3">
<h3 class="anchored" data-anchor-id="the-augmented-merit-score-s_5">(7.3) The Augmented Merit Score <span class="math inline">\(S'_5\)</span></h3>
<p>Now let’s use the same functions we used to compute <span class="math inline">\(S'_3\)</span> here, to compute the needed <span class="math inline">\(x\)</span>-to-<span class="math inline">\(x\)</span> and <span class="math inline">\(x\)</span>-to-<span class="math inline">\(y\)</span> correlation coefficients for the subset of 5 features:</p>
<div id="cell-141" class="cell" data-outputid="41e4021c-eb01-4839-eca0-cc1dffa984e7" data-execution_count="65">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>included_vars_sp5 <span class="op">=</span> [<span class="st">'mean_height'</span>, <span class="st">'To'</span>, <span class="st">'Reb'</span>, <span class="st">'Oreb'</span>, <span class="st">'Dreb'</span>]</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>included_vars_df <span class="op">=</span> X_train_scaled_df[included_vars_sp5].copy()</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>mean_xx_corr <span class="op">=</span> compute_mean_xx_corr(included_vars_df)</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>mean_xy_corr <span class="op">=</span> compute_mean_xy_corr(included_vars_df, y_train)</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>mean_xx_corr, mean_xy_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>(0.0601995027606065, 0.07702604065042143)</code></pre>
</div>
</div>
<div id="cell-142" class="cell" data-outputid="d0087687-4d48-4d25-e167-067c41848894" data-execution_count="66">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>merit_score_sp5 <span class="op">=</span> compute_merit_score(<span class="dv">5</span>, mean_xx_corr, mean_xy_corr)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Merit score (S'_5): </span><span class="sc">{</span>merit_score_sp5<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Merit score (S'_5): 0.10025394179618205</code></pre>
</div>
</div>
</section>
<section id="putting-the-three-scores-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-the-three-scores-together">(7.4) Putting the Three Scores Together</h3>
<p>So we can see that, just checking these three potential subsets of all the features, we obtain the following merit scores:</p>
<table class="table">
<thead>
<tr class="header">
<th>Subset</th>
<th>Variables Included</th>
<th>Merit Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(S_2\)</span></td>
<td><code>mean_height</code>, <code>To</code></td>
<td><strong>0.185</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(S'_3\)</span></td>
<td><code>mean_height</code>, <code>To</code>, <code>PtsSeason</code></td>
<td><strong>0.142</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(S'_5\)</span></td>
<td><code>mean_height</code>, <code>To</code>, <code>Reb</code>, <code>Oreb</code>, <code>Dreb</code></td>
<td><strong>0.100</strong></td>
</tr>
</tbody>
</table>
<p>Like we expected, the subset with the lowest merit was the subset containing redundant information: we don’t need all three of <code>Reb</code>, <code>Oreb</code>, and <code>Dreb</code>, but only (if we want to include information about rebounds) at most two of the three. So, for example, let’s compute one more merit score for a new subset <span class="math inline">\(S''_3\)</span>, seeing if we can improve upon the 0.100 score if we only include <code>Reb</code>:</p>
<div id="cell-145" class="cell" data-outputid="62071e95-5bb1-49b0-f8a7-3e445668f18c" data-execution_count="67">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>included_vars_spp3 <span class="op">=</span> [<span class="st">'mean_height'</span>, <span class="st">'To'</span>, <span class="st">'Reb'</span>]</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>included_vars_df <span class="op">=</span> X_train_scaled_df[included_vars_spp3].copy()</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>mean_xx_corr <span class="op">=</span> compute_mean_xx_corr(included_vars_df)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>mean_xy_corr <span class="op">=</span> compute_mean_xy_corr(included_vars_df, y_train)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>mean_xx_corr, mean_xy_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>(-0.022629044631427443, 0.10761536360841224)</code></pre>
</div>
</div>
<div id="cell-146" class="cell" data-outputid="7b261346-ef56-4597-8875-031bb3ca2505" data-execution_count="68">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>merit_score_spp3 <span class="op">=</span> compute_merit_score(<span class="dv">3</span>, mean_xx_corr, mean_xy_corr)</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Merit score (S''_3): </span><span class="sc">{</span>merit_score_spp3<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Merit score (S''_3): 0.14727417486943475</code></pre>
</div>
</div>
<p>And, to add one more relevant piece to our investigation, let’s also compute the score for a new subset <span class="math inline">\(S'_4\)</span> where we include <code>Reb</code> <strong>as well as</strong> <code>PtsSeason</code> on top of our base variables:</p>
<div id="cell-148" class="cell" data-outputid="2f3b885b-096f-422b-d5bc-938c931c81dc" data-execution_count="69">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>included_vars_sp4 <span class="op">=</span> [<span class="st">'mean_height'</span>, <span class="st">'To'</span>, <span class="st">'Reb'</span>, <span class="st">'PtsSeason'</span>]</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>included_vars_df <span class="op">=</span> X_train_scaled_df[included_vars_sp4].copy()</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>mean_xx_corr <span class="op">=</span> compute_mean_xx_corr(included_vars_df)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>mean_xy_corr <span class="op">=</span> compute_mean_xy_corr(included_vars_df, y_train)</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>mean_xx_corr, mean_xy_corr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>(0.007649292847057176, 0.10839650518198726)</code></pre>
</div>
</div>
<div id="cell-149" class="cell" data-outputid="4b7434e3-fa44-450e-d75d-b0716b6722a5" data-execution_count="70">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>merit_score_sp4 <span class="op">=</span> compute_merit_score(<span class="dv">4</span>, mean_xx_corr, mean_xy_corr)</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Merit score (S'_4): </span><span class="sc">{</span>merit_score_sp4<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Merit score (S'_4): 0.15156660582136663</code></pre>
</div>
</div>
<p>Our final merit score table (sorted by merit score, in decreasing order) now looks like:</p>
<table class="table">
<thead>
<tr class="header">
<th>Subset</th>
<th>Variables Included</th>
<th>Merit Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(S_2\)</span></td>
<td><code>mean_height</code>, <code>To</code></td>
<td><strong>0.185</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(S'_4\)</span></td>
<td><code>mean_height</code>, <code>To</code>, <code>Reb</code>, <code>PtsSeason</code></td>
<td><strong>0.152</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(S''_3\)</span></td>
<td><code>mean_height</code>, <code>To</code>, <code>Reb</code></td>
<td><strong>0.147</strong></td>
</tr>
<tr class="even">
<td><span class="math inline">\(S'_3\)</span></td>
<td><code>mean_height</code>, <code>To</code>, <code>PtsSeason</code></td>
<td><strong>0.142</strong></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(S'_5\)</span></td>
<td><code>mean_height</code>, <code>To</code>, <code>Reb</code>, <code>Oreb</code>, <code>Dreb</code></td>
<td><strong>0.100</strong></td>
</tr>
</tbody>
</table>
<p>And so we see that, despite getting somewhat close to our base (2-variable) subset <span class="math inline">\(S_2\)</span> by including <code>Reb</code> and <code>PtsSeason</code> while excluding <code>Oreb</code> and <code>Dreb</code>, there still wasn’t enough <strong>additional information</strong> contained in <code>Reb</code> and/or <code>PtsSeason</code> to <strong>justify</strong> using these in addition to <code>mean_height</code> and <code>To</code>. In other words, if we adopt this merit score as our metric, then out of these five possible subsets of the full feature space, we expect that we will get the best performance by just using the subset <span class="math inline">\(S_2\)</span> to predict the labels. Let’s see what happens when we try this, using our SVM classifier:</p>
<div id="cell-151" class="cell" data-outputid="4904959a-d368-40ef-cd88-1356b9bf1cfa" data-execution_count="71">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>included_vars_s2 <span class="op">=</span> [<span class="st">'mean_height'</span>, <span class="st">'To'</span>]</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>X_train_scaled_s2 <span class="op">=</span> X_train_scaled_df[included_vars_s2].copy()</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll call our new classifier, where we're using feature selection to train</span></span>
<span id="cb108-4"><a href="#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="co"># it on only a subset of the full set of features, fs_clf, where the "fs"</span></span>
<span id="cb108-5"><a href="#cb108-5" aria-hidden="true" tabindex="-1"></a><span class="co"># stands for "feature selection"</span></span>
<span id="cb108-6"><a href="#cb108-6" aria-hidden="true" tabindex="-1"></a>fs_clf <span class="op">=</span> svm.SVC()</span>
<span id="cb108-7"><a href="#cb108-7" aria-hidden="true" tabindex="-1"></a>fs_clf.fit(X_train_scaled_s2, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox" checked=""><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC()</pre></div></div></div></div></div>
</div>
</div>
<div id="cell-152" class="cell" data-outputid="1141f9db-eb76-44c7-f713-008fbbd6a086" data-execution_count="72">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="co"># And evaluate, as before</span></span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>X_test_scaled_s2 <span class="op">=</span> X_test_scaled_df[included_vars_s2].copy()</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>test_predictions <span class="op">=</span> fs_clf.predict(X_test_scaled_s2)</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>test_predictions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>array([0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,
       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,
       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,
       0, 0, 1, 1, 1])</code></pre>
</div>
</div>
<div id="cell-153" class="cell" data-outputid="01c7a45d-9567-4211-f551-d996605cea7b" data-execution_count="73">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>f1_score(</span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>    y_true <span class="op">=</span> y_test,</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> test_predictions</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>0.6293706293706294</code></pre>
</div>
</div>
<p>And we see, to our horror, that our fairly <em>ad hoc</em> feature selection step did <strong>not</strong> improve our performance at all. In fact, it decreased our performance to the point that now our SVM does worse (in terms of F1 score, at least) than the guess-most-frequent-label approach 😰</p>
<p>Why did this happen? There are many potential reasons, but first off we can notice that we <strong>did not check all possible subsets of our features!</strong> The two variables we chose as our “base” variables may have been bad choices in this case, which constrained all of our subsequent subsets to also be bad choices for predictive subsets of the full feature space.</p>
<p>It also may be that, in terms of our particular setting, the merit score over-penalizes the inclusion of additional features: typically, when we’re dealing with large datasets with e.g.&nbsp;thousands or millions of features, it’s a <strong>good thing</strong> for the merit score to harshly penalize the inclusion of lots of additional variables, since we are trying to trim the feature space way down to just a few features which are extremely effective (in combination) for predicting the labels. Here, since we only have <strong>14 features</strong> in total, we may not need to apply such a harsh penalty until (say) the number of included features passes 7, since anything below 7 means we’re cutting the number of features in half.</p>
<p>So, I’ll leave it to you to figure out if perhaps there is a better <strong>feature selection approach</strong> in this case (notice how many different feature selection methods are <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection" target="_blank">implemented in Scikit-learn</a>, each one having different advantages and drawbacks in any given ML scenario!), but I used some fairly-simple methods available in Scikit-learn to arrive at the following subset:</p>
<p><span class="math display">\[
S''_4 = \{\texttt{mean\_age}, \texttt{OppPtsSeason}, \texttt{PtsDiff}, \texttt{To}\}
\]</span></p>
<p>which is <strong>not</strong> guaranteed to be the optimal subset, but <strong>does</strong> beat out our baseline off-the-shelf ML model, to produce a final <strong>F1 score table</strong> as follows (where I’ve given abbreviations to each algorithm, which I use in the plot below):</p>
<table class="table">
<thead>
<tr class="header">
<th>ID</th>
<th>Algorithm</th>
<th>F1 Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ML+FS</td>
<td>ML Algorithm + <strong>Feature Selection</strong></td>
<td><strong>0.898</strong></td>
</tr>
<tr class="even">
<td>ML</td>
<td>ML Algorithm <strong>without</strong> Feature Selection</td>
<td><strong>0.878</strong></td>
</tr>
<tr class="odd">
<td>MFL</td>
<td>Baseline: Guess Most Frequent Label</td>
<td><strong>0.700</strong></td>
</tr>
<tr class="even">
<td>RG</td>
<td>Baseline: Random Guessing</td>
<td><strong>0.531</strong></td>
</tr>
</tbody>
</table>
<p>So that we can <strong>plot the progress we made</strong> as we used more and more sophisticated techniques:</p>
<div id="cell-155" class="cell" data-outputid="27a61099-a082-4c82-8c0d-a1f2d43d4874" data-execution_count="74">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>sns.set_style(<span class="st">'whitegrid'</span>)</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>algs <span class="op">=</span> [<span class="st">'RG'</span>, <span class="st">'MFL'</span>, <span class="st">'ML'</span>, <span class="st">'ML+FS'</span>]</span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> [<span class="fl">0.531</span>, <span class="fl">0.700</span>, <span class="fl">0.878</span>, <span class="fl">0.898</span>]</span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a>alg_df <span class="op">=</span> pd.DataFrame({<span class="st">'Algorithm'</span>: algs, <span class="st">'F1 Score'</span>: scores})</span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>sns.lineplot(data<span class="op">=</span>alg_df, x<span class="op">=</span><span class="st">'Algorithm'</span>, y<span class="op">=</span><span class="st">'F1 Score'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"F1 Score Improvement: Random-Guessing to ML+Feature Selection"</span>)</span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="DSAN5000_Feature_Selection_Full_files/figure-html/cell-75-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In fact, to illustrate a bit more how effective feature selection can be: the approach I used, while being extremely “greedy” and thus not guaranteed to be optimal, still beat out the baseline ML model for every value of <span class="math inline">\(K\)</span> (the number of features to select) besides <span class="math inline">\(K = 13\)</span>, as can be seen in the following plot:</p>
<div id="cell-157" class="cell" data-outputid="f7f9320a-e998-4aa6-a8db-2ce053fc50cb" data-execution_count="75">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>fs_data_url <span class="op">=</span> <span class="st">"https://jpj.georgetown.domains/dsan5000-scratch/feature-selection/fs_scores.csv"</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>fs_df <span class="op">=</span> pd.read_csv(fs_data_url)</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>plot_obj <span class="op">=</span> sns.lineplot(data<span class="op">=</span>fs_df, x<span class="op">=</span><span class="st">'k'</span>, y<span class="op">=</span><span class="st">'f1_score'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>plot_obj.axhline(<span class="fl">0.878</span>, ls<span class="op">=</span><span class="st">'--'</span>, color<span class="op">=</span><span class="st">'orange'</span>)</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"F1 Scores for Increasingly-Larger Subsets of Features"</span>)</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb114-7"><a href="#cb114-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="DSAN5000_Feature_Selection_Full_files/figure-html/cell-76-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="appendix-possibly-helpful-code" class="level2">
<h2 class="anchored" data-anchor-id="appendix-possibly-helpful-code">Appendix: Possibly Helpful Code</h2>
<div id="cell-159" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In case you need this, to get all possible subsets of a python list!</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> chain, combinations</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> powerset(iterable):</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"powerset([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)"</span></span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> <span class="bu">list</span>(iterable)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> chain.from_iterable(combinations(s, r) <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(s)<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a><span class="co">#list(powerset(my_list))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>