---
title: "Week 7: Unsupervised Learning"
date: "Tuesday, October 3, 2023"
date-format: full
#date: last-modified
#date-format: "dddd MMM D, YYYY, HH:mm:ss"
lecnum: 7
categories:
  - "Class Sessions"
bibliography: "../_DSAN5000.bib"
format:
  revealjs:
    output-file: slides.html
    cache: false
    footer: "DSAN 5000-<span class='sec-num'>02</span> Week 7: Unsupervised Learning"
  html:
    output-file: index.html
    html-math-method: mathjax
    cache: false
metadata-files: 
  - "../_slide-meta.yml"
---



::: {.content-visible unless-format="revealjs"}

<center>
<a class="h2" href="./slides.html" target="_blank">Open slides in new window &rarr;</a>
</center>

:::

# Schedule {.smaller .small-title .crunch-title .crunch-callout data-name="Schedule"}

{{< include ../_r-globals.qmd >}}

{{< include ../_py-globals.qmd >}}

{{< include ../_tex-globals.qmd >}}

Today's Planned Schedule (Section <span class='sec-num'>02</span>):

{{< include ../_components/sched-w07.qmd >}}

# Week 06 Recap {data-stack-name="Recap"}

* Normalization
* Correlation and Covariance
* Distance Metrics

## Normalization

* Recall from last week's slides:
* IQR Rule (Tukey), suitable for general data
* Three-Sigma Rule, suitable for **Normally-distributed** data
* In either case: when we **remove outliers** using one of these methods, in the context of machine learning we call this **feature clipping**

## One More: Log-Scaling

* In math (I wish I had learned it like this), the $\log()$ function is a magic function that "reduces" complicated operations to less-complicated operations:
* **Exponentiation $\rightarrow$ Multiplication**:

  $$
  \log(a^b) = b\cdot \log(a)
  $$

* **Multiplication $\rightarrow$ Addition**:

  $$
  \log(a\cdot b) = \log(a) + \log(b)
  $$

## Why Does This Help Us?

* Tl;dr Humans have superpowers for identifying **linear relationships**: $y = mx + b$
* $\implies$ if we can use $\log()$, we also get superpowers for identifying **exponential relationships** for free, since

$$
y = e^{mx + b} \iff \log(y) = mx + b
$$

* If we see $mx + b$ in a **log-scale** plot, we can immediately infer the functional relationship!

## In Pictures {.crunch-title .crunch-images .crunch-math}

::: columns
::: {.column width="50%"}

```{r}
#| label: linear-plot
#| fig-width: 6
#| fig-height: 5
library(tidyverse)
N <- 50
x_min <- 1
x_max <- 5
x_vals <- runif(N, x_min, x_max)
noise_vals <- rnorm(N, 0, exp(5))
my_exp <- function(x) exp(3*x + 1)
y_exp <- my_exp(x_vals) + noise_vals
exp_df <- tibble(x=x_vals, y=y_exp)
ggplot(exp_df) +
  stat_function(data=data.frame(x=c(x_min,x_max)), fun = my_exp, linewidth = g_linewidth, linetype="dashed") +
  geom_point(aes(x=x, y=y), size = g_pointsize / 2) +
  dsan_theme("half") +
  labs(
    title="y = exp(3x + 1), Linear Scale"
  )
```

:::
::: {.column width="50%"}

```{r}
#| label: exp-plot
#| fig-width: 6
#| fig-height: 5
# Log2 scaling of the y axis (with visually-equal spacing)
library(scales)
ggplot(exp_df) +
  stat_function(data=data.frame(x=c(x_min,x_max)), fun = my_exp, linewidth = g_linewidth, linetype="dashed") +
  geom_point(aes(x=x, y=y), size = g_pointsize / 2) +
  dsan_theme("half") +
  scale_y_continuous(trans = log_trans(),
    breaks = log_breaks()) +
  labs(
    title="y = exp(3x + 1), Log Scale"
  )
```

:::

:::

$$
y = e^{mx + b} \iff \log(y) = mx + b
$$

## Covariance: Intuition 1.0 {.smaller .crunch-title .crunch-code .crunch-images .crunch-figures .crunch-ul}

* If we are **at the mean** $(\mu_x,\mu_y)$, what is **likelihood points to the right are also above?**
* Similarly,what is the **likelihood that points to the left are also below?**

::: columns
::: {.column width="33%"}

```{r}
#| label: collinear-cov-plot
#| fig-width: 3.8
#| fig-height: 3.8
library(tidyverse)
library(latex2exp)
gen_y_noisy <- function(x_val, eps) {
  lower <- max(-1, x_val - eps)
  upper <- min(1, x_val + eps)
  y_noisy <- runif(1, lower, upper)
  return(y_noisy)
}
N <- 100
x_vals <- runif(N, -1, 1)
x_mean <- mean(x_vals)
y_collinear <- x_vals
y_coll_mean <- mean(y_collinear, drop.na = TRUE)
df_collinear <- tibble(x=x_vals, y=y_collinear, rel="collinear")
# Force the points to be inside [-1,1]
y_noisy <- x_vals
for (i in 1:length(y_noisy)) {
  cur_x_val <- x_vals[i]
  y_noisy[i] <- gen_y_noisy(cur_x_val, 0.75)
}
y_noisy_mean <- mean(y_noisy, na.rm = TRUE)
#print(y_noisy_mean)
df_noisy <- tibble(x = x_vals, y = y_noisy, rel="noise")
# Label vals above and below mean
label_df <- tribble(
  ~x, ~y, ~label,
  0.5, 0.5, "+",
  -0.5, -0.5, "+",
  0.5, -0.5, "\u2212",
  -0.5, 0.5, "\u2212"
)
gen_cov_plot <- function(df) {
  x_mean = mean(df$x)
  y_mean = mean(df$y)
  ggplot(df, aes(x=x, y=y)) +
    geom_point() +
    geom_vline(xintercept = x_mean) +
    geom_hline(yintercept = y_mean) +
    #facet_grid(. ~ rel) + 
    geom_label(
      data=label_df,
      aes(x=x, y=y, label=label, color=label),
      alpha=0.75,
      size = g_pointsize * 1.5
    ) +
    scale_color_manual(values=c("darkgreen","red")) +
    dsan_theme() +
    remove_legend() +
    theme(
      #axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      #axis.ticks.x = element_blank(),
      #axis.text.y = element_blank(),
      #axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    ) +
    xlim(c(-1,1)) + ylim(c(-1,1)) +
    coord_fixed(ratio=1) +
    scale_x_continuous(breaks=c(x_mean), labels=c(TeX(r"($\mu_x$)"))) +
    scale_y_continuous(breaks=c(y_mean), labels=c(TeX(r"($\mu_y$)")))
}
gen_cov_table <- function(df, print_matches = FALSE) {
  x_mean <- mean(df$x, na.rm = TRUE)
  y_mean <- mean(df$y, na.rm = TRUE)
  df <- df |> mutate(
    x_contrib = ifelse(x > x_mean, "+", "-"),
    y_contrib = ifelse(y > y_mean, "+", "-"),
    match = x_contrib == y_contrib
  )
  contrib_crosstab <- table(df$y_contrib, df$x_contrib)
  colnames(contrib_crosstab) <- c("x-", "x+")
  rownames(contrib_crosstab) <- c("y-", "y+")
  if (!print_matches) {
    print(contrib_crosstab)
  } else {
    # Num matches
    num_matches <- sum(df$match)
    num_mismatch <- nrow(df) - num_matches
    writeLines(paste0(num_matches, " matches, ",num_mismatch," mismatches"))
    writeLines("\nCovariance:")
    writeLines(paste0(cov(df$x, df$y)))
  }
}
gen_cov_plot(df_collinear)
```

::: {layout-ncol=2}

```{r}
#| label: collinear-cov-table
#| echo: false
gen_cov_table(df_collinear)
```

```{r}
#| label: collinear-cov-matches
#| echo: false
gen_cov_table(df_collinear, print_matches = TRUE)
```

:::

:::
::: {.column width="33%"}

```{r}
#| label: noisy-cov-plot
#| fig-align: center
#| fig-width: 3.8
#| fig-height: 3.8
gen_cov_plot(df_noisy)
```

::: {layout-ncol=2}

```{r}
#| label: cov-table-noisy
#| echo: false
gen_cov_table(df_noisy)
```

```{r}
#| label: cov-matches-noisy
#| echo: false
gen_cov_table(df_noisy, print_matches = TRUE)
```

:::

:::
::: {.column width="33%"}

```{r}
#| label: neg-cov-plot
#| fig-align: center
#| fig-width: 3.8
#| fig-height: 3.8
#| # Force the points to be inside [-1,1]
y_noisy_neg <- x_vals
for (i in 1:length(y_noisy_neg)) {
  cur_x_val <- x_vals[i]
  y_noisy_neg[i] <- -gen_y_noisy(cur_x_val, 0.75)
}
y_noisy_neg_mean <- mean(y_noisy_neg, na.rm = TRUE)
#print(y_noisy_mean)
df_noisy_neg <- tibble(x = x_vals, y = y_noisy_neg, rel="noise")
gen_cov_plot(df_noisy_neg)
```

::: {layout-ncol=2}

```{r}
#| label: neg-cov-table
#| echo: false
gen_cov_table(df_noisy_neg)
```

```{r}
#| label: neg-cov-matches
#| echo: false
gen_cov_table(df_noisy_neg, print_matches = TRUE)
```

:::

:::
:::

## Covariance: Intuition 2.0 {.smaller .crunch-title .crunch-code .crunch-images .crunch-figures}

* Now, rather than just **is this point above-right?** (binary), let's compute **how above-right** it is!:

::: columns
::: {.column width="33%"}

```{r}
#| label: collinear-rect-plot
#| fig-width: 3.8
#| fig-height: 3.8
gen_rect_plot <- function(df, col_order=c("red","darkgreen")) {
  x_mean = mean(df$x)
  y_mean = mean(df$y)
  df <- df |> mutate(
      x_contrib = ifelse(x > x_mean, "+", "-"),
      y_contrib = ifelse(y > y_mean, "+", "-"),
      match = x_contrib == y_contrib
  )
  ggplot(df, aes(x=x, y=y)) +
    geom_point() +
    geom_vline(xintercept = x_mean) +
    geom_hline(yintercept = y_mean) +
    #facet_grid(. ~ rel) + 
    geom_rect(aes(xmin=x_mean, xmax=x, ymin=y_mean, ymax=y, fill=match), color='black', linewidth=0.1, alpha=0.075) +
    scale_color_manual(values=c("darkgreen","red")) +
    scale_fill_manual(values=col_order) +
    geom_label(
      data=label_df,
      aes(x=x, y=y, label=label, color=label),
      alpha=0.75,
      size = g_pointsize * 1.5
    ) +
    dsan_theme() +
    remove_legend() +
    theme(
      #axis.text.x = element_blank(),
      axis.title.x = element_blank(),
      #axis.ticks.x = element_blank(),
      #axis.text.y = element_blank(),
      #axis.ticks.y = element_blank(),
      axis.title.y = element_blank()
    ) +
    xlim(c(-1,1)) + ylim(c(-1,1)) +
    coord_fixed(ratio=1) +
    scale_x_continuous(breaks=c(x_mean), labels=c(TeX(r"($\mu_x$)"))) +
    scale_y_continuous(breaks=c(y_mean), labels=c(TeX(r"($\mu_y$)")))
}
gen_rect_plot(df_collinear, col_order=c("darkgreen","red"))
```

::: {layout-ncol=2}

```{r}
#| label: collinear-rect-table
#| echo: false
gen_cov_table(df_collinear)
```

```{r}
#| label: collinear-rect-matches
#| echo: false
gen_cov_table(df_collinear, print_matches = TRUE)
```

:::

:::
::: {.column width="33%"}

```{r}
#| label: noisy-rect-plot
#| fig-align: center
#| fig-width: 3.8
#| fig-height: 3.8
gen_rect_plot(df_noisy)
```

::: {layout-ncol=2}

```{r}
#| label: noisy-rect-table
#| echo: false
gen_cov_table(df_noisy)
```

```{r}
#| label: noisy-rect-matches
#| echo: false
gen_cov_table(df_noisy, print_matches = TRUE)
```

:::

:::
::: {.column width="33%"}

```{r}
#| label: neg-rect-plot
#| fig-align: center
#| fig-width: 3.8
#| fig-height: 3.8
gen_rect_plot(df_noisy_neg)
```

::: {layout-ncol=2}

```{r}
#| label: neg-rect-table
#| echo: false
gen_cov_table(df_noisy_neg)
```

```{r}
#| label: neg-rect-matches
#| echo: false
gen_cov_table(df_noisy_neg, print_matches = TRUE)
```

:::

:::
:::

## Distance Metrics {.crunch-title .crunch-ul .crunch-images .crunch-figures}

::: {layout-ncol=2}

::: {#cosine-dist}

* One More Important Metric! **Cosine Distance**

![$A = (5,0), B = (3,4) \implies \cos(A,B) = \frac{3}{5}$](images/cosine_dist_projection.svg){fig-align="center" width="360"}



:::

::: {#new-names}

* Plus **new names** for ones you already know!

* "Levenshtein Distance": Edit distance
* "Minkowski Distance": $L^p$-norm (Generalizes Euclidean, Manhattan, Min, and Max)

:::

:::

# Quiz Time!

* <a href='https://georgetown.instructure.com/courses/173310/quizzes/201583?module_item_id=3421671' target='_blank'>Quiz 3.2 <i class='bi bi-box-arrow-up-right ps-1'></i></a>

# Unsupervised Learning {data-stack-name="Unsupervised Learning"}

## What is Unsupervised Learning?

* Getting the computer to **do EDA for us!**

# K-Nearest Neighbors (KNN) {data-stack-name="KNN"}

## The KNN Algorithm

# Naïve Bayes Classifiers {data-stack-name="Naïve Bayes"}

## What is "Naïve" About It?

# Lab: Feature Selection in Scikit-Learn

## Scikit-Learn

# Lab Assignment Overview {data-stack-name="Lab"}

## References